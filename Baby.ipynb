{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop\n",
    "\n",
    "a = 0\n",
    "\n",
    "if a == 0:\n",
    "    b = 1\n",
    "else:\n",
    "    b = 2\n",
    "    \n",
    "# Dictionary \n",
    "    \n",
    "dict = {'Name': 'Zara', 'Age': 7, 'Class': 'First'}\n",
    "\n",
    "# How to write in python ?\n",
    "\n",
    "# Constant -> NAME_SURNAME\n",
    "# Type -> NameSurname\n",
    "# Global variable, local variable, fonction -> name_surname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path Desktop\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "from sklearn.externals import joblib\n",
    "from pylab import *\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import mixture\n",
    "from os.path import basename\n",
    "from itertools import izip_longest\n",
    "from collections import Counter\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from rpy2 import robjects as ro\n",
    "from rpy2.robjects.numpy2ri import numpy2ri\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "from rpy2.robjects.packages import importr\n",
    "R = ro.r\n",
    "pandas2ri.activate()\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "lme4 = importr('lme4') \n",
    "\n",
    "from swipe import Swipe\n",
    "\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "\n",
    "import rpy2\n",
    "import itertools\n",
    "import subprocess\n",
    "import math\n",
    "import scipy.stats as stats_scipy\n",
    "import time\n",
    "import pysptk\n",
    "import glob,os\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sb\n",
    "import librosa.display\n",
    "import numpy as np, scipy, matplotlib.pyplot as plt, sklearn, pandas as pd, librosa, urllib, IPython.display, os.path\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANNOTATIONS EXCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the labels file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame.from_csv('labels.csv')\n",
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_files = ['044/044.xls','050/050.xls','051/051.xls']\n",
    "pages = ['hungry','pee','sleepy']\n",
    "labels = pd.DataFrame()\n",
    "\n",
    "for label_file in label_files:\n",
    "    for page in pages: \n",
    "        data = pd.read_excel(label_file,page)\n",
    "        data = data[['Expiration/Inspiration', 'syllable duration', 'syllable start', \"comments\"]]\n",
    "        data.rename(inplace=True, columns={'Expiration/Inspiration':'label', 'syllable duration':'dur', 'syllable start':'start', 'comments':'file'})\n",
    "        labels = pd.concat([labels,data],ignore_index=True)  \n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Localisation Colums 0-3 : test.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename labels : 'SI' -> 0, 'IN' -> 1 et 'EX' -> 2. \n",
    "\n",
    "def rename_label1(label_str):\n",
    "    if label_str == 'IN':\n",
    "        return 1\n",
    "    if label_str == 'EX':\n",
    "        return 2\n",
    "    return label_str\n",
    "    \n",
    "labels['label']=labels['label'].apply(rename_label1)\n",
    "labels['end']=labels['start']+labels['dur']\n",
    "\n",
    "# Switch colums\n",
    "\n",
    "labels = labels[['label', 'dur','start','end','file']]\n",
    "labels.to_csv('labels.csv')\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the first file if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_name1 = '01m00d-1(pee&hungry)_050.wav'\n",
    "test_name2 = '01m13d-1(hungry)_051.wav'\n",
    "labels[labels.file == test_name1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATION DATA BASE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the mfcc file here, if it's already saved. \n",
    "\n",
    "We have mfcc normalized file because SVM only works with normalized features. We will compare performance for GMM to know if it's better to use normalization or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_file_mfcc_norm = np.load('all_file_mfcc_norm.npy').item()\n",
    "all_file_mfcc = np.load('all_file_mfcc.npy').item()\n",
    "\n",
    "# all_file_mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the labels per frames file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_file_frame_labels = np.load('all_file_frame_labels.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mfcc extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Windowing parameters for MFCCs\n",
    "\n",
    "SAMPLING_RATE = 44100\n",
    "L_WINDOWS = 0.020\n",
    "L_HOPS = 0.010\n",
    "L_WINDOW_N = int(round((L_WINDOWS)/(1./SAMPLING_RATE)))\n",
    "L_HOP_N = int(round((L_HOPS)/(1./SAMPLING_RATE)))\n",
    "L_WINDOW_N2 = 2**(L_WINDOW_N-1).bit_length() # POWER OF 2 FOR FFT\n",
    "L_HOP_N2 = 2**(L_WINDOW_N-1).bit_length()\n",
    "N_MFCC = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "babies = ['044','050','051']\n",
    "\n",
    "mfcc_scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# Dictionary of mfcc\n",
    "\n",
    "all_file_mfcc = {}\n",
    "all_file_mfcc_norm = {}\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states: \n",
    "        file_path = baby +'/'+baby_state+'/audio/'  \n",
    "        for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "            audio_data = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "            # normalize\n",
    "            audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "            # mfcc extraction\n",
    "            file_mfcc = librosa.feature.mfcc(audio_data, sr=SAMPLING_RATE, \\\n",
    "                                            n_mfcc=N_MFCC, n_fft = L_WINDOW_N, \\\n",
    "                                            hop_length = L_HOP_N).T\n",
    "            all_file_mfcc[os.path.basename(audio_file_path)]= file_mfcc\n",
    "            all_file_mfcc_norm[os.path.basename(audio_file_path)]= mfcc_scaler.fit_transform(file_mfcc)\n",
    "\n",
    "# We transpose the result to accommodate scikit-learn which assumes that each row is one observation, and each column is one feature dimension\n",
    "\n",
    "np.save('all_file_mfcc.npy', all_file_mfcc)\n",
    "np.save('all_file_mfcc_norm.npy', all_file_mfcc_norm) \n",
    "\n",
    "# all_file_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract label for each frame in each file\n",
    "\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "babies = ['044','050','051']\n",
    "\n",
    "# Dictionary of labels\n",
    "\n",
    "all_file_frame_labels = {}\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        file_path = baby +'/'+baby_state+'/audio/'\n",
    "        for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "            audio_file = os.path.basename(audio_file_path)\n",
    "            file_labels = labels[labels['file']==audio_file]\n",
    "            # frame labels are 0 unless they're in a IN or EX segment (=1 or 2)\n",
    "            file_frame_labels = []\n",
    "            frame_positions = [10+10*x for x in range(len(all_file_mfcc[os.path.basename(audio_file_path)]))]\n",
    "            for frame_position in frame_positions:\n",
    "                segment = file_labels[(file_labels['start']<frame_position) &(file_labels['end']>frame_position)]\n",
    "                if not segment.empty:\n",
    "                    file_frame_labels.append(segment.label.values[0])\n",
    "                else:\n",
    "                    file_frame_labels.append(0)\n",
    "            all_file_frame_labels[os.path.basename(audio_file_path)]= file_frame_labels\n",
    "                    \n",
    "np.save('all_file_frame_labels.npy', all_file_frame_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exemple of a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 44100\n",
    "\n",
    "audio_file_path = '044/hungry/audio/00m12d-1(hungry)_044.wav'\n",
    "\n",
    "audio_data = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "\n",
    "IPython.display.Audio(audio_data, rate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot WaveSound\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.waveplot(audio_data, SAMPLING_RATE)\n",
    "plt.ylabel('Sound Amplitude')\n",
    "\n",
    "# Plot excel annotations\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(all_file_frame_labels[os.path.basename(audio_file_path)])\n",
    "plt.ylabel('Label')\n",
    "plt.xlabel('Annotated Frames')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM creation and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose hyperparameters C and Gamma with tenfold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-3, 3, 7)\n",
    "gamma_range = np.logspace(-3, 3, 7)\n",
    "param_grid = {'gamma' : gamma_range, 'C' : C_range}\n",
    "cv = sklearn.model_selection.KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(mfcc_dset_norm, labels_dset)\n",
    "print(\"The best parameters are \" + str(grid.best_params_) + \" with a score of \" + str(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finer tuning on basis 2 for Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_range = [1.0]\n",
    "gamma_range = [0.015625,0.03125,0.0625,0.125,0.5]\n",
    "param_grid = {'gamma' : gamma_range, 'C' : C_range}\n",
    "cv = sklearn.model_selection.KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(mfcc_dset_norm, labels_dset)\n",
    "print(\"The best parameters are \" + str(grid.best_params_) + \" with a score of \" + str(grid.best_score_))\n",
    "\n",
    "C = grid.best_params_['C']\n",
    "gamma = grid.best_params_['gamma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing C and gamma. Best C is always 1 and best gamma is often 0.0625 or 0.125. We are taking C = 1 and gamma = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "GAMMA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create svm classifer model object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_svm = sklearn.svm.SVC(C = C, gamma = GAMMA, probability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and training three GMM for three classes. We choose to use diagonale covariance matrix for GMM because it gives better results than full covariance matrix. It can be explain because full covariance type overfit for small datasets. Or, we know than for some cases we have few datas for case 'IN'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_gmm3(features_train,labels_train,n_components):\n",
    "    M0 = mixture.GaussianMixture(n_components = n_components, max_iter = 50, n_init = 20, init_params = 'kmeans', covariance_type='diag')\n",
    "    M1 = mixture.GaussianMixture(n_components = n_components, max_iter = 50, n_init = 20, init_params = 'kmeans', covariance_type='diag')\n",
    "    M2 = mixture.GaussianMixture(n_components = n_components, max_iter = 50, n_init = 20, init_params = 'kmeans', covariance_type='diag')\n",
    "    #\n",
    "    label_0 = features_train[np.where(labels_train == 0)]\n",
    "    label_1 = features_train[np.where(labels_train == 1)]\n",
    "    label_2 = features_train[np.where(labels_train == 2)]\n",
    "    #\n",
    "    M0.fit(label_0)\n",
    "    M1.fit(label_1)\n",
    "    M2.fit(label_2)\n",
    "    #\n",
    "    model = [M0,M1,M2]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels and log probability of features for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_gmm3(features_test,model):\n",
    "    M0 = model[0]\n",
    "    M1 = model[1]\n",
    "    M2 = model[2]\n",
    "    # log probability per labels\n",
    "    lprob_0 = M0.score_samples(features_test)\n",
    "    lprob_1 = M1.score_samples(features_test)\n",
    "    lprob_2 = M2.score_samples(features_test)\n",
    "    lprob = [lprob_0,lprob_1,lprob_2]\n",
    "    prob = np.exp(lprob)\n",
    "    # add exits and entries states\n",
    "    a = np.zeros((1,prob.shape[1]))\n",
    "    a[a < 1e-100] = 1e-100\n",
    "    b = prob\n",
    "    b[b < 1e-100] = 1e-100\n",
    "    lprob = np.log(concatenate((a,b,a)))\n",
    "    # predict !\n",
    "    state_seq = np.zeros(len(lprob_0))\n",
    "    state_seq [np.where((lprob_0 > lprob_1)&(lprob_0 > lprob_2))] = 0\n",
    "    state_seq [np.where((lprob_1 > lprob_0)&(lprob_1 > lprob_2))] = 1\n",
    "    state_seq [np.where((lprob_2 > lprob_0)&(lprob_2 > lprob_1))] = 2\n",
    "    return lprob, state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gmm3_score(features_test,labels_test,model):\n",
    "    state_seq = predict_gmm3(features_test,model)[1]\n",
    "    new_score = float(len(np.squeeze(np.where(state_seq == labels_test))))/float(len(labels_test))\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the outputs of SVM into a probability distribution over classes with Platt scaling extended for the multiclasse case by Wu et al. (A,B) are estimated by maximization of maximum likelihood, probabilities given by 5fold cross validation.     \n",
    "\n",
    "See \"Probability Estimates for Multi-class Classification by Pairwise Coupling\" (2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_lhood_svm(features,model):\n",
    "    predicted_proba_labels = model.predict_proba(features)\n",
    "    predicted_proba_labels = predicted_proba_labels.transpose()\n",
    "    # Add entry and exit states for viterbi\n",
    "    a = np.zeros((1,predicted_proba_labels.shape[1]))\n",
    "    a[a < 1e-100] = 1e-100\n",
    "    b = predicted_proba_labels\n",
    "    b[b < 1e-100] = 1e-100\n",
    "    # take the log-probability\n",
    "    log_lhood = np.log(concatenate((a,b,a)))\n",
    "    return log_lhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training the Transition probability Matrix for Viterbi decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_trans_matrix(states):\n",
    "    transition_prob_matrix = np.zeros((3,3))\n",
    "    for state in range(len(states)-1):\n",
    "        state1 = states[state]\n",
    "        state2 = states[state+1]\n",
    "        transition_prob_matrix[state1,state2] = transition_prob_matrix[state1,state2] + 1\n",
    "    # Normalize\n",
    "    row_sums = transition_prob_matrix.sum(axis=1)\n",
    "    row_sums = row_sums[:, np.newaxis]\n",
    "    transition_prob_matrix = transition_prob_matrix/(row_sums*np.ones((1,3))+1e-100)\n",
    "    # Add entry and exit states into transition matrix\n",
    "    transition_prob_matrix2 = np.zeros((5,5))\n",
    "    transition_prob_matrix2[1:4,1:4] = transition_prob_matrix\n",
    "    transition_prob_matrix2[0,1:4] = np.ones((1,3))/np.ones((1,3)).sum()\n",
    "    transition_prob_matrix2[transition_prob_matrix2 < 1e-100] = 1e-100\n",
    "    # return the log matrix transition\n",
    "    log_trans = np.log(transition_prob_matrix2)\n",
    "    return log_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the classifications decisions using the viterbi algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_viterbi(log_lhood, log_trans):\n",
    "    # Parameters and initialization \n",
    "    num_states, num_pts = log_lhood.shape\n",
    "    delta = np.zeros(num_states)\n",
    "    psi = np.zeros((num_pts,num_states))\n",
    "    # State 0 : 'entry state', state 1 : 'SI', state 2 : 'IN', state 3 : 'EX', state 4 :'exit state'\n",
    "    for i in range(1, num_states-1):\n",
    "        delta[i] = log_trans[0,i] + log_lhood[i,0]\n",
    "        psi[0,i] = 0\n",
    "    # Recursion\n",
    "    for t in range(1,num_pts):\n",
    "        delta_before = delta\n",
    "        for i in range(1,num_states-1):\n",
    "            temp = delta_before[1:num_states-1] + log_trans[1:num_states-1,i]\n",
    "            max_delta = np.amax(temp)\n",
    "            index = np.argmax(temp)\n",
    "            delta[i] = max_delta + log_lhood[i,t]\n",
    "            psi[t,i] = index + 1\n",
    "    # termination\n",
    "    state_seq = np.zeros(num_pts+2)\n",
    "    state_seq[num_pts+1] = num_states-1\n",
    "    temp = delta[1:num_states-1] + log_trans[1:num_states-1,num_states-1]\n",
    "    max_delta = np.amax(temp)\n",
    "    index = np.argmax(temp)\n",
    "    state_seq[num_pts] = index + 1\n",
    "    #Backtracking\n",
    "    for t in range(num_pts-1,0,-1):\n",
    "        state_seq[t] = psi[t,int(state_seq[t+1])]\n",
    "    state_seq[0] = 0\n",
    "    state_seq = state_seq[1:num_pts+1]\n",
    "    state_seq = state_seq -1\n",
    "    return state_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score after viterbi filtering : model is SVM or GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_viterbi_score(features_test,labels_test,log_trans,model):\n",
    "    log_lhood = log_lhood_svm(features_test,model)\n",
    "    state_seq = log_viterbi(log_lhood, log_trans)\n",
    "    new_score = float(len(np.squeeze(np.where(state_seq == labels_test))))/float(len(labels_test))\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm3_viterbi_score(features_test,labels_test,log_trans,model):\n",
    "    log_lhood = predict_gmm3(features_test,model)[0]\n",
    "    state_seq = log_viterbi(log_lhood, log_trans)\n",
    "    new_score = float(len(np.squeeze(np.where(state_seq == labels_test))))/float(len(labels_test))\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing for the two classifier for one context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the data base for this by taking size_dset consecutive frames after randoming files. \n",
    "\n",
    "We take only one context ('hungry') and one baby ('044')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creation data-set for one context\n",
    "\n",
    "baby_states = ['hungry']\n",
    "babies = ['044']\n",
    "\n",
    "concatenate_all_file_mfcc_norm = np.array([]).reshape(0,13)\n",
    "concatenate_all_file_mfcc = np.array([]).reshape(0,13)\n",
    "concatenate_all_file_frame_labels = []\n",
    "audio_files = []\n",
    "\n",
    "size_dset = 10000\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        file_path = baby +'/'+baby_state+'/audio/'\n",
    "        for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "            audio_file = os.path.basename(audio_file_path)\n",
    "            audio_files = audio_files + [audio_file]\n",
    "            \n",
    "# Shuffle files for keeping time relations for HMM\n",
    "\n",
    "random.shuffle(audio_files)\n",
    "            \n",
    "for audio_file in audio_files:\n",
    "    concatenate_all_file_frame_labels = concatenate_all_file_frame_labels + all_file_frame_labels[audio_file]\n",
    "    concatenate_all_file_mfcc = np.append(concatenate_all_file_mfcc,all_file_mfcc[audio_file], axis = 0)\n",
    "    concatenate_all_file_mfcc_norm = np.append(concatenate_all_file_mfcc_norm,all_file_mfcc_norm[audio_file], axis = 0)\n",
    "    \n",
    "startpoint = int(floor((len(concatenate_all_file_frame_labels)-size_dset)*random.random()))\n",
    "\n",
    "labels_dset = copy(concatenate_all_file_frame_labels[startpoint:startpoint+size_dset])\n",
    "mfcc_dset = copy(concatenate_all_file_mfcc[startpoint:startpoint+size_dset])\n",
    "mfcc_dset_norm = copy(concatenate_all_file_mfcc_norm[startpoint:startpoint+size_dset])\n",
    "\n",
    "print(shape(labels_dset),shape(mfcc_dset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the two svm model file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_svm = joblib.load('model_svm.pkl')\n",
    "\n",
    "# model_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data base in a training base (90% of the data base) and a test base (10% of the data base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate in training and data base\n",
    "\n",
    "number_train = int(round((0.90*len(labels_dset))))\n",
    "\n",
    "labels_dset_train = copy(labels_dset[:number_train])\n",
    "labels_dset_test = copy(labels_dset[number_train:])\n",
    "mfcc_dset_train = copy(mfcc_dset[:number_train])\n",
    "mfcc_dset_test = copy(mfcc_dset[number_train:])\n",
    "\n",
    "mfcc_dset_train_norm = copy(mfcc_dset_norm[:number_train])\n",
    "mfcc_dset_test_norm = copy(mfcc_dset_norm[number_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the svm classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_svm.fit(mfcc_dset_train_norm, labels_dset_train)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model_svm, 'model_svm.pkl') \n",
    "\n",
    "model_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the gmm classifier (with and without mfcc normalization) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "\n",
    "start = time.time()\n",
    "model_gmm3_norm = train_gmm3(mfcc_dset_train_norm,labels_dset_train,n_components)\n",
    "model_gmm3 = train_gmm3(mfcc_dset_train,labels_dset_train,n_components)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "model_gmm3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the log transition Matrix for Viterbi decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_trans = log_trans_matrix(labels_dset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy score of the two classifier on the train data : frame per frame and fpf + viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = {}\n",
    "\n",
    "score_svm = model_svm.score(mfcc_dset_train_norm, labels_dset_train)\n",
    "score_svm_vtb = svm_viterbi_score(mfcc_dset_train_norm,labels_dset_train,log_trans,model_svm)\n",
    "score_gmm3 = gmm3_score(mfcc_dset_train,labels_dset_train,model_gmm3)\n",
    "score_gmm3_vtb = gmm3_viterbi_score(mfcc_dset_train,labels_dset_train,log_trans,model_gmm3)\n",
    "score_gmm3_norm = gmm3_score(mfcc_dset_train_norm,labels_dset_train,model_gmm3_norm)\n",
    "score_gmm3_vtb_norm = gmm3_viterbi_score(mfcc_dset_train_norm,labels_dset_train,log_trans,model_gmm3_norm)\n",
    "\n",
    "score['score_svm']= score_svm\n",
    "score['score_svm_vtb'] = score_svm_vtb\n",
    "score['score_gmm3']= score_gmm3\n",
    "score['score_gmm3_vtb'] = score_gmm3_vtb\n",
    "score['score_gmm3_norm']= score_gmm3_norm\n",
    "score['score_gmm3_vtb_norm'] = score_gmm3_vtb_norm\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute the accuracy score of the classifier on the test data: frame per frame and fpf + viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = {}\n",
    "\n",
    "score_svm = model_svm.score(mfcc_dset_test_norm, labels_dset_test)\n",
    "score_svm_vtb = svm_viterbi_score(mfcc_dset_test_norm,labels_dset_test,log_trans,model_svm)\n",
    "score_gmm3 = gmm3_score(mfcc_dset_test,labels_dset_test,model_gmm3)\n",
    "score_gmm3_vtb = gmm3_viterbi_score(mfcc_dset_test,labels_dset_test,log_trans,model_gmm3)\n",
    "score_gmm3_norm = gmm3_score(mfcc_dset_test_norm,labels_dset_test,model_gmm3_norm)\n",
    "score_gmm3_vtb_norm = gmm3_viterbi_score(mfcc_dset_test_norm,labels_dset_test,log_trans,model_gmm3_norm)\n",
    "\n",
    "score['score_svm']= score_svm\n",
    "score['score_svm_vtb'] = score_svm_vtb\n",
    "score['score_gmm3']= score_gmm3\n",
    "score['score_gmm3_vtb'] = score_gmm3_vtb\n",
    "score['score_gmm3_norm']= score_gmm3_norm\n",
    "score['score_gmm3_vtb_norm'] = score_gmm3_vtb_norm\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ON A FILE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between audio, excel annotations, classification by frame per frame, classification by fpf + viterbi on a random files of the context '044-hungry'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 14)\n",
    "\n",
    "SAMPLING_RATE = 44100\n",
    "audio_file_path = '044/hungry/audio/00m12d-1(hungry)_044.wav'\n",
    "audio_data = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "\n",
    "# Plot WaveSound\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(8, 1, 1)\n",
    "librosa.display.waveplot(audio_data, SAMPLING_RATE)\n",
    "plt.ylabel('Sound Amplitude')\n",
    "\n",
    "# Plot excel annotations\n",
    "\n",
    "plt.subplot(8, 1, 2)\n",
    "plt.plot(all_file_frame_labels[os.path.basename(audio_file_path)])\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Annotated Frames')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (SVM)\n",
    "\n",
    "file_mfcc_norm = all_file_mfcc_norm[os.path.basename(audio_file_path)]\n",
    "predicted_labels = model_svm.predict(file_mfcc_norm)\n",
    "\n",
    "plt.subplot(8, 1, 3)\n",
    "plt.plot(predicted_labels)\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by SVM')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (SVM) + viterbi \n",
    "\n",
    "log_lhood = log_lhood_svm(file_mfcc_norm,model_svm)\n",
    "state_seq = log_viterbi(log_lhood, log_trans)\n",
    "\n",
    "plt.subplot(8, 1, 4)\n",
    "plt.plot(state_seq)\n",
    "axes = plt.gca()\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by SVM + vtb')\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (GMM)\n",
    "\n",
    "file_mfcc = all_file_mfcc[os.path.basename(audio_file_path)]\n",
    "log_lhood, predicted_labels = predict_gmm3(file_mfcc,model_gmm3)\n",
    "\n",
    "plt.subplot(8, 1, 5)\n",
    "plt.plot(predicted_labels)\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by GMM')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (GMM) + viterbi \n",
    "\n",
    "state_seq = log_viterbi(log_lhood, log_trans)\n",
    "\n",
    "plt.subplot(8, 1, 6)\n",
    "plt.plot(state_seq)\n",
    "axes = plt.gca()\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by GMM + vtb')\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot classification by frame per frame (GMM) normalized\n",
    "\n",
    "log_lhood, predicted_labels = predict_gmm3(file_mfcc_norm,model_gmm3_norm)\n",
    "\n",
    "plt.subplot(8, 1, 7)\n",
    "plt.plot(predicted_labels)\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by GMM normalized')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (GMM) + viterbi normalized\n",
    "\n",
    "state_seq = log_viterbi(log_lhood, log_trans)\n",
    "\n",
    "plt.subplot(8, 1, 8)\n",
    "plt.plot(state_seq)\n",
    "axes = plt.gca()\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by GMM + vtb normalized')\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of predict states duration and predict start with labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To change !\n",
    "\n",
    "def states_description(state_seq):\n",
    "    # Init\n",
    "    start_states = []\n",
    "    duration_states = []\n",
    "    type_states = []\n",
    "    #\n",
    "    duration_state = 0\n",
    "    start_state = 0\n",
    "    frame_before = state_seq[0]\n",
    "    for i in range(len(state_seq)):\n",
    "        if (frame_before != state_seq[i]):\n",
    "            start_states = start_states + [start_state]\n",
    "            duration_states = duration_states + [duration_state]\n",
    "            type_states = type_states + [frame_before]\n",
    "            #\n",
    "            start_state = start_state + duration_state \n",
    "            duration_state = 0\n",
    "        duration_state = duration_state + 10\n",
    "        frame_before = state_seq[i]\n",
    "    # Add the last one\n",
    "    start_states = start_states + [start_state]\n",
    "    duration_states = duration_states + [duration_state]\n",
    "    type_states = type_states + [frame_before]\n",
    "    # Creation a DataFrame\n",
    "    df_states = pd.DataFrame({'start' : start_states,\\\n",
    "                         'dur' : duration_states,\\\n",
    "                         'label' : type_states})\n",
    "    return df_states\n",
    "\n",
    "def rename_label2(label_str):\n",
    "    if label_str == 1:\n",
    "        return 'IN'\n",
    "    if label_str == 2:\n",
    "        return 'EX'\n",
    "    if label_str == 0:\n",
    "        return 'SI'\n",
    "    return label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_states = states_description(state_seq)\n",
    "df_states['label']=df_states['label'].apply(rename_label2)\n",
    "\n",
    "df_states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION AND ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ten fold cross validation : model_type = 1 for svm and svm+vtb, model_type = 2 for gmm and gmm+vtb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy_cv(features,labels,model_type,C,gamma,n_components,normalized):\n",
    "    # Initialization\n",
    "    kf = KFold(n_splits=10)\n",
    "    scores_model = np.array([])\n",
    "    scores_model_vtb = np.array([])\n",
    "    # Choice of algorithm\n",
    "    if (model_type == 1):\n",
    "        algo = 'svm'\n",
    "    else:\n",
    "        if (n_components == 5):\n",
    "            algo = 'gmm5'\n",
    "        else:\n",
    "            algo = 'gmm20'\n",
    "    # Cross_validation\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        features_train, features_test = features[train_index], features[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "        if (model_type == 1):\n",
    "            model = sklearn.svm.SVC(C = C, gamma = gamma, probability=True)\n",
    "            model.fit(features_train, labels_train)\n",
    "            score_model = model.score(features_test, labels_test)\n",
    "            log_trans = log_trans_matrix(labels_train)\n",
    "            score_model_vtb = svm_viterbi_score(features_test,labels_test,log_trans,model)\n",
    "        else:\n",
    "            model = train_gmm3(features_train,labels_train,n_components)\n",
    "            score_model = gmm3_score(features_test,labels_test,model)\n",
    "            log_trans = log_trans_matrix(labels_train)\n",
    "            score_model_vtb = gmm3_viterbi_score(features_test,labels_test,log_trans,model)\n",
    "        scores_model = np.append(scores_model,score_model)\n",
    "        scores_model_vtb = np.append(scores_model_vtb,score_model_vtb)\n",
    "    # display scores\n",
    "    if (normalized == False):\n",
    "        print('Accuracy ' + algo +  ' with ten fold cross validation :')\n",
    "    else:\n",
    "        print('Accuracy ' + algo +  ' normalized with ten fold cross validation :')\n",
    "    print(scores_model)\n",
    "    accuracy_model = scores_model.mean()\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy_model, scores_model.std() * 2))\n",
    "    print('\\n')\n",
    "    if (normalized == False):\n",
    "        print('Accuracy ' + algo + ' + viterbi with ten fold cross validation :')\n",
    "    else:\n",
    "        print('Accuracy ' + algo + ' + viterbi normalized with ten fold cross validation :')\n",
    "    print(scores_model_vtb)\n",
    "    accuracy_model_vtb = scores_model_vtb.mean()\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy_model_vtb, scores_model_vtb.std() * 2))\n",
    "    print('\\n')\n",
    "    return accuracy_model, accuracy_model_vtb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION FOR DIFFERENTS DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def creation_data_sets(size_dset,nb_baby):\n",
    "    baby_states = ['hungry','pee',\"sleepy\"]\n",
    "    if (nb_baby == 2):\n",
    "        babies = ['044','050']\n",
    "    else:\n",
    "        babies = ['044','050','051']        \n",
    "    labels_dsets = {}\n",
    "    mfcc_dsets = {}\n",
    "    mfcc_dsets_norm = {}\n",
    "    for baby in babies:\n",
    "        for baby_state in baby_states:\n",
    "            if ((baby!='051')|(baby_state!='pee')):\n",
    "                concatenate_all_file_mfcc = np.array([]).reshape(0,13)\n",
    "                concatenate_all_file_mfcc_norm = np.array([]).reshape(0,13)\n",
    "                concatenate_all_file_frame_labels = []\n",
    "                audio_files = []\n",
    "                file_path = baby +'/'+baby_state+'/audio/'\n",
    "                for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "                    audio_file = os.path.basename(audio_file_path)\n",
    "                    audio_files = audio_files + [audio_file]\n",
    "                # Shuffle files for keeping time relations for HMM   \n",
    "                random.shuffle(audio_files) \n",
    "                for audio_file in audio_files:\n",
    "                    concatenate_all_file_frame_labels = concatenate_all_file_frame_labels + all_file_frame_labels[audio_file]\n",
    "                    concatenate_all_file_mfcc = np.append(concatenate_all_file_mfcc,all_file_mfcc[audio_file], axis = 0)\n",
    "                    concatenate_all_file_mfcc_norm = np.append(concatenate_all_file_mfcc_norm,all_file_mfcc_norm[audio_file], axis = 0)\n",
    "                # Taking size_dset frames    \n",
    "                startpoint = int(floor((len(concatenate_all_file_frame_labels)-size_dset)*random.random()))\n",
    "                labels_dset = copy(concatenate_all_file_frame_labels[startpoint:startpoint+size_dset])\n",
    "                mfcc_dset = copy(concatenate_all_file_mfcc[startpoint:startpoint+size_dset])\n",
    "                mfcc_dset_norm = copy(concatenate_all_file_mfcc_norm[startpoint:startpoint+size_dset])\n",
    "                labels_dsets[baby + '-' + baby_state] = labels_dset\n",
    "                mfcc_dsets[baby + '-' + baby_state] = mfcc_dset\n",
    "                mfcc_dsets_norm[baby + '-' + baby_state] = mfcc_dset_norm\n",
    "    return labels_dsets, mfcc_dsets, mfcc_dsets_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the results1 file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 6 contexts with different algorithms with 40000 frames : \n",
    "#              SVM normalized, GMM5 normalized and not normalized, GMM10 normalized and not normalized\n",
    "#              SVM+vtb normalized, GMM5+vtb normalized and not normalized, GMM10+vtb normalized and not normalized\n",
    "\n",
    "results1 = pd.DataFrame.from_csv('results1.csv')\n",
    "\n",
    "# results1\n",
    "\n",
    "# Best results in case of algorithm with viterbi and normalization for both GMM and SVM.\n",
    "# GMM seems to work better with 20 gaussians than with only 5.\n",
    "# SVM + viterbi seems to be the best algorithm (mean accuracy of 85,83% for 6 datasets with tenfold cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "GAMMA = 0.1\n",
    "accuracies1 = {}\n",
    "babies = ['044','050']\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "size_dset = 40000\n",
    "labels_dsets, mfcc_dsets, mfcc_dsets_norm = creation_data_sets(size_dset,2)\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        print(baby + '-' + baby_state)\n",
    "        print('\\n')\n",
    "        name_dataset = baby + '-' + baby_state\n",
    "        mfcc_dset = mfcc_dsets[name_dataset]\n",
    "        mfcc_dset_norm = mfcc_dsets_norm[name_dataset]\n",
    "        labels_dset = labels_dsets[name_dataset]\n",
    "        # SVM + SVM/Vtb\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,1,C,GAMMA,5,True)\n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'svm norm'] = accuracy_model        \n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'svm+vtb norm'] = accuracy_model_vtb\n",
    "        # GMM5 + GMM5/vtb\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset,labels_dset,2,C,GAMMA,5,False)\n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'gmm5'] = accuracy_model        \n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'gmm5+vtb'] = accuracy_model_vtb\n",
    "        # GMM5 + GMM5/vtb normalized\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,2,C,GAMMA,5,True)\n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'gmm5 norm'] = accuracy_model        \n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'gmm5+vtb norm'] = accuracy_model_vtb\n",
    "        # GMM20 + GMM20/vtb\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset,labels_dset,2,C,GAMMA,20,False)\n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'gmm20'] = accuracy_model        \n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'gmm20+vtb'] = accuracy_model_vtb\n",
    "        # GMM20 + GMM20/vtb normalized\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,2,C,GAMMA,20,True)\n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'gmm20 norm'] = accuracy_model        \n",
    "        accuracies1[baby + '-' + baby_state + '-' + 'gmm20+vtb norm'] = accuracy_model_vtb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babies = ['044','050']\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "types_algo = ['svm norm','svm+vtb norm','gmm5','gmm5+vtb','gmm5 norm','gmm5+vtb norm','gmm20','gmm20+vtb','gmm20 norm','gmm20+vtb norm']\n",
    "Number = ['44', '44', '44', '50','50', '50']\n",
    "\n",
    "results1 = pd.DataFrame({'Context' : ['Hung', 'Pee', 'Sleepy', 'Hung','Pee', 'Sleepy'],\\\n",
    "                          'svm norm (%)' : np.random.randn(6),\\\n",
    "                   'svm+vtb norm (%)' : np.random.randn(6),\\\n",
    "                  'gmm5 (%)' : np.random.randn(6),\\\n",
    "                  'gmm5+vtb (%)' : np.random.randn(6),\\\n",
    "                  'gmm5 norm (%)' : np.random.randn(6),\\\n",
    "                  'gmm5+vtb norm (%)' : np.random.randn(6),\\\n",
    "                  'gmm20 (%)' : np.random.randn(6),\\\n",
    "                  'gmm20+vtb (%)' : np.random.randn(6),\\\n",
    "                  'gmm20 norm (%)' : np.random.randn(6),\\\n",
    "                  'gmm20+vtb norm (%)' : np.random.randn(6),\\\n",
    "                  'Number' : Number})\n",
    "\n",
    "results1 = results1[['Number','Context','svm norm (%)','svm+vtb norm (%)','gmm5 (%)','gmm5+vtb (%)','gmm5 norm (%)','gmm5+vtb norm (%)','gmm20 (%)','gmm20+vtb (%)','gmm20 norm (%)','gmm20+vtb norm (%)']]\n",
    "\n",
    "i = 0\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        for type_algo in types_algo:\n",
    "            if (type_algo == 'svm norm'):\n",
    "                results1.iat[i,2] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'svm+vtb norm'):\n",
    "                results1.iat[i,3] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm5'):\n",
    "                results1.iat[i,4] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm5+vtb'):\n",
    "                results1.iat[i,5] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm5 norm'):\n",
    "                results1.iat[i,6] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm5+vtb norm'):\n",
    "                results1.iat[i,7] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm20'):\n",
    "                results1.iat[i,8] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm20+vtb'):\n",
    "                results1.iat[i,9] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm20 norm'):\n",
    "                results1.iat[i,10] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm20+vtb norm'):\n",
    "                results1.iat[i,11] = accuracies1[baby + '-' + baby_state + '-' + type_algo]\n",
    "        i = i + 1        \n",
    "                \n",
    "results1.to_csv('results1.csv')\n",
    "\n",
    "results1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the results2 file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8 contexts with different algorihms with 30000 frames: \n",
    "#              SVM normalized, GMM5 normalized , GMM10 normalized\n",
    "#              SVM+vtb normalized, GMM5+vtb normalized, GMM10+vtb normalized \n",
    "\n",
    "results2 = pd.DataFrame.from_csv('results2.csv')\n",
    "\n",
    "# results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "GAMMA = 0.1\n",
    "accuracies2 = {}\n",
    "size_dset = 30000\n",
    "labels_dsets, mfcc_dsets, mfcc_dsets_norm = creation_data_sets(size_dset,3)\n",
    "babies = ['044','050',\"051\"]\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        print(baby + '-' + baby_state)\n",
    "        print('\\n')\n",
    "        if ((baby!='051')|(baby_state!='pee')):\n",
    "            name_dataset = baby + '-' + baby_state\n",
    "            mfcc_dset = mfcc_dsets[name_dataset]\n",
    "            mfcc_dset_norm = mfcc_dsets_norm[name_dataset]\n",
    "            labels_dset = labels_dsets[name_dataset]\n",
    "            # SVM + SVM/Vtb normalized\n",
    "            accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,1,C,GAMMA,5,True)\n",
    "            accuracies2[baby + '-' + baby_state + '-' + 'svm norm'] = accuracy_model        \n",
    "            accuracies2[baby + '-' + baby_state + '-' + 'svm+vtb norm'] = accuracy_model_vtb\n",
    "            # GMM5 + GMM5/vtb normalized\n",
    "            accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,2,C,GAMMA,5,True)\n",
    "            accuracies2[baby + '-' + baby_state + '-' + 'gmm5 norm'] = accuracy_model        \n",
    "            accuracies2[baby + '-' + baby_state + '-' + 'gmm5+vtb norm'] = accuracy_model_vtb\n",
    "            # GMM20 + GMM20/vtb normalized\n",
    "            accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,2,C,GAMMA,20,True)\n",
    "            accuracies2[baby + '-' + baby_state + '-' + 'gmm20 norm'] = accuracy_model        \n",
    "            accuracies2[baby + '-' + baby_state + '-' + 'gmm20+vtb norm'] = accuracy_model_vtb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "babies = ['044','050','051']\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "types_algo = ['svm norm','svm+vtb norm','gmm5 norm','gmm5+vtb norm','gmm20 norm','gmm20+vtb norm']\n",
    "\n",
    "Number = ['44', '44', '44', '50','50', '50','51','51','51']\n",
    "\n",
    "results2 = pd.DataFrame({'Context' : ['Hung', 'Pee', 'Sleepy', 'Hung','Pee', 'Sleepy','Hung','Pee', 'Sleepy'],\\\n",
    "                          'svm norm (%)' : np.random.randn(9),\\\n",
    "                   'svm+vtb norm (%)' : np.random.randn(9),\\\n",
    "                  'gmm5 norm (%)' : np.random.randn(9),\\\n",
    "                  'gmm5+vtb norm (%)' : np.random.randn(9),\\\n",
    "                  'gmm20 norm (%)' : np.random.randn(9),\\\n",
    "                  'gmm20+vtb norm (%)' : np.random.randn(9),\\\n",
    "                  'Number' : Number})\n",
    "\n",
    "results2 = results2[['Number','Context','svm norm (%)','svm+vtb norm (%)','gmm5 norm (%)','gmm5+vtb norm (%)','gmm20 norm (%)','gmm20+vtb norm (%)']]\n",
    "\n",
    "i = 0\n",
    "            \n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        if ((baby=='051')&(baby_state=='pee')):\n",
    "            results2.iat[i,2] = NaN\n",
    "            results2.iat[i,3] = NaN\n",
    "            results2.iat[i,4] = NaN\n",
    "            results2.iat[i,5] = NaN\n",
    "            results2.iat[i,6] = NaN\n",
    "            results2.iat[i,7] = NaN\n",
    "        else:    \n",
    "            for type_algo in types_algo:\n",
    "                if (type_algo == 'svm norm'):\n",
    "                    results2.iat[i,2] = accuracies2[baby + '-' + baby_state + '-' + type_algo]\n",
    "                elif (type_algo == 'svm+vtb norm'):\n",
    "                    results2.iat[i,3] = accuracies2[baby + '-' + baby_state + '-' + type_algo]\n",
    "                elif (type_algo == 'gmm5 norm'):\n",
    "                    results2.iat[i,4] = accuracies2[baby + '-' + baby_state + '-' + type_algo]\n",
    "                elif (type_algo == 'gmm5+vtb norm'):\n",
    "                    results2.iat[i,5] = accuracies2[baby + '-' + baby_state + '-' + type_algo]\n",
    "                elif (type_algo == 'gmm20 norm'):\n",
    "                    results2.iat[i,6] = accuracies2[baby + '-' + baby_state + '-' + type_algo]\n",
    "                elif (type_algo == 'gmm20+vtb norm'):\n",
    "                    results2.iat[i,7] = accuracies2[baby + '-' + baby_state + '-' + type_algo]\n",
    "        i = i + 1        \n",
    "                \n",
    "results2.to_csv('results2.csv')\n",
    "\n",
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color the best CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Color dataframe\n",
    "\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results1.columns = ['Number','Context','SVM norm','SVM+vtb norm','GMM5','GMM5+vtb','GMM5 norm','GMM5+vtb norm','GMM20','GMM20+vtb','GMM20 norm','GMM20+vtb norm']\n",
    "results2 = results2[['Number','Context','svm+vtb norm (%)','gmm5+vtb norm (%)','gmm20+vtb norm (%)']]\n",
    "results2.columns = ['Number','Context','SVM+vtb norm','GMM5+vtb norm','GMM20+vtb norm']\n",
    "\n",
    "# Color the best CV results\n",
    "\n",
    "results1_color = results1.style.apply(highlight_max, axis=1,subset=['SVM norm','SVM+vtb norm','GMM5','GMM5+vtb','GMM5 norm','GMM5+vtb norm','GMM20','GMM20+vtb','GMM20 norm','GMM20+vtb norm'])\n",
    "results2_color = results2.style.apply(highlight_max, axis=1,subset=['SVM+vtb norm','GMM5+vtb norm','GMM20+vtb norm'])\n",
    "\n",
    "# Display data frame\n",
    "\n",
    "results1_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results2_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the entire Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the chosen algorithm (svm+viterbi on the entire datasets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "babies = ['044','050','051']\n",
    "\n",
    "mfcc_dset_norm = np.array([]).reshape(0,13)\n",
    "labels_dset = []\n",
    "audio_files = []\n",
    "\n",
    "# DUPLICATE ... ? (10 doubles) \n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        file_path = baby +'/'+baby_state+'/audio/'\n",
    "        for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "            audio_file = os.path.basename(audio_file_path)\n",
    "            audio_files = audio_files + [audio_file]\n",
    "                        \n",
    "for audio_file in audio_files:\n",
    "    labels_dset = labels_dset + all_file_frame_labels[audio_file]\n",
    "    mfcc_dset_norm = np.append(mfcc_dset_norm,all_file_mfcc_norm[audio_file], axis = 0)\n",
    "    \n",
    "print(shape(labels_dset),shape(mfcc_dset_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the final svm model file here, if it's already saved. It's formed by one SVM model and one log transition matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_model = np.load('final_model.npy').item()\n",
    "\n",
    "model_svm = final_model['model_svm']\n",
    "log_trans = final_model['log_trans']\n",
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the svm and the transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "C = 1\n",
    "GAMMA = 0.1\n",
    "model_svm = sklearn.svm.SVC(C = C, gamma = GAMMA, probability=True)\n",
    "model_svm.fit(mfcc_dset_norm, labels_dset)\n",
    "log_trans = log_trans_matrix(labels_dset)\n",
    "\n",
    "# Saving\n",
    "final_model = {}\n",
    "final_model['model_svm'] = model_svm\n",
    "final_model['log_trans'] = log_trans\n",
    "np.save('final_model.npy', final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_svm_viterbi(file_mfcc_norm,model_svm,log_trans):\n",
    "    log_lhood = log_lhood_svm(file_mfcc_norm,model_svm)\n",
    "    state_seq = log_viterbi(log_lhood, log_trans)\n",
    "    return state_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparision between file, annotations et classification with the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "SAMPLING_RATE = 44100\n",
    "audio_file_path = '044/hungry/audio/00m12d-1(hungry)_044.wav'\n",
    "audio_data = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "\n",
    "# Plot WaveSound\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 1, 1)\n",
    "librosa.display.waveplot(audio_data, SAMPLING_RATE)\n",
    "plt.ylabel('Sound Amplitude')\n",
    "\n",
    "# Plot excel annotations\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(all_file_frame_labels[os.path.basename(audio_file_path)])\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Annotated Frames')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (svm)\n",
    "\n",
    "file_mfcc_norm = all_file_mfcc_norm[os.path.basename(audio_file_path)]\n",
    "predicted_labels = model_svm.predict(file_mfcc_norm)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(predicted_labels)\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by SVM')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification with final model (svm+viterbi) train on all data set.\n",
    "\n",
    "state_seq = predict_svm_viterbi(file_mfcc_norm,model_svm,log_trans)\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(state_seq)\n",
    "axes = plt.gca()\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by final_model (SVM + vtb)')\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy on the training dataset of 89.02%\n",
    "\n",
    "accuracy_final_model = svm_viterbi_score(mfcc_dset_norm,labels_dset,log_trans,model_svm)\n",
    "\n",
    "accuracy_final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving all esteemed states in .csv format for babies not annoted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we save esteemed states for not annoted babies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 44100\n",
    "L_WINDOWS = 0.020\n",
    "L_HOPS = 0.010\n",
    "L_WINDOW_N = int(round((L_WINDOWS)/(1./SAMPLING_RATE)))\n",
    "L_HOP_N = int(round((L_HOPS)/(1./SAMPLING_RATE)))\n",
    "L_WINDOW_N2 = 2**(L_WINDOW_N-1).bit_length() # POWER OF 2 FOR FFT\n",
    "L_HOP_N2 = 2**(L_WINDOW_N-1).bit_length()\n",
    "N_MFCC = 13\n",
    "\n",
    "mfcc_scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "directory = os.getcwd()\n",
    "path_files = [x[0] for x in os.walk(directory)]\n",
    "\n",
    "final_model = np.load('final_model.npy').item()\n",
    "model_svm = final_model['model_svm']\n",
    "log_trans = final_model['log_trans']\n",
    "\n",
    "for path_file in path_files:\n",
    "    if (path_file.find('audio') != -1):\n",
    "        file_path_audio = path_file+'/'\n",
    "        file_path_labels = file_path_audio[:-6]+'labels/'\n",
    "        for audio_file_path in glob.glob(file_path_audio+'*.wav'):\n",
    "            print(audio_file_path)\n",
    "            audio_data = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "            audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "            file_mfcc = librosa.feature.mfcc(audio_data, sr=SAMPLING_RATE, \\\n",
    "                                             n_mfcc=N_MFCC, n_fft = L_WINDOW_N, \\\n",
    "                                             hop_length = L_HOP_N).T\n",
    "            file_mfcc_norm = mfcc_scaler.fit_transform(file_mfcc)\n",
    "            state_seq = predict_svm_viterbi(file_mfcc_norm,model_svm,log_trans)\n",
    "            df_states = states_description(state_seq)\n",
    "            df_states['label']=df_states['label'].apply(rename_label2)\n",
    "            df_states[['start','dur']] = df_states[['start','dur']].astype(int)\n",
    "            df_states = df_states[['start','dur','label']]\n",
    "            df_states.to_csv(file_path_labels+os.path.splitext(os.path.basename(audio_file_path))[0]+'.csv',sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we save states for babies annoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame.from_csv('labels.csv')\n",
    "labels['label'] = labels['label'].apply(rename_label2)\n",
    "labels= labels[['start','dur','label','file']]\n",
    "labels[['start','dur']] = labels[['start','dur']].astype(int)\n",
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = []\n",
    "dur = []\n",
    "label = []\n",
    "file_name = []\n",
    "\n",
    "babies = ['044','050','051']\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        file_path_audio = baby +'/'+baby_state+'/audio/'\n",
    "        file_path_labels = baby +'/'+baby_state+'/labels/'\n",
    "        for audio_file_path in glob.glob(file_path_audio+'*.wav'):\n",
    "            labels_file = labels[labels.file == os.path.basename(audio_file_path)]\n",
    "            # Erase duplicate because of file with multiple context\n",
    "            lenght_label_file = len(labels_file.index)\n",
    "            # Add Initilial states\n",
    "            if (labels_file.iloc[0]['start']>0):\n",
    "                start = start + [0]\n",
    "                dur = dur + [labels_file.iloc[0]['start']]\n",
    "                label = label + ['SI']\n",
    "                file_name = file_name + [os.path.basename(audio_file_path)]\n",
    "            # Add silence between expiration and inspiration\n",
    "            for i in range(lenght_label_file-1):\n",
    "                start = start + [labels_file.iloc[i]['start']]\n",
    "                dur = dur + [labels_file.iloc[i]['dur']]\n",
    "                label = label + [labels_file.iloc[i]['label']]\n",
    "                file_name = file_name + [os.path.basename(audio_file_path)]\n",
    "                if ((labels_file.iloc[i]['start']+labels_file.iloc[i]['dur'])<labels_file.iloc[i+1]['start']):\n",
    "                    start_time = labels_file.iloc[i]['start']+labels_file.iloc[i]['dur']\n",
    "                    start = start + [start_time]\n",
    "                    dur = dur + [labels_file.iloc[i+1]['start']-start_time]\n",
    "                    label = label + ['SI']\n",
    "                    file_name = file_name + [os.path.basename(audio_file_path)]\n",
    "            start = start + [labels_file.iloc[lenght_label_file-1]['start']]\n",
    "            dur = dur + [labels_file.iloc[lenght_label_file-1]['dur']]\n",
    "            label = label + [labels_file.iloc[lenght_label_file-1]['label']]\n",
    "            file_name = file_name + [os.path.basename(audio_file_path)]\n",
    "\n",
    "label_Annotated = pd.DataFrame({'start' : start,\\\n",
    "                                'dur' : dur,\\\n",
    "                                'label' : label,\\\n",
    "                                'file' : file_name})\n",
    "\n",
    "label_Annotated= label_Annotated[['start','dur','label','file']]\n",
    "label_Annotated[['start','dur']] = label_Annotated[['start','dur']].astype(int)\n",
    "label_Annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babies = ['044','050','051']\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:  \n",
    "        file_path_audio = baby +'/'+baby_state+'/audio/'\n",
    "        file_path_labels = baby +'/'+baby_state+'/labels/'\n",
    "        for audio_file_path in glob.glob(file_path_audio+'*.wav'):\n",
    "            labels_file = label_Annotated[label_Annotated.file == os.path.basename(audio_file_path)]\n",
    "            labels_file = labels_file.drop_duplicates()\n",
    "            labels_file = labels_file[['start','dur','label']]\n",
    "            labels_file.to_csv(file_path_labels+os.path.splitext(os.path.basename(audio_file_path))[0]+'.csv',sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitch on expiration : SWIPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can install swipe easily by tapping in terminal 'pip install pysptk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SWIPE : 1500 fichiers environ en tout\n",
    "\n",
    "SAMPLING_RATE = 44100\n",
    "\n",
    "directory = os.getcwd()\n",
    "path_files = [x[0] for x in os.walk(directory)]\n",
    "\n",
    "position = 0 \n",
    "\n",
    "for path_file in path_files:\n",
    "    if (path_file.find('audio') != -1):\n",
    "        file_path_audio = path_file+'/'\n",
    "        file_path_labels = file_path_audio[:-6]+'labels/'\n",
    "        for audio_file_path in glob.glob(file_path_audio+'*.wav'):\n",
    "            print(audio_file_path, position)\n",
    "            start = []\n",
    "            dur = []\n",
    "            label = []\n",
    "            pitch_Swipe = []\n",
    "            # normalise file\n",
    "            audio = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "            audio = (audio - np.mean(audio))/ max(abs(audio-np.mean(audio)))\n",
    "            f0 = pysptk.swipe(audio.astype(np.float64), fs=SAMPLING_RATE, hopsize=80, min=150, max=550, otype=\"f0\")\n",
    "            #\n",
    "            df_states = pd.DataFrame.from_csv(file_path_labels+os.path.splitext(os.path.basename(audio_file_path))[0]+'.csv',sep='\\t',index_col=None)\n",
    "            for i in range(len(df_states)):\n",
    "                start = start + [df_states.iloc[i]['start']]\n",
    "                dur = dur + [df_states.iloc[i]['dur']]\n",
    "                label = label + [df_states.iloc[i]['label']]\n",
    "                if (df_states.iloc[i]['label']=='EX'):\n",
    "                    start_N = int(round((float(df_states.iloc[i]['start'])/1000)/(1./SAMPLING_RATE)))\n",
    "                    start_swipe = int(round(float(start_N)/80))\n",
    "                    end_N = int(round((float(df_states.iloc[i]['start']+df_states.iloc[i]['dur'])/1000)/(1./SAMPLING_RATE)))\n",
    "                    end_swipe = int(round(float(end_N)/80))\n",
    "                    f0_intervalle = f0[start_swipe:end_swipe+1]\n",
    "                    f0_intervalle = f0_intervalle[f0_intervalle>150]\n",
    "                    if (f0_intervalle.size == 0):\n",
    "                        pitch_Swipe = pitch_Swipe + [-1]\n",
    "                    else:\n",
    "                        value_pitch = mean(f0_intervalle)\n",
    "                        pitch_Swipe = pitch_Swipe + [value_pitch]\n",
    "                else:\n",
    "                    pitch_Swipe = pitch_Swipe + [nan]\n",
    "            df_states_and_pitch = pd.DataFrame({'start' : start,'dur' : dur,'label' : label,'pitch_Swipe' : pitch_Swipe})\n",
    "            df_states_and_pitch = df_states_and_pitch[['start','dur','label','pitch_Swipe']]\n",
    "            df_states_and_pitch.to_csv(file_path_labels+os.path.splitext(os.path.basename(audio_file_path))[0]+'+pitch_Swipe'+'.csv',sep='\\t', index=False)\n",
    "            position = position + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of audio features on expiration phases with Praat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 44100\n",
    "\n",
    "directory = os.getcwd()\n",
    "path_files = [x[0] for x in os.walk(directory)]\n",
    "\n",
    "position = 0 \n",
    "\n",
    "for path_file in path_files:\n",
    "    if (path_file.find('audio') != -1):\n",
    "        file_path_audio = path_file+'/'\n",
    "        file_path_labels = file_path_audio[:-6]+'labels/'\n",
    "        for audio_file_path in glob.glob(file_path_audio+'*.wav'):\n",
    "            print(audio_file_path, position)\n",
    "            start = []\n",
    "            dur = []\n",
    "            label = []\n",
    "            pitch_Swipe = []\n",
    "            pitch_Praat = []\n",
    "            sdPitch_Praat = []\n",
    "            minPitch_Praat = []\n",
    "            maxPitch_Praat = []\n",
    "            jitter_loc = []\n",
    "            jitter_loc_abs = []\n",
    "            jitter_rap = []\n",
    "            jitter_ppq5 = []\n",
    "            vshimmer_loc = []\n",
    "            shimmer_loc_dB = []\n",
    "            shimmer_apq3 = []\n",
    "            shimmer_apq5 = []\n",
    "            shimmer_apq11 = []\n",
    "            nhr = []\n",
    "            hnr = []\n",
    "            # normalise file\n",
    "            audio = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "            audio = (audio - np.mean(audio))/ max(abs(audio-np.mean(audio)))\n",
    "            scipy.io.wavfile.write('audio_norm.wav', SAMPLING_RATE, audio)\n",
    "            #\n",
    "            df_states = pd.DataFrame.from_csv(file_path_labels+os.path.splitext(os.path.basename(audio_file_path))[0]+'+pitch_Swipe'+'.csv',sep='\\t',index_col=None)\n",
    "            for i in range(len(df_states)):\n",
    "                start = start + [df_states.iloc[i]['start']]\n",
    "                dur = dur+ [df_states.iloc[i]['dur']]\n",
    "                label = label + [df_states.iloc[i]['label']]\n",
    "                pitch_Swipe = pitch_Swipe + [df_states.iloc[i]['pitch_Swipe']]  \n",
    "                if (df_states.iloc[i]['label']=='EX'):\n",
    "                    if (df_states.iloc[i]['dur'] > 30):\n",
    "                        if (os.path.isfile('Praat_features.csv') == True):\n",
    "                            os.remove('Praat_features.csv')\n",
    "                        out = subprocess.check_output(['/Users/vaglio/Desktop/BABY_STAGE/Praat.app/Contents/MacOS/Praat', \"--run\",'extract_features.psc', 'audio_norm.wav',str(float(df_states.iloc[i]['start'])/1000),str(float(df_states.iloc[i]['start'])/1000+float(df_states.iloc[i]['dur'])/1000), 'Praat_features.csv']);\n",
    "                        Praat_features = pd.read_csv('Praat_features.csv',sep='\\t',index_col=None,header = None)\n",
    "                        Praat_features.columns = ['pitch_Praat','sdPitch_Praat','minPitch_Praat','maxPitch_Praat','jitter_loc','jitter_loc_abs','jitter_rap','jitter_ppq5','vshimmer_loc','shimmer_loc_dB','shimmer_apq3','shimmer_apq5','shimmer_apq11','nhr','hnr']\n",
    "                        pitch_Praat = pitch_Praat + [Praat_features.iloc[0]['pitch_Praat']]\n",
    "                        sdPitch_Praat = sdPitch_Praat + [Praat_features.iloc[0]['sdPitch_Praat']]\n",
    "                        minPitch_Praat = minPitch_Praat + [Praat_features.iloc[0]['minPitch_Praat']]\n",
    "                        maxPitch_Praat = maxPitch_Praat + [Praat_features.iloc[0]['maxPitch_Praat']]\n",
    "                        jitter_loc = jitter_loc + [Praat_features.iloc[0]['jitter_loc']]\n",
    "                        jitter_loc_abs = jitter_loc_abs + [Praat_features.iloc[0]['jitter_loc_abs']]\n",
    "                        jitter_rap = jitter_rap + [Praat_features.iloc[0]['jitter_rap']]\n",
    "                        jitter_ppq5 = jitter_ppq5 + [Praat_features.iloc[0]['jitter_ppq5']]\n",
    "                        vshimmer_loc = vshimmer_loc + [Praat_features.iloc[0]['vshimmer_loc']]\n",
    "                        shimmer_loc_dB = shimmer_loc_dB + [Praat_features.iloc[0]['shimmer_loc_dB']]\n",
    "                        shimmer_apq3 = shimmer_apq3 + [Praat_features.iloc[0]['shimmer_apq3']]\n",
    "                        shimmer_apq5 = shimmer_apq5 + [Praat_features.iloc[0]['shimmer_apq5']]\n",
    "                        shimmer_apq11 = shimmer_apq11 + [Praat_features.iloc[0]['shimmer_apq11']]\n",
    "                        nhr = nhr + [Praat_features.iloc[0]['nhr']]\n",
    "                        hnr = hnr + [Praat_features.iloc[0]['hnr']]\n",
    "                    else:\n",
    "                        pitch_Praat = pitch_Praat + [-1]\n",
    "                        sdPitch_Praat = sdPitch_Praat + [-1]\n",
    "                        minPitch_Praat = minPitch_Praat + [-1]\n",
    "                        maxPitch_Praat = maxPitch_Praat + [-1]\n",
    "                        jitter_loc = jitter_loc + [-1]\n",
    "                        jitter_loc_abs = jitter_loc_abs + [-1]\n",
    "                        jitter_rap = jitter_rap + [-1]\n",
    "                        jitter_ppq5 = jitter_ppq5 + [-1]\n",
    "                        vshimmer_loc = vshimmer_loc + [-1]\n",
    "                        shimmer_loc_dB = shimmer_loc_dB + [-1]\n",
    "                        shimmer_apq3 = shimmer_apq3 + [-1]\n",
    "                        shimmer_apq5 = shimmer_apq5 + [-1]\n",
    "                        shimmer_apq11 = shimmer_apq11 + [-1]\n",
    "                        nhr = nhr + [-1]\n",
    "                        hnr = hnr + [-1]\n",
    "                else:\n",
    "                    pitch_Praat = pitch_Praat + [nan]\n",
    "                    sdPitch_Praat = sdPitch_Praat + [nan]\n",
    "                    minPitch_Praat = minPitch_Praat + [nan]\n",
    "                    maxPitch_Praat = maxPitch_Praat + [nan]\n",
    "                    jitter_loc = jitter_loc + [nan]\n",
    "                    jitter_loc_abs = jitter_loc_abs + [nan]\n",
    "                    jitter_rap = jitter_rap + [nan]\n",
    "                    jitter_ppq5 = jitter_ppq5 + [nan]\n",
    "                    vshimmer_loc = vshimmer_loc + [nan]\n",
    "                    shimmer_loc_dB = shimmer_loc_dB + [nan]\n",
    "                    shimmer_apq3 = shimmer_apq3 + [nan]\n",
    "                    shimmer_apq5 = shimmer_apq5 + [nan]\n",
    "                    shimmer_apq11 = shimmer_apq11 + [nan]\n",
    "                    nhr = nhr + [nan]\n",
    "                    hnr = hnr + [nan]\n",
    "            os.remove('audio_norm.wav')\n",
    "            os.remove('Praat_features.csv')\n",
    "            df_states_and_pitch = pd.DataFrame({'start' : start,'dur' : dur,'label' : label,'pitch_Swipe' : pitch_Swipe,'pitch_Praat' : pitch_Praat,'sdPitch_Praat' : sdPitch_Praat,'minPitch_Praat' : minPitch_Praat,'maxPitch_Praat' : maxPitch_Praat,'jitter_loc' : jitter_loc,'jitter_loc_abs' : jitter_loc_abs,'jitter_rap' : jitter_rap,'jitter_ppq5' : jitter_ppq5,'vshimmer_loc' : vshimmer_loc,'shimmer_loc_dB' : shimmer_loc_dB,'shimmer_apq3' : shimmer_apq3,'shimmer_apq5' : shimmer_apq5,'shimmer_apq11' : shimmer_apq11,'nhr' : nhr,'hnr' : hnr})\n",
    "            df_states_and_pitch = df_states_and_pitch[['start','dur','label','pitch_Swipe','pitch_Praat','sdPitch_Praat','minPitch_Praat','maxPitch_Praat','jitter_loc','jitter_loc_abs','jitter_rap','jitter_ppq5','vshimmer_loc','shimmer_loc_dB','shimmer_apq3','shimmer_apq5','shimmer_apq11','nhr','hnr']]\n",
    "            df_states_and_pitch.to_csv(file_path_labels+os.path.splitext(os.path.basename(audio_file_path))[0]+'+pitch_Swipe+Praat'+'.csv',sep='\\t', index=False)\n",
    "            position = position + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced design with missing cells and differents sizes for each cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function because when import .csv, python understand the baby_ID string as a Int\n",
    "\n",
    "def correct_baby_id(baby_id):\n",
    "        return '0' + str(baby_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import if already saved\n",
    "\n",
    "all_babies_features = pd.DataFrame.from_csv('all_babies_features.csv',sep='\\t',index_col=None)\n",
    "\n",
    "all_babies_features['baby_id']=all_babies_features['baby_id'].apply(correct_baby_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_column(dataframe,name_column):\n",
    "    dataframe_copy = dataframe\n",
    "    column_select = dataframe_copy[name_column].tolist()\n",
    "    column_select_pos = [i for i in column_select if  np.isnan(i) == False and i != -1]\n",
    "    if column_select_pos:\n",
    "        mean_column = mean(column_select_pos)\n",
    "    else:\n",
    "        mean_column = nan\n",
    "    return mean_column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mean of audio features for each baby\n",
    "\n",
    "directory = os.getcwd()\n",
    "path_files = [x[0] for x in os.walk(directory)]\n",
    "\n",
    "baby_sex_data = pd.DataFrame.from_csv('baby_sex_data.csv',sep='\\t',index_col=None, header = None)\n",
    "baby_sex_data.columns = ['baby_id','sex']\n",
    "baby_sex_data['baby_id']=baby_sex_data['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "babies_id = []\n",
    "sex = []\n",
    "months = []\n",
    "contexts = []\n",
    "mean_EXs = []\n",
    "mean_INs = [] \n",
    "mean_pitchs_Swipe = []\n",
    "mean_pitchs_Praat = []\n",
    "mean_sdPitchs_Praat = []\n",
    "mean_minPitchs_Praat = [] \n",
    "mean_maxPitchs_Praat = [] \n",
    "mean_jitters_loc = []\n",
    "mean_jitters_loc_abs = []\n",
    "mean_jitters_rap = []\n",
    "mean_jitters_ppq5 = []\n",
    "mean_vshimmers_loc = []\n",
    "mean_shimmers_loc_dB = []\n",
    "mean_shimmers_apq3 = []\n",
    "mean_shimmers_apq5 = []\n",
    "mean_shimmers_apq11 = []\n",
    "mean_nhrs = []\n",
    "mean_hnrs = []\n",
    "\n",
    "for path_file in path_files:\n",
    "    if (path_file.find('labels') != -1):\n",
    "        files_path_labels = path_file+'/'\n",
    "        for file_path_labels in glob.glob(files_path_labels+'*.csv'):\n",
    "            if (file_path_labels.find('pitch_Swipe+Praat') != -1):\n",
    "                print(file_path_labels)\n",
    "                month = os.path.split(file_path_labels)[1][:3]\n",
    "                context = os.path.split(os.path.split(os.path.split(file_path_labels)[0])[0])[1]\n",
    "                baby_id = os.path.split(os.path.split(os.path.split(os.path.split(file_path_labels)[0])[0])[0])[1]\n",
    "                sex_baby = baby_sex_data[baby_sex_data.baby_id == baby_id].sex.values[0]\n",
    "                babies_id = babies_id + [baby_id]\n",
    "                months = months + [month]\n",
    "                contexts = contexts + [context]\n",
    "                sex = sex + [sex_baby]\n",
    "                file_labels = pd.DataFrame.from_csv(file_path_labels,sep='\\t',index_col=None)\n",
    "                #\n",
    "                label_INs = file_labels[file_labels.label == 'IN']\n",
    "                if label_INs.empty:\n",
    "                    mean_INs = mean_INs + [nan]\n",
    "                else:\n",
    "                    mean_INs = mean_INs + [int(mean_column(label_INs,'dur'))]\n",
    "                #\n",
    "                label_EXs = file_labels[file_labels.label == 'EX']\n",
    "                mean_EXs = mean_EXs + [int(mean_column(label_EXs,'dur'))]\n",
    "                mean_pitchs_Swipe = mean_pitchs_Swipe + [mean_column(label_EXs,'pitch_Swipe')]\n",
    "                mean_pitchs_Praat = mean_pitchs_Praat + [mean_column(label_EXs,'pitch_Praat')]\n",
    "                mean_sdPitchs_Praat = mean_sdPitchs_Praat + [mean_column(label_EXs,'sdPitch_Praat')]\n",
    "                mean_minPitchs_Praat = mean_minPitchs_Praat + [mean_column(label_EXs,'minPitch_Praat')]\n",
    "                mean_maxPitchs_Praat = mean_maxPitchs_Praat + [mean_column(label_EXs,'maxPitch_Praat')]\n",
    "                mean_jitters_loc = mean_jitters_loc + [mean_column(label_EXs,'jitter_loc')]\n",
    "                mean_jitters_loc_abs = mean_jitters_loc_abs + [mean_column(label_EXs,'jitter_loc_abs')]\n",
    "                mean_jitters_rap = mean_jitters_rap + [mean_column(label_EXs,'jitter_rap')]\n",
    "                mean_jitters_ppq5 = mean_jitters_ppq5 + [mean_column(label_EXs,'jitter_ppq5')]\n",
    "                mean_vshimmers_loc = mean_vshimmers_loc + [mean_column(label_EXs,'vshimmer_loc')]\n",
    "                mean_shimmers_loc_dB = mean_shimmers_loc_dB + [mean_column(label_EXs,'shimmer_loc_dB')]\n",
    "                mean_shimmers_apq3 = mean_shimmers_apq3 + [mean_column(label_EXs,'shimmer_apq3')]\n",
    "                mean_shimmers_apq5 = mean_shimmers_apq5 + [mean_column(label_EXs,'shimmer_apq5')]\n",
    "                mean_shimmers_apq11 = mean_shimmers_apq11 + [mean_column(label_EXs,'shimmer_apq11')]\n",
    "                mean_nhrs = mean_nhrs + [mean_column(label_EXs,'nhr')]\n",
    "                mean_hnrs = mean_hnrs + [mean_column(label_EXs,'hnr')]            \n",
    "\n",
    "all_babies_features = pd.DataFrame({'baby_id' : babies_id,\\\n",
    "                    'sex' :sex,\\\n",
    "                    'month' :months,\\\n",
    "                    'context' : contexts,\\\n",
    "                    'mean_EX' : mean_EXs,\\\n",
    "                    'mean_IN' : mean_INs,\\\n",
    "                    'mean_pitch_Swipe' : mean_pitchs_Swipe,\\\n",
    "                    'mean_pitch_Praat' : mean_pitchs_Praat,\\\n",
    "                    'mean_sdPitch_Praat' : mean_sdPitchs_Praat,\\\n",
    "                    'mean_minPitch_Praat' : mean_minPitchs_Praat,\\\n",
    "                    'mean_maxPitch_Praat' : mean_maxPitchs_Praat,\\\n",
    "                    'mean_jitter_loc' : mean_jitters_loc,\\\n",
    "                    'mean_jitter_loc_abs' : mean_jitters_loc_abs,\\\n",
    "                    'mean_jitter_rap' : mean_jitters_rap,\\\n",
    "                    'mean_jitter_ppq5' : mean_jitters_ppq5,\\\n",
    "                    'mean_vshimmer_loc' : mean_vshimmers_loc,\\\n",
    "                    'mean_shimmer_loc_dB' : mean_shimmers_loc_dB,\\\n",
    "                    'mean_shimmer_apq3' : mean_shimmers_apq3,\\\n",
    "                    'mean_shimmer_apq5' : mean_shimmers_apq5,\\\n",
    "                    'mean_shimmer_apq11' : mean_shimmers_apq11,\\\n",
    "                    'mean_nhr' : mean_nhrs,\\\n",
    "                    'mean_hnr' : mean_hnrs\n",
    "                     })\n",
    "\n",
    "all_babies_features=all_babies_features[['baby_id','sex','month','context','mean_EX','mean_IN','mean_pitch_Swipe','mean_pitch_Praat','mean_sdPitch_Praat','mean_minPitch_Praat','mean_maxPitch_Praat','mean_jitter_loc','mean_jitter_loc_abs','mean_jitter_rap','mean_jitter_ppq5','mean_vshimmer_loc','mean_shimmer_loc_dB','mean_shimmer_apq3','mean_shimmer_apq5','mean_shimmer_apq11','mean_nhr','mean_hnr']]\n",
    "\n",
    "all_babies_features.to_csv('all_babies_features.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selection of a design of experiments for anova\n",
    "\n",
    "babies_selected = ['011','042','044','050','051','054','058','061','063','064','066','067','068']\n",
    "contexts_selected = ['hungry','pee','sleepy']\n",
    "months_selected = ['00m','01m','02m','03m']\n",
    "\n",
    "anova_design = all_babies_features[all_babies_features.baby_id.isin(babies_selected)]\n",
    "anova_design = anova_design[anova_design.month.isin(months_selected)]\n",
    "anova_design = anova_design[anova_design.context.isin(contexts_selected)]\n",
    "\n",
    "anova_design=anova_design[['baby_id','sex','month','context','mean_EX','mean_IN','mean_pitch_Swipe','mean_pitch_Praat','mean_sdPitch_Praat','mean_minPitch_Praat','mean_maxPitch_Praat','mean_jitter_loc','mean_jitter_loc_abs','mean_jitter_rap','mean_jitter_ppq5','mean_vshimmer_loc','mean_shimmer_loc_dB','mean_shimmer_apq3','mean_shimmer_apq5','mean_shimmer_apq11','mean_nhr','mean_hnr']]\n",
    "\n",
    "anova_design.to_csv('anova_design.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANOVA TEST\n",
    "\n",
    "anova_design = pd.DataFrame.from_csv('anova_design.csv',sep='\\t',index_col=None)\n",
    "anova_design['baby_id']=anova_design['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "anova_design=anova_design[['baby_id','month','context','mean_EX']]\n",
    "\n",
    "# Transform the anova dataFrame to a R-object\n",
    "\n",
    "rdf_anova=pandas2ri.py2ri(anova_design)\n",
    "rdf_anova[rdf_anova.names.index('month')] = R.ordered(rdf_anova.rx2('month'))\n",
    "\n",
    "R.str(rdf_anova)\n",
    "#print(R.head(rdf))\n",
    "#print(R.tail(anova))\n",
    "\n",
    "# mean_EX is modelised in fonction of factors month and context, baby_id identify each independante observations, month and context are intra-sujects variable\n",
    "fml = Formula(\"mean_EX~(month*context)+Error(baby_id/(month*context))\")\n",
    "result = R.aov(fml,data=rdf_anova)\n",
    "print(R.summary(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Design non balancé \n",
    "\n",
    "print(R.summary(rdf_anova))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced design with missing cells but same sizes for each cells (mean of each cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babies_selected = ['011','042','044','050','051','054','058','061','063','064','066','067','068']\n",
    "contexts_selected = ['hungry','pee','sleepy']\n",
    "months_selected = ['00m','01m','02m','03m']\n",
    "                \n",
    "# Mean of each cells\n",
    "\n",
    "anova_design = pd.DataFrame.from_csv('anova_design.csv',sep='\\t',index_col=None)\n",
    "anova_design['baby_id']=anova_design['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "baby_sex_data = pd.DataFrame.from_csv('baby_sex_data.csv',sep='\\t',index_col=None, header = None)\n",
    "baby_sex_data.columns = ['baby_id','sex']\n",
    "baby_sex_data['baby_id']=baby_sex_data['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "babies_id = []\n",
    "sex = []\n",
    "months = []\n",
    "contexts = []\n",
    "mean_INs = []\n",
    "mean_EXs = []\n",
    "mean_pitchs_Swipe = []\n",
    "mean_pitchs_Praat = []\n",
    "mean_sdPitchs_Praat = []\n",
    "mean_minPitchs_Praat = [] \n",
    "mean_maxPitchs_Praat = [] \n",
    "mean_jitters_loc = []\n",
    "mean_jitters_loc_abs = []\n",
    "mean_jitters_rap = []\n",
    "mean_jitters_ppq5 = []\n",
    "mean_vshimmers_loc = []\n",
    "mean_shimmers_loc_dB = []\n",
    "mean_shimmers_apq3 = []\n",
    "mean_shimmers_apq5 = []\n",
    "mean_shimmers_apq11 = []\n",
    "mean_nhrs = []\n",
    "mean_hnrs = []\n",
    "\n",
    "for babie_selected in babies_selected:\n",
    "    for context_selected in contexts_selected:\n",
    "        for month_selected in months_selected:\n",
    "            anova_cell = anova_design[anova_design.baby_id == babie_selected]\n",
    "            anova_cell = anova_cell[anova_cell.context == context_selected]\n",
    "            anova_cell = anova_cell[anova_cell.month == month_selected]\n",
    "            #\n",
    "            babies_id = babies_id +  [babie_selected]\n",
    "            sex = sex + [baby_sex_data[baby_sex_data.baby_id == babie_selected].sex.values[0]]\n",
    "            contexts = contexts + [context_selected]\n",
    "            months = months + [month_selected]\n",
    "            if not anova_cell.empty:\n",
    "                mean_INs = mean_INs + [mean_column(anova_cell,'mean_IN')]\n",
    "                mean_EXs = mean_EXs + [int(anova_cell.mean_EX.mean(axis=0))]\n",
    "                mean_pitchs_Swipe = mean_pitchs_Swipe + [anova_cell.mean_pitch_Swipe.mean(axis=0)]\n",
    "                mean_pitchs_Praat = mean_pitchs_Praat + [anova_cell.mean_pitch_Praat.mean(axis=0)]\n",
    "                mean_sdPitchs_Praat = mean_sdPitchs_Praat + [anova_cell.mean_sdPitch_Praat.mean(axis=0)]\n",
    "                mean_minPitchs_Praat = mean_minPitchs_Praat + [anova_cell.mean_minPitch_Praat.mean(axis=0)]\n",
    "                mean_maxPitchs_Praat = mean_maxPitchs_Praat + [anova_cell.mean_maxPitch_Praat.mean(axis=0)]\n",
    "                mean_jitters_loc = mean_jitters_loc + [anova_cell.mean_jitter_loc.mean(axis=0)]\n",
    "                mean_jitters_loc_abs = mean_jitters_loc_abs + [anova_cell.mean_jitter_loc_abs.mean(axis=0)]\n",
    "                mean_jitters_rap = mean_jitters_rap + [anova_cell.mean_jitter_rap.mean(axis=0)]\n",
    "                mean_jitters_ppq5 = mean_jitters_ppq5 + [anova_cell.mean_jitter_ppq5.mean(axis=0)]\n",
    "                mean_vshimmers_loc = mean_vshimmers_loc + [anova_cell.mean_vshimmer_loc.mean(axis=0)]\n",
    "                mean_shimmers_loc_dB = mean_shimmers_loc_dB + [anova_cell.mean_shimmer_loc_dB.mean(axis=0)]\n",
    "                mean_shimmers_apq3 = mean_shimmers_apq3 + [anova_cell.mean_shimmer_apq3.mean(axis=0)]\n",
    "                mean_shimmers_apq5 = mean_shimmers_apq5 + [anova_cell.mean_shimmer_apq5.mean(axis=0)]\n",
    "                mean_shimmers_apq11 = mean_shimmers_apq11 + [anova_cell.mean_shimmer_apq11.mean(axis=0)]\n",
    "                mean_nhrs = mean_nhrs + [anova_cell.mean_nhr.mean(axis=0)]\n",
    "                mean_hnrs = mean_hnrs + [anova_cell.mean_hnr.mean(axis=0)]\n",
    "            else:\n",
    "                mean_INs = mean_INs + [nan]\n",
    "                mean_EXs = mean_EXs + [nan]\n",
    "                mean_pitchs_Swipe = mean_pitchs_Swipe + [nan]\n",
    "                mean_pitchs_Praat = mean_pitchs_Praat + [nan]\n",
    "                mean_sdPitchs_Praat = mean_sdPitchs_Praat + [nan]\n",
    "                mean_minPitchs_Praat = mean_minPitchs_Praat + [nan]\n",
    "                mean_maxPitchs_Praat = mean_maxPitchs_Praat + [nan]\n",
    "                mean_jitters_loc = mean_jitters_loc + [nan]\n",
    "                mean_jitters_loc_abs = mean_jitters_loc_abs + [nan]\n",
    "                mean_jitters_rap = mean_jitters_rap + [nan]\n",
    "                mean_jitters_ppq5 = mean_jitters_ppq5 + [nan]\n",
    "                mean_vshimmers_loc = mean_vshimmers_loc + [nan]\n",
    "                mean_shimmers_loc_dB = mean_shimmers_loc_dB + [nan]\n",
    "                mean_shimmers_apq3 = mean_shimmers_apq3 + [nan]\n",
    "                mean_shimmers_apq5 = mean_shimmers_apq5 + [nan]\n",
    "                mean_shimmers_apq11 = mean_shimmers_apq11 + [nan]\n",
    "                mean_nhrs = mean_nhrs + [nan]\n",
    "                mean_hnrs = mean_hnrs + [nan]\n",
    "            \n",
    "anova_mean = pd.DataFrame({'baby_id' : babies_id,\\\n",
    "                           'sex' : sex,\\\n",
    "                           'month' :months,\\\n",
    "                           'context' : contexts,\\\n",
    "                           'mean_EX' : mean_EXs,\\\n",
    "                           'mean_IN' : mean_INs,\\\n",
    "                           'mean_pitch_Swipe' : mean_pitchs_Swipe,\\\n",
    "                           'mean_pitch_Praat' : mean_pitchs_Praat,\\\n",
    "                           'mean_sdPitch_Praat' : mean_sdPitchs_Praat,\\\n",
    "                           'mean_minPitch_Praat' : mean_minPitchs_Praat,\\\n",
    "                           'mean_maxPitch_Praat' : mean_maxPitchs_Praat,\\\n",
    "                           'mean_jitter_loc' : mean_jitters_loc,\\\n",
    "                           'mean_jitter_loc_abs' : mean_jitters_loc_abs,\\\n",
    "                           'mean_jitter_rap' : mean_jitters_rap,\\\n",
    "                           'mean_jitter_ppq5' : mean_jitters_ppq5,\\\n",
    "                           'mean_vshimmer_loc' : mean_vshimmers_loc,\\\n",
    "                           'mean_shimmer_loc_dB' : mean_shimmers_loc_dB,\\\n",
    "                           'mean_shimmer_apq3' : mean_shimmers_apq3,\\\n",
    "                           'mean_shimmer_apq5' : mean_shimmers_apq5,\\\n",
    "                           'mean_shimmer_apq11' : mean_shimmers_apq11,\\\n",
    "                           'mean_nhr' : mean_nhrs,\\\n",
    "                           'mean_hnr' : mean_hnrs\n",
    "                            })\n",
    "\n",
    "anova_mean=anova_mean[['baby_id','sex','month','context','mean_EX','mean_IN','mean_pitch_Swipe','mean_pitch_Praat','mean_sdPitch_Praat','mean_minPitch_Praat','mean_maxPitch_Praat','mean_jitter_loc','mean_jitter_loc_abs','mean_jitter_rap','mean_jitter_ppq5','mean_vshimmer_loc','mean_shimmer_loc_dB','mean_shimmer_apq3','mean_shimmer_apq5','mean_shimmer_apq11','mean_nhr','mean_hnr']]\n",
    "anova_mean.to_csv('anova_mean.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import if already saved\n",
    "\n",
    "anova_mean = pd.DataFrame.from_csv('anova_mean.csv',sep='\\t',index_col=None)\n",
    "anova_mean['baby_id']=anova_mean['baby_id'].apply(correct_baby_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANOVA on the last design \n",
    "\n",
    "rdf_mean = pandas2ri.py2ri(anova_mean)\n",
    "rdf_mean[rdf_mean.names.index('month')] = R.ordered(rdf_mean.rx2('month'))\n",
    "\n",
    "print(R.str(rdf_mean))\n",
    "\n",
    "fml = Formula(\"mean_EX~(month*context)+Error(baby_id/(month*context))\")\n",
    "result = R.aov(fml,data=rdf_mean)\n",
    "R.summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced design (mean of each cells) with no missing cells (new cells create with interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def anova_interpolation(anova_dataframe,i,index,feature):\n",
    "    if (np.isnan(anova_dataframe.iloc[index[i]][feature])):\n",
    "        if(i == 0):\n",
    "            j = 1\n",
    "            while(np.isnan(anova_dataframe.iloc[index[j]][feature])):\n",
    "                j = j + 1\n",
    "            anova_dataframe.at[index[i],feature] = anova_dataframe.iloc[index[j]][feature]\n",
    "        elif((i == 1)|(i == 2)):\n",
    "            if(np.isnan(anova_dataframe.iloc[index[i+1]][feature])):\n",
    "                anova_dataframe.at[index[i],feature] = anova_dataframe.iloc[index[i-1]][feature]\n",
    "            else:\n",
    "                anova_dataframe.at[index[i],feature] = (anova_dataframe.iloc[index[i-1]][feature]+anova_dataframe.iloc[index[i+1]][feature])/2\n",
    "        elif(i == 3):\n",
    "            j = 2\n",
    "            while(np.isnan(anova_dataframe.iloc[index[j]][feature])):\n",
    "                j = j - 1\n",
    "            anova_dataframe.at[index[i],feature] = anova_dataframe.iloc[index[j]][feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "babies_selected = ['011','042','044','050','051','054','058','061','063','064','066','067','068']\n",
    "contexts_selected = ['hungry','pee','sleepy']\n",
    "months_selected = ['00m','01m','02m','03m']\n",
    "\n",
    "# Interpolation of missing datas to create un balanced design\n",
    "\n",
    "anova_mean = pd.DataFrame.from_csv('anova_mean.csv',sep='\\t',index_col=None)\n",
    "anova_mean['baby_id']=anova_mean['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "anova_balanced = anova_mean.copy()\n",
    "\n",
    "for babie_selected in babies_selected:\n",
    "    for context_selected in contexts_selected:\n",
    "        temp = anova_balanced.loc[(anova_balanced[\"baby_id\"] == babie_selected) & (anova_balanced[\"context\"] == context_selected)]\n",
    "        index = temp.index\n",
    "        for i in range(len(months_selected)):\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_EX')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_IN')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_pitch_Swipe')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_pitch_Praat')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_sdPitch_Praat')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_minPitch_Praat')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_maxPitch_Praat')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_jitter_loc')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_jitter_loc_abs')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_jitter_rap')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_jitter_ppq5')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_vshimmer_loc')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_shimmer_loc_dB')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_shimmer_apq3')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_shimmer_apq5')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_shimmer_apq11')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_nhr')\n",
    "            anova_interpolation(anova_balanced,i,index,'mean_hnr')\n",
    "\n",
    "anova_balanced[['mean_EX']] = anova_balanced[['mean_EX']].astype(int)\n",
    "anova_balanced[['mean_IN']] = anova_balanced[['mean_IN']].astype(int)\n",
    "anova_balanced.to_csv('anova_balanced.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import if already saved\n",
    "\n",
    "anova_balanced = pd.DataFrame.from_csv('anova_balanced.csv',sep='\\t',index_col=None)\n",
    "anova_balanced['baby_id']=anova_balanced['baby_id'].apply(correct_baby_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANOVA on the last design\n",
    "\n",
    "# Warning ! Think of Bonferroni correction for multiple comparisons\n",
    "\n",
    "rdf_balanced = pandas2ri.py2ri(anova_balanced)\n",
    "rdf_balanced[rdf_balanced.names.index('month')] = R.ordered(rdf_balanced.rx2('month'))\n",
    "\n",
    "# print(R.summary(rdf_balanced))\n",
    "\n",
    "fml = Formula(\"mean_EX~(month*context)+Error(baby_id/(month*context))\")\n",
    "result = R.aov(fml,data=rdf_balanced)\n",
    "print(R.summary(result))\n",
    "\n",
    "fml = Formula(\"mean_EX~(sex*month*context)+Error(baby_id/(month*context))\")\n",
    "result = R.aov(fml,data=rdf_balanced)\n",
    "print(R.summary(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anova_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print figures about data\n",
    "\n",
    "sb.pointplot(x='month', y='mean_hnr', hue = 'context', data = anova_balanced);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1);\n",
    "sb.boxplot(x='month', y='mean_pitch_Swipe', hue = 'context', data = anova_balanced);\n",
    "\n",
    "plt.subplot(2, 1, 2);\n",
    "sb.violinplot(x='month', y='mean_pitch_Swipe', hue = 'context', data = anova_balanced);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear mixed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function because when import .csv, python understand the baby_ID string as a Int\n",
    "\n",
    "def correct_baby_id(baby_id):\n",
    "        return '0' + str(baby_id)\n",
    "    \n",
    "# To convert '01m' to a int (too many degree of a liberty if we consider month as a categorical variable and not\n",
    "# a continuous variable)\n",
    "    \n",
    "def correct_baby_month(month):\n",
    "    return int(month[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Installation lme4 if not already installed \n",
    "\n",
    "#import rpy2.interactive as r\n",
    "#import rpy2.interactive.packages \n",
    "#rlib = r.packages.packages\n",
    "#r.packages.importr(\"utils\")\n",
    "#package_name = \"lme4\"\n",
    "#rlib.utils.install_packages(package_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import if already saved\n",
    "\n",
    "anova_design = pd.DataFrame.from_csv('anova_design.csv',sep='\\t',index_col=None)\n",
    "anova_design['baby_id']=anova_design['baby_id'].apply(correct_baby_id)\n",
    "anova_design['month']=anova_design['month'].apply(correct_baby_month)\n",
    "\n",
    "rdf_anova=pandas2ri.py2ri(anova_design)\n",
    "\n",
    "print(R.summary(rdf_anova))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LME PAR lme4\n",
    "\n",
    "# Need number of observations >= number of random effects (here number_month*number_context*number_baby)\n",
    "\n",
    "# LME without the effect of sex\n",
    "\n",
    "def function_lme1(rdf,feature):\n",
    "    fml_nul = Formula(feature + \" ~ 1 + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v0 = Formula(feature + \" ~ month + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v1 = Formula(feature + \" ~ month + context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v2 = Formula(feature + \" ~ month * context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "\n",
    "    nul = lme4.lmer(fml_nul, data = rdf, REML = False)\n",
    "    v0 = lme4.lmer(fml_v0, data = rdf, REML = False) \n",
    "    v1 = lme4.lmer(fml_v1, data = rdf, REML = False) \n",
    "    v2 = lme4.lmer(fml_v2, data = rdf, REML = False) \n",
    "\n",
    "    print(R.anova(nul,v0,v1,v2))\n",
    "    \n",
    "# LME with the effect of sex\n",
    "\n",
    "def function_lme2(rdf,feature):\n",
    "    fml_nul = Formula(feature + \" ~ 1 + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v0 = Formula(feature + \" ~ sex + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v1 = Formula(feature + \" ~ sex + month + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v2 = Formula(feature + \" ~ sex + month + context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v31 = Formula(feature + \" ~ sex + month * context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v32 = Formula(feature + \" ~ sex * context + month + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v33 = Formula(feature + \" ~ sex * month + context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v4 = Formula(feature + \" ~ sex * month * context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "\n",
    "    nul = lme4.lmer(fml_nul, data = rdf, REML = False)\n",
    "    v0 = lme4.lmer(fml_v0, data = rdf, REML = False) \n",
    "    v1 = lme4.lmer(fml_v1, data = rdf, REML = False) \n",
    "    v2 = lme4.lmer(fml_v2, data = rdf, REML = False) \n",
    "    v31 = lme4.lmer(fml_v31, data = rdf, REML = False)\n",
    "    v32 = lme4.lmer(fml_v32, data = rdf, REML = False)\n",
    "    v33 = lme4.lmer(fml_v33, data = rdf, REML = False)\n",
    "    v4 = lme4.lmer(fml_v4, data = rdf, REML = False)\n",
    "\n",
    "    print(R.anova(nul,v0,v1,v2,v31,v4))\n",
    "    print(R.anova(nul,v0,v1,v2,v32,v4))\n",
    "    print(R.anova(nul,v0,v1,v2,v33,v4))\n",
    "    \n",
    "# LME with the effect of sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Design 1 : same than ANOVA_design (no need of mean and interpolation)\n",
    "\n",
    "start = time.time()\n",
    "function_lme1(rdf_anova,\"mean_hnr\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger desing for LME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selection of a larger design\n",
    "\n",
    "# If we want Design 2 : number_design == 2 => 4 months, all babies and three contexts\n",
    "# If we want Design 3 : number_design == 3 => 8 months, all babies and three contexts\n",
    "\n",
    "number_design = 3\n",
    "\n",
    "#\n",
    "\n",
    "contexts_selected = ['hungry','pee','sleepy']\n",
    "\n",
    "if (number_design == 2):\n",
    "    months_selected = range(4)\n",
    "elif (number_design == 3):\n",
    "    months_selected = range(8)\n",
    "\n",
    "all_babies_features = pd.DataFrame.from_csv('all_babies_features.csv',sep='\\t',index_col=None)\n",
    "all_babies_features['baby_id']=all_babies_features['baby_id'].apply(correct_baby_id)\n",
    "all_babies_features['month']=all_babies_features['month'].apply(correct_baby_month)\n",
    "\n",
    "lme_design = all_babies_features[all_babies_features.month.isin(months_selected)]\n",
    "lme_design = lme_design[lme_design.context.isin(contexts_selected)]\n",
    "\n",
    "rdf_lme = pandas2ri.py2ri(lme_design)\n",
    "\n",
    "print(R.summary(rdf_lme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "function_lme1(rdf_lme,\"mean_EX\")\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "start = time.time()\n",
    "function_lme2(rdf_lme,\"mean_EX\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to display with seaborn a feature in function of month, we must first calculate the mean of the feature\n",
    "# for each baby and for each month.\n",
    "\n",
    "baby_id = ['011','012','042','043','044','045','046','047','048','049','050','051','052','053','054','055','056','057','058','059','060','061','062','063','064','065','066','067','068','069','070','071','072']\n",
    "\n",
    "name_static_feature = 'mean_maxPitch_Praat'\n",
    "\n",
    "# One static invariant in function of months\n",
    "\n",
    "baby_number = []\n",
    "month_number = []\n",
    "static_feature = []\n",
    "\n",
    "for j in range(len(baby_id)):\n",
    "    for k in range(len(months_selected)):\n",
    "        temp = lme_design[['baby_id','month',name_static_feature]]\n",
    "        #\n",
    "        temp = temp[temp.baby_id == baby_id[j]]\n",
    "        temp = temp[temp.month == months_selected[k]]\n",
    "        baby_number = baby_number + [baby_id[j]]\n",
    "        month_number = month_number + [months_selected[k]]\n",
    "        static_feature = static_feature + [temp[name_static_feature].mean()]\n",
    "            \n",
    "static_feature_month = pd.DataFrame({'baby_id' : baby_number,\\\n",
    "                                     'month' : month_number,\\\n",
    "                                     name_static_feature : static_feature\n",
    "                                      })\n",
    "\n",
    "static_feature_month = static_feature_month.dropna(axis = 0)\n",
    "\n",
    "sb.pointplot(x='month', y=name_static_feature, data = static_feature_month);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to display with seaborn a feature in function of context, we must first calculate the mean of the feature\n",
    "# for each baby and for each context.\n",
    "\n",
    "baby_id = ['011','012','042','043','044','045','046','047','048','049','050','051','052','053','054','055','056','057','058','059','060','061','062','063','064','065','066','067','068','069','070','071','072']\n",
    "\n",
    "name_static_feature = 'mean_shimmer_apq5'\n",
    "\n",
    "# One static invariant in function of months\n",
    "\n",
    "baby_number = []\n",
    "context_name = []\n",
    "static_feature = []\n",
    "\n",
    "for j in range(len(baby_id)):\n",
    "    for k in range(len(contexts_selected)):\n",
    "        temp = lme_design[['baby_id','context',name_static_feature]]\n",
    "        #\n",
    "        temp = temp[temp.baby_id == baby_id[j]]\n",
    "        temp = temp[temp.context == contexts_selected[k]]\n",
    "        baby_number = baby_number + [baby_id[j]]\n",
    "        context_name = context_name + [contexts_selected[k]]\n",
    "        static_feature = static_feature + [temp[name_static_feature].mean()]\n",
    "            \n",
    "static_feature_month = pd.DataFrame({'baby_id' : baby_number,\\\n",
    "                                     'context' : context_name,\\\n",
    "                                     name_static_feature : static_feature\n",
    "                                      })\n",
    "\n",
    "static_feature_month = static_feature_month.dropna(axis = 0)\n",
    "\n",
    "sb.pointplot(x='context', y=name_static_feature, data = static_feature_month);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to display with seaborn a feature in function of month and context,\n",
    "# we must first calculate the mean of the feature\n",
    "# for each baby, for each month and for each context.\n",
    "\n",
    "baby_id = ['011','012','042','043','044','045','046','047','048','049','050','051','052','053','054','055','056','057','058','059','060','061','062','063','064','065','066','067','068','069','070','071','072']\n",
    "\n",
    "name_static_feature = 'mean_shimmer_apq5'\n",
    "\n",
    "# One static invariant in function of months\n",
    "\n",
    "baby_number = []\n",
    "month_number = []\n",
    "context_name = []\n",
    "static_feature = []\n",
    "\n",
    "for j in range(len(baby_id)):\n",
    "    for k in range(len(months_selected)):\n",
    "        for l in range(len(contexts_selected)):\n",
    "            temp = lme_design[['baby_id','month','context',name_static_feature]]\n",
    "            #\n",
    "            temp = temp[temp.baby_id == baby_id[j]]\n",
    "            temp = temp[temp.month == months_selected[k]]\n",
    "            temp = temp[temp.context == contexts_selected[l]]\n",
    "            baby_number = baby_number + [baby_id[j]]\n",
    "            month_number = month_number + [months_selected[k]]\n",
    "            context_name = context_name + [contexts_selected[l]]\n",
    "            static_feature = static_feature + [temp[name_static_feature].mean()]\n",
    "            \n",
    "static_feature_context_month = pd.DataFrame({'baby_id' : baby_number,\\\n",
    "                                             'month' : month_number,\\\n",
    "                                             'context' : context_name,\\\n",
    "                                             name_static_feature : static_feature\n",
    "                                              })\n",
    "\n",
    "static_feature_context_month = static_feature_context_month.dropna(axis = 0)\n",
    "\n",
    "sb.pointplot(x='month', y=name_static_feature, hue = 'context', data = static_feature_context_month);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Pitch contour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract pitch contour each 10 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SWIPE : 1763 files \n",
    "\n",
    "SAMPLING_RATE = 44100\n",
    "\n",
    "directory = os.getcwd()\n",
    "path_files = [x[0] for x in os.walk(directory)]\n",
    "\n",
    "position = 0 \n",
    "\n",
    "for path_file in path_files:\n",
    "    if (path_file.find('audio') != -1):\n",
    "        file_path_audio = path_file+'/'\n",
    "        file_path_labels = file_path_audio[:-6]+'labels/'\n",
    "        for audio_file_path in glob.glob(file_path_audio+'*.wav'):\n",
    "            print(audio_file_path, position)\n",
    "            start = []\n",
    "            dur = []\n",
    "            label = []\n",
    "            pitch_Swipe = []\n",
    "            per_none_interpo = []\n",
    "            # normalise file\n",
    "            audio = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "            audio = (audio - np.mean(audio))/ max(abs(audio-np.mean(audio)))\n",
    "            f0 = pysptk.swipe(audio.astype(np.float64), fs=SAMPLING_RATE, hopsize=440, min=150, max=550, otype=\"f0\")\n",
    "            for i in range(4):\n",
    "                None_nan = len(f0[f0 > 150])/float(len(f0))\n",
    "                #\n",
    "                f0temp = pysptk.swipe(audio.astype(np.float64), fs=SAMPLING_RATE, hopsize=440, min=150, max=550, otype=\"f0\")\n",
    "                None_nan_temp = len(f0temp[f0temp > 150])/float(len(f0temp))\n",
    "                if (None_nan_temp > None_nan):\n",
    "                    f0 = f0temp\n",
    "            #\n",
    "            df_states = pd.DataFrame.from_csv(file_path_labels+os.path.splitext(os.path.basename(audio_file_path))[0]+'.csv',sep='\\t',index_col=None)\n",
    "            for i in range(len(df_states)):\n",
    "                start = start + [df_states.iloc[i]['start']]\n",
    "                dur = dur + [df_states.iloc[i]['dur']]\n",
    "                label = label + [df_states.iloc[i]['label']]\n",
    "                if (df_states.iloc[i]['label']=='EX'):\n",
    "                    start_N = int(round((float(df_states.iloc[i]['start'])/1000)/(1./SAMPLING_RATE)))\n",
    "                    start_swipe = int(round(float(start_N)/440))\n",
    "                    end_N = int(round((float(df_states.iloc[i]['start']+df_states.iloc[i]['dur'])/1000)/(1./SAMPLING_RATE)))\n",
    "                    end_swipe = int(round(float(end_N)/440))\n",
    "                    f0_intervalle = f0[start_swipe:end_swipe+1]\n",
    "                    f0_intervalle[f0_intervalle <= 150] = nan\n",
    "                    if (np.size(f0_intervalle[np.isnan(f0_intervalle) == False]) == 0):\n",
    "                        pitch_Swipe = pitch_Swipe + [-1]\n",
    "                        per_none_interpo = per_none_interpo + [-1]\n",
    "                    else:\n",
    "                        # Supress beginning and end NaN\n",
    "                        i = 0\n",
    "                        j = len(f0_intervalle) - 1\n",
    "                        while(np.isnan(f0_intervalle[i]) == True):\n",
    "                            i = i + 1\n",
    "                        while(np.isnan(f0_intervalle[j]) == True):\n",
    "                            j = j - 1\n",
    "                        f0_intervalle = f0_intervalle[i:j+1]\n",
    "                        # Percentage none interpolated values\n",
    "                        per_none_interpo = per_none_interpo +[len(f0_intervalle[isnan(f0_intervalle) == False])/float(len(f0_intervalle))]\n",
    "                        # Interpolation missing values\n",
    "                        nans, x= np.isnan(f0_intervalle), lambda z: z.nonzero()[0]\n",
    "                        f0_intervalle[nans]= np.interp(x(nans), x(~nans), f0_intervalle[~nans])\n",
    "                        pitch_Swipe = pitch_Swipe + [f0_intervalle]\n",
    "                else:\n",
    "                    pitch_Swipe = pitch_Swipe + [nan]\n",
    "                    per_none_interpo = per_none_interpo + [nan]\n",
    "            df_states_and_pitch = pd.DataFrame({'start' : start,'dur' : dur,'label' : label,'pitch_Swipe' : pitch_Swipe,'per_none_interpo' : per_none_interpo})\n",
    "            df_states_and_pitch = df_states_and_pitch[['start','dur','label','pitch_Swipe','per_none_interpo']]\n",
    "            df_states_and_pitch.to_csv(file_path_labels+os.path.splitext(os.path.basename(audio_file_path))[0]+'+pitch_contour_Swipe'+'.csv',sep='\\t', index=False)\n",
    "            position = position + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function because when import .csv, python understand the baby_ID string as a Int\n",
    "\n",
    "def correct_baby_id(baby_id):\n",
    "        return '0' + str(baby_id)\n",
    "\n",
    "directory = os.getcwd()\n",
    "path_files = [x[0] for x in os.walk(directory)]\n",
    "\n",
    "baby_sex_data = pd.DataFrame.from_csv('baby_sex_data.csv',sep='\\t',index_col=None, header = None)\n",
    "baby_sex_data.columns = ['baby_id','sex']\n",
    "baby_sex_data['baby_id']=baby_sex_data['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "babies_id = []\n",
    "sex = []\n",
    "months = []\n",
    "contexts = []\n",
    "start = []\n",
    "durations = []\n",
    "pitch_contours = []\n",
    "per_none_interpo = []\n",
    "audio_file_path = []\n",
    "\n",
    "for path_file in path_files:\n",
    "    if (path_file.find('labels') != -1):\n",
    "        files_path_labels = path_file+'/'\n",
    "        for file_path_labels in glob.glob(files_path_labels+'*.csv'):\n",
    "            if (file_path_labels.find('pitch_contour_Swipe') != -1):\n",
    "                print(file_path_labels)\n",
    "                month = os.path.split(file_path_labels)[1][:3]\n",
    "                context = os.path.split(os.path.split(os.path.split(file_path_labels)[0])[0])[1]\n",
    "                baby_id = os.path.split(os.path.split(os.path.split(os.path.split(file_path_labels)[0])[0])[0])[1]\n",
    "                sex_baby = baby_sex_data[baby_sex_data.baby_id == baby_id].sex.values[0]\n",
    "                df_states_and_pitch = pd.DataFrame.from_csv(file_path_labels,sep='\\t',index_col=None)\n",
    "                for i in range(len(df_states_and_pitch)):\n",
    "                    if (df_states_and_pitch.iloc[i]['label']=='EX'):\n",
    "                        babies_id = babies_id + [baby_id]\n",
    "                        months = months + [month]\n",
    "                        contexts = contexts + [context]\n",
    "                        sex = sex + [sex_baby]\n",
    "                        start = start + [df_states_and_pitch.iloc[i]['start']]\n",
    "                        durations = durations + [df_states_and_pitch.iloc[i]['dur']]\n",
    "                        pitch_contours = pitch_contours + [df_states_and_pitch.iloc[i]['pitch_Swipe']]\n",
    "                        per_none_interpo = per_none_interpo + [df_states_and_pitch.iloc[i]['per_none_interpo']]\n",
    "                        audio_file_path = audio_file_path + [os.path.split(file_path_labels)[0][:-6]+'audio/'+os.path.split(file_path_labels)[1][:-24]+'.wav']\n",
    "            \n",
    "all_babies_pitch_contours = pd.DataFrame({'baby_id' : babies_id,\\\n",
    "                                          'sex' :sex,\\\n",
    "                                          'month' :months,\\\n",
    "                                          'context' : contexts,\\\n",
    "                                          'start' : start,\\\n",
    "                                          'duration' : durations,\\\n",
    "                                          'pitch_contour' : pitch_contours,\\\n",
    "                                          'per_none_interpo' : per_none_interpo,\\\n",
    "                                          'audio_file_path' : audio_file_path\n",
    "                                         })\n",
    "\n",
    "all_babies_pitch_contours=all_babies_pitch_contours[['baby_id','sex','month','context','start','duration','pitch_contour','per_none_interpo','audio_file_path']]\n",
    "\n",
    "all_babies_pitch_contours.to_csv('all_babies_pitch_contours.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_babies_pitch_contours = pd.DataFrame.from_csv('all_babies_pitch_contours.csv',sep='\\t',index_col=None)\n",
    "\n",
    "all_babies_pitch_contours = all_babies_pitch_contours[all_babies_pitch_contours.pitch_contour != '-1']\n",
    "\n",
    "all_babies_pitch_contours['baby_id'] = all_babies_pitch_contours['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "all_babies_pitch_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We only take pitch contours with more than 8 points, with more than 90% none interpolation points and between \n",
    "# 250 and 1500 ms of duration.\n",
    "\n",
    "all_babies_pitch_contours = all_babies_pitch_contours[all_babies_pitch_contours.duration >= 250]\n",
    "all_babies_pitch_contours = all_babies_pitch_contours[all_babies_pitch_contours.duration <= 1500]\n",
    "all_babies_pitch_contours = all_babies_pitch_contours[all_babies_pitch_contours.per_none_interpo >=0.9]\n",
    "\n",
    "babies_id = []\n",
    "sex = []\n",
    "months = []\n",
    "contexts = []\n",
    "starts = []\n",
    "durations = []\n",
    "pitch_contours = []\n",
    "per_none_interpo = []\n",
    "audio_file_path = []\n",
    "\n",
    "for i in range(len(all_babies_pitch_contours)):\n",
    "    test = all_babies_pitch_contours.iloc[i]['pitch_contour']\n",
    "    results = [float(t) for t in test[0:len(test)-1].split(' ') if ((t.find('[')==-1)&(len(t)>0))]\n",
    "    if (len(results) > 22):\n",
    "        babies_id = babies_id + [all_babies_pitch_contours.iloc[i]['baby_id']]\n",
    "        months = months + [all_babies_pitch_contours.iloc[i]['month']]\n",
    "        contexts = contexts + [all_babies_pitch_contours.iloc[i]['context']]\n",
    "        sex = sex + [all_babies_pitch_contours.iloc[i]['sex']]\n",
    "        starts = starts + [all_babies_pitch_contours.iloc[i]['start']]\n",
    "        durations = durations + [all_babies_pitch_contours.iloc[i]['duration']]\n",
    "        pitch_contours = pitch_contours + [all_babies_pitch_contours.iloc[i]['pitch_contour']]\n",
    "        per_none_interpo = per_none_interpo + [all_babies_pitch_contours.iloc[i]['per_none_interpo']]\n",
    "        audio_file_path = audio_file_path + [all_babies_pitch_contours.iloc[i]['audio_file_path']]\n",
    "\n",
    "selected_pitch_contours = pd.DataFrame({'baby_id' : babies_id,\\\n",
    "                                          'sex' :sex,\\\n",
    "                                          'month' :months,\\\n",
    "                                          'context' : contexts,\\\n",
    "                                          'duration' : durations,\\\n",
    "                                          'start' : starts,\\\n",
    "                                          'pitch_contour' : pitch_contours,\\\n",
    "                                          'per_none_interpo' : per_none_interpo,\\\n",
    "                                          'audio_file_path' : audio_file_path\n",
    "                                         })\n",
    "\n",
    "selected_pitch_contours=selected_pitch_contours[['baby_id','sex','month','context','start','duration','pitch_contour','per_none_interpo','audio_file_path']]\n",
    "\n",
    "selected_pitch_contours.to_csv('selected_pitch_contours.csv',sep='\\t',index=False)\n",
    "\n",
    "len(selected_pitch_contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_pitch_contour(pitch_contour):\n",
    "    results = [float(t) for t in pitch_contour[0:len(pitch_contour)-1].split(' ') if ((t.find('[')==-1)&(len(t)>0))]\n",
    "    return results\n",
    "\n",
    "selected_pitch_contours = pd.DataFrame.from_csv('selected_pitch_contours.csv',sep='\\t',index_col=None)\n",
    "selected_pitch_contours['pitch_contour']=selected_pitch_contours['pitch_contour'].apply(correct_pitch_contour)\n",
    "selected_pitch_contours['baby_id']=selected_pitch_contours['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "# INIT PARAMETERS\n",
    "\n",
    "REF = 110 # Ref for Hz -> cent conversion\n",
    "\n",
    "HOP_SIZE = 440 # Swipe hop_size\n",
    "\n",
    "FS = 44100\n",
    "\n",
    "# TEST on two differents signals\n",
    "\n",
    "X1 = np.array([311,321,329,332,334,330,331,333,329,311])\n",
    "X2 = selected_pitch_contours.iloc[0]['pitch_contour']\n",
    "\n",
    "X_cents = np.asarray(map(lambda x: 1200*math.log(x/float(REF), 2), X2))\n",
    "X_cents = X_cents - mean(X_cents)\n",
    "\n",
    "Y1 = np.array([411,421,429,432,434,430,431,433,429,411])\n",
    "Y2 = selected_pitch_contours.iloc[1]['pitch_contour']\n",
    "\n",
    "Y_cents = np.asarray(map(lambda x: 1200*math.log(x/float(REF), 2), Y2))\n",
    "Y_cents = Y_cents - mean(Y_cents)\n",
    "\n",
    "# Weights on the three possible steps (D, H, V)\n",
    "\n",
    "weights_mul = np.array([2, 1, 1]) #No priviligied direction\n",
    "\n",
    "# Applies global constraints to the cost matrix C \n",
    "\n",
    "global_constraints = True\n",
    "\n",
    "# Sakoe-Chiba band radius (1/2 of the width) will be int(radius*min(C.shape)) with C the cost function\n",
    "\n",
    "band_rad = 1  \n",
    "\n",
    "if len(X_cents) > len(Y_cents):\n",
    "    temp = X_cents\n",
    "    X_cents = Y_cents\n",
    "    Y_cents = temp\n",
    "    \n",
    "D, wp = librosa.dtw(X_cents, Y_cents,weights_mul=weights_mul, global_constraints=global_constraints,band_rad = band_rad)\n",
    "\n",
    "print(D[len(X_cents)-1,len(Y_cents)-1])\n",
    "\n",
    "# Display 1 : Warping Path on Acc. Cost Matrix (A first display)\n",
    "\n",
    "librosa.display.specshow(D, x_axis='frames', y_axis='frames')\n",
    "plt.title('Warping Path on Acc. Cost Matrix $D$ between two pitch contour of a cry')\n",
    "plt.plot(wp[:, 1], wp[:, 0], label='Optimal path', color='y')\n",
    "plt.legend();\n",
    "\n",
    "# Display 2 : Warping Path on Acc. Cost Matrix (A second display) \n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "wp_s = np.asarray(wp)*float(HOP_SIZE)/FS \n",
    "\n",
    "librosa.display.specshow(D,sr=FS, x_axis='time', y_axis='time',\n",
    "                         cmap='gray_r', hop_length=HOP_SIZE)\n",
    "imax = ax.imshow(D, cmap=plt.get_cmap('gray_r'),\n",
    "                 origin='lower', interpolation='nearest', aspect='auto')\n",
    "ax.plot(wp_s[:, 1], wp_s[:, 0], marker='o', color='r')\n",
    "plt.title('Warping Path on Acc. Cost Matrix $D$ between two pitch contour of a cry')\n",
    "plt.colorbar();\n",
    "\n",
    "# Display 3 : alignement of the two time series\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "axis1 = map(float,range(len(X_cents)))\n",
    "axis1 = [x*float(HOP_SIZE)/FS for x in axis1]\n",
    "\n",
    "axis2 = map(float,range(len(Y_cents)))\n",
    "axis2 = [x*float(HOP_SIZE)/FS for x in axis2]\n",
    " \n",
    "plt.subplot(2, 1, 1) # plot X_cents\n",
    "plt.plot(axis1,X_cents)\n",
    "plt.title('DTW between two pitch contours (in cents)')\n",
    "plt.ylabel('Frequency (in Cents)')\n",
    "plt.xlabel('time (in s)')\n",
    "ax1 = plt.gca()\n",
    "\n",
    "plt.subplot(2, 1, 2) # plot Y_cents\n",
    "plt.plot(axis2,Y_cents)\n",
    "plt.ylabel('Frequency (in Cents)')\n",
    "plt.xlabel('time (in s)')\n",
    "ax2 = plt.gca()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "trans_figure = fig.transFigure.inverted()\n",
    "lines = []\n",
    "arrows = 30\n",
    "points_idx = np.int16(np.round(np.linspace(0, wp.shape[0] - 1, arrows)))\n",
    "\n",
    "for tp1, tp2 in wp[points_idx]:\n",
    "    # get position on axis for a given index-pair\n",
    "    coord1 = trans_figure.transform(ax1.transData.transform([axis1[tp1], X_cents[tp1]]))\n",
    "    coord2 = trans_figure.transform(ax2.transData.transform([axis2[tp2], Y_cents[tp2]]))\n",
    "\n",
    "    # draw a line\n",
    "    line = matplotlib.lines.Line2D((coord1[0], coord2[0]),\n",
    "                                   (coord1[1], coord2[1]),\n",
    "                                   transform=fig.transFigure,\n",
    "                                   color='r')\n",
    "    lines.append(line)\n",
    "\n",
    "    fig.lines = lines\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test on various signal to try different weights_mul and band_rad\n",
    "\n",
    "X2 = selected_pitch_contours.iloc[0]['pitch_contour']\n",
    "X_cents = np.asarray(map(lambda x: 1200*math.log(x/float(REF), 2), X2))\n",
    "X_cents = X_cents - mean(X_cents)\n",
    "#\n",
    "X2 = selected_pitch_contours.iloc[1]['pitch_contour']\n",
    "Y_cents = np.asarray(map(lambda x: 1200*math.log(x/float(REF), 2), Y2))\n",
    "Y_cents = Y_cents - mean(Y_cents)\n",
    "\n",
    "PC0 = X_cents\n",
    "PC1 = Y_cents\n",
    "PC2 = - X_cents\n",
    "PC3 = - Y_cents\n",
    "PC4 = X_cents*0.4\n",
    "PC5 = Y_cents*0.6\n",
    "PC6 = - X_cents*0.4\n",
    "PC7 = - Y_cents*0.6\n",
    "\n",
    "all_PC = {}\n",
    "all_PC['PC0'] = PC0\n",
    "all_PC['PC1'] = PC1\n",
    "all_PC['PC2'] = PC2\n",
    "all_PC['PC3'] = PC3\n",
    "all_PC['PC4'] = PC4\n",
    "all_PC['PC5'] = PC5\n",
    "all_PC['PC6'] = PC6\n",
    "all_PC['PC7'] = PC7\n",
    "\n",
    "# Init parameters\n",
    "\n",
    "weights_mul = np.array([2, 1, 1]) \n",
    "\n",
    "band_rad = 0.25\n",
    "\n",
    "# Creation matrice distance\n",
    "\n",
    "MAT_DTW_PC = np.zeros((len(all_PC),len(all_PC)))\n",
    "\n",
    "for i in range(len(all_PC)):\n",
    "    for j in range(i):\n",
    "        PCA = all_PC['PC'+str(i)]\n",
    "        PCB = all_PC['PC'+str(j)]\n",
    "        if len(PCA) > len(PCB):\n",
    "            temp = PCA\n",
    "            PCA = PCB\n",
    "            PCB = temp\n",
    "        D, wp = librosa.dtw(PCA, PCB,weights_mul=weights_mul, global_constraints=global_constraints,band_rad = band_rad)\n",
    "        DTW = D[len(PCA)-1,len(PCB)-1]\n",
    "        MAT_DTW_PC[i,j] = DTW\n",
    "\n",
    "MAT_DTW_PC = MAT_DTW_PC + MAT_DTW_PC.transpose()\n",
    "\n",
    "# Display matrice distance\n",
    "        \n",
    "fig = plt.figure(figsize=(12, 6.4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Measured distance with DTW between multiple time series')\n",
    "plt.imshow(MAT_DTW_PC, origin='lower')\n",
    "plt.xticks(np.arange(0, len(all_PC), dtype=np.int))\n",
    "plt.xlabel('Number of time series')\n",
    "plt.ylabel('Number of time series')\n",
    "plt.yticks(np.arange(0, len(all_PC), dtype=np.int))\n",
    "cbar = plt.colorbar(orientation='vertical');\n",
    "cbar.set_label('Distance of DTW', rotation=270)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering time series (Algorithm Clarans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we preprocessing (Hz -> Cents + centering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct_baby_id(baby_id):\n",
    "        return '0' + str(baby_id)\n",
    "\n",
    "def correct_pitch_contour(pitch_contour):\n",
    "    results = [float(t) for t in pitch_contour[0:len(pitch_contour)-1].split(' ') if ((t.find('[')==-1)&(len(t)>0))]\n",
    "    return results\n",
    "\n",
    "selected_pitch_contours = pd.DataFrame.from_csv('selected_pitch_contours.csv',sep='\\t',index_col=None)\n",
    "selected_pitch_contours['pitch_contour']=selected_pitch_contours['pitch_contour'].apply(correct_pitch_contour)\n",
    "selected_pitch_contours['baby_id']=selected_pitch_contours['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "pitch_contour = []\n",
    "\n",
    "for k in range(len(selected_pitch_contours)):\n",
    "    pitch_contour = pitch_contour + [selected_pitch_contours.pitch_contour.iloc[k]]\n",
    "\n",
    "# INIT PARAMETERS\n",
    "\n",
    "ref = 110 # Ref for Hz -> cents conversion\n",
    "\n",
    "# Hz -> + centering conversion\n",
    "\n",
    "def pitch_contour_conversion(pitch_contour):\n",
    "    X_cents = np.asarray(map(lambda x: 1200*math.log(x/float(ref), 2), pitch_contour))\n",
    "    X_cents = X_cents - mean(X_cents)\n",
    "    return X_cents\n",
    "\n",
    "selected_pitch_contours['pitch_contour']=selected_pitch_contours['pitch_contour'].apply(pitch_contour_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_pitch_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function clarans_DTW : a time series clustering algorithm on large data_set (more than 100 datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class clarans_DTW:\n",
    "    \"\"\"!\n",
    "    @brief Class represents clustering algorithm CLARANS (a method for clustering objects for spatial data mining).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, number_clusters, numlocal, maxneighbor, weights_mul, global_constraints, band_rad):\n",
    "        \"\"\"!\n",
    "        @brief Constructor of clustering algorithm CLARANS, here applied to time series.\n",
    "        @details The higher the value of maxneighbor, the closer is CLARANS to K-Medoids (PAM - Partitioning Around Medoids), and the longer is each search of a local minima.\n",
    "        \n",
    "        @param[in] data (list): Input data that is presented as a list of times series\n",
    "        @param[in] number_clusters (uint): amount of clusters that should be allocated.\n",
    "        @param[in] numlocal (uint): the number of local minima obtained (amount of iterations for solving the problem).\n",
    "        @param[in] maxneighbor (uint): the maximum number of neighbors examined.\n",
    "        @param[in] weights_mul (uint): Weights on the three possible steps (D, H, V) of the DTW\n",
    "        @param[in] global_constraints (bool): Applies global constraints to the cost matrix of the DTW\n",
    "        @param[in] band_rad (floor): Sakoe-Chiba band radius (1/2 of the width) will be int(radius*min(C.shape)) with C the cost function\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.__pointer_data = data;\n",
    "        self.__numlocal = numlocal;\n",
    "        self.__maxneighbor = maxneighbor;\n",
    "        self.__number_clusters = number_clusters;\n",
    "        self.__weights_mul = weights_mul;\n",
    "        self.__global_constraints = global_constraints;\n",
    "        self.__band_rad = band_rad;\n",
    "        \n",
    "        # Gave a list of index of all data in each clusters\n",
    "        self.__clusters = [];\n",
    "        # Current list of medoids\n",
    "        self.__current = [];\n",
    "        # Cluster appartenance of each time series\n",
    "        self.__belong = [];\n",
    "        \n",
    "        # Better medoids\n",
    "        self.__optimal_medoids = [];\n",
    "        # Optimal cost of clustering\n",
    "        self.__optimal_estimation = float('inf');\n",
    "    \n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"!\n",
    "        @brief Performs cluster analysis in line with rules of CLARANS algorithm.\n",
    "        \n",
    "        @see get_clusters()\n",
    "        @see get_medoids()\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        random.seed();\n",
    "        \n",
    "        for _ in range(0, self.__numlocal):\n",
    "            # set (current) random medoids\n",
    "            self.__current = random.sample(range(len(self.__pointer_data)), self.__number_clusters);\n",
    "            \n",
    "            # update clusters in line with random allocated medoids\n",
    "            self.__update_clusters(self.__current);\n",
    "            \n",
    "            # optimize configuration : search better clustering in a sampled neighbourhood of medoids\n",
    "            self.__optimize_configuration();\n",
    "            \n",
    "            # obtain cost of current cluster configuration and compare it with the best obtained\n",
    "            estimation = self.__calculate_estimation();\n",
    "            if (estimation < self.__optimal_estimation):\n",
    "                self.__optimal_medoids = self.__current[:];\n",
    "                self.__optimal_estimation = estimation;\n",
    "        \n",
    "        self.__update_clusters(self.__optimal_medoids);\n",
    "    \n",
    "    \n",
    "    def get_clusters(self):\n",
    "        \"\"\"!\n",
    "        @brief Returns allocated clusters by the algorithm.\n",
    "        \n",
    "        @remark Allocated clusters can be returned only after data processing (use method process()), otherwise empty list is returned.\n",
    "        \n",
    "        @return (list) List of allocated clusters, each cluster contains indexes of objects in list of data.\n",
    "        \n",
    "        @see process()\n",
    "        @see get_medoids()\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.__clusters;\n",
    "    \n",
    "    \n",
    "    def get_medoids(self):\n",
    "        \n",
    "        \"\"\"!\n",
    "        @brief Returns list of medoids of allocated clusters.\n",
    "        \n",
    "        @see process()\n",
    "        @see get_clusters()\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        return self.__optimal_medoids;\n",
    "    \n",
    "    def get_estimation(self):\n",
    "        \n",
    "        \"\"\"!\n",
    "        @brief Returns list of computed DTW\n",
    "        \n",
    "        @see process()\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        return self.__optimal_estimation;\n",
    "\n",
    "    def __update_DTW_Matrix(self,index_point1,index_point2):\n",
    "        \n",
    "        D = librosa.dtw(self.__pointer_data[index_point1], self.__pointer_data[index_point2], weights_mul=self.__weights_mul, global_constraints=self.__global_constraints, band_rad = self.__band_rad)[0]\n",
    "        dist = D[len(self.__pointer_data[index_point1])-1,len(self.__pointer_data[index_point2])-1]\n",
    "        \n",
    "        return dist;\n",
    "\n",
    "    def __update_clusters(self, medoids):\n",
    "        \"\"\"!\n",
    "        @brief Forms cluster in line with specified medoids by calculation distance from each point to medoids. \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.__belong = [0] * len(self.__pointer_data);\n",
    "        self.__clusters = [[] for i in range(len(medoids))];\n",
    "        for index_point in range(len(self.__pointer_data)):\n",
    "            index_optim = -1;\n",
    "            dist_optim = 0.0;\n",
    "            \n",
    "            for index in range(len(medoids)):\n",
    "                \n",
    "                dist = self.__update_DTW_Matrix(index_point, medoids[index]);\n",
    "                    \n",
    "                if ( (dist < dist_optim) or (index is 0)):\n",
    "                    index_optim = index;\n",
    "                    dist_optim = dist;\n",
    "             \n",
    "            self.__clusters[index_optim].append(index_point);\n",
    "            self.__belong[index_point] = index_optim;\n",
    "        \n",
    "        # If cluster is not able to capture object it should be removed\n",
    "        self.__clusters = [cluster for cluster in self.__clusters if len(cluster) > 0];\n",
    "    \n",
    "    def __optimize_configuration(self):\n",
    "        \"\"\"!\n",
    "        @brief Finds quasi-optimal medoids and updates in line with them clusters in line with algorithm's rules. \n",
    "        \n",
    "        \"\"\"\n",
    "        index_neighbor = 0;\n",
    "        while (index_neighbor < self.__maxneighbor):\n",
    "            # get random current medoid that is to be replaced\n",
    "            current_medoid_index = self.__current[random.randint(0, self.__number_clusters - 1)];\n",
    "            current_medoid_cluster_index = self.__belong[current_medoid_index];\n",
    "            \n",
    "            # get new candidate to be medoid\n",
    "            candidate_medoid_index = random.randint(0, len(self.__pointer_data) - 1);\n",
    "            \n",
    "            while (candidate_medoid_index in self.__current):\n",
    "                candidate_medoid_index = random.randint(0, len(self.__pointer_data) - 1);\n",
    "            \n",
    "            candidate_cost = 0.0;\n",
    "            for point_index in range(0, len(self.__pointer_data)):\n",
    "                if (point_index not in self.__current):\n",
    "                    # get non-medoid point and its medoid\n",
    "                    point_cluster_index = self.__belong[point_index];\n",
    "                    point_medoid_index = self.__current[point_cluster_index];\n",
    "                    \n",
    "                    # get other medoid that is nearest to the point (except current and candidate)\n",
    "                    other_medoid_index = self.__find_another_nearest_medoid(point_index, current_medoid_index);\n",
    "                    other_medoid_cluster_index = self.__belong[other_medoid_index];\n",
    "                    \n",
    "                    # for optimization calculate all required distances\n",
    "                    # from the point to current medoid\n",
    "                    \n",
    "                    distance_current = self.__update_DTW_Matrix(point_index, current_medoid_index);\n",
    "                    \n",
    "                    # from the point to candidate medoid\n",
    "                    \n",
    "                    distance_candidate = self.__update_DTW_Matrix(point_index, candidate_medoid_index);\n",
    "                    \n",
    "                    # from the point to nearest medoid (except current and candidate)\n",
    "                  \n",
    "                    distance_nearest = self.__update_DTW_Matrix(point_index, other_medoid_index);\n",
    "                    \n",
    "                    # apply rules for cost calculation\n",
    "                    if (point_cluster_index == current_medoid_cluster_index):\n",
    "                        # case 1:\n",
    "                        if (distance_candidate >= distance_nearest):\n",
    "                            candidate_cost += distance_nearest - distance_current;\n",
    "                        \n",
    "                        # case 2:\n",
    "                        else:\n",
    "                            candidate_cost += distance_candidate - distance_current;\n",
    "                    \n",
    "                    elif (point_cluster_index == other_medoid_cluster_index):\n",
    "                        # case 3 ('nearest medoid' is the representative object of that cluster and object is more similar to 'nearest' than to 'candidate'):\n",
    "                        if (distance_candidate > distance_nearest):\n",
    "                            pass;\n",
    "                        \n",
    "                        # case 4:\n",
    "                        else:\n",
    "                            candidate_cost += distance_candidate - distance_nearest;\n",
    "            \n",
    "            if (candidate_cost < 0):\n",
    "                # set candidate that has won\n",
    "                self.__current[current_medoid_cluster_index] = candidate_medoid_index;\n",
    "                \n",
    "                # recalculate clusters\n",
    "                self.__update_clusters(self.__current);\n",
    "                \n",
    "                # reset iterations and starts investigation from the begining\n",
    "                index_neighbor = 0;\n",
    "                \n",
    "            else:\n",
    "                index_neighbor += 1;\n",
    "    \n",
    "    # Error : index_medoid instead of current_medoid_index\n",
    "    \n",
    "    def __find_another_nearest_medoid(self, point_index, current_medoid_index):\n",
    "        \"\"\"!\n",
    "        @brief Finds the another nearest medoid for the specified point that is differ from the specified medoid. \n",
    "        \n",
    "        @param[in] point_index: index of point in dataspace for that searching of medoid in current list of medoids is perfomed.\n",
    "        @param[in] current_medoid_index: index of medoid that shouldn't be considered as a nearest.\n",
    "        \n",
    "        @return (uint) index of the another nearest medoid for the point.\n",
    "        \n",
    "        \"\"\"\n",
    "        other_medoid_index = -1;\n",
    "        other_distance_nearest = float('inf');\n",
    "        for index_medoid in self.__current:\n",
    "            if (index_medoid != current_medoid_index):\n",
    "                \n",
    "                other_distance_candidate = self.__update_DTW_Matrix(point_index, index_medoid);\n",
    "                \n",
    "                if (other_distance_candidate < other_distance_nearest):\n",
    "                    other_distance_nearest = other_distance_candidate;\n",
    "                    other_medoid_index = index_medoid;\n",
    "        \n",
    "        return other_medoid_index;\n",
    "    \n",
    "    \n",
    "    def __calculate_estimation(self):\n",
    "        \"\"\"!\n",
    "        @brief Calculates estimation (cost) of the current clusters. The lower the estimation,\n",
    "               the more optimally configuration of clusters.\n",
    "        \n",
    "        @return (double) estimation of current clusters.\n",
    "        \n",
    "        \"\"\"\n",
    "        estimation = 0.0;\n",
    "        for index_cluster in range(len(self.__clusters)):\n",
    "            cluster = self.__clusters[index_cluster];\n",
    "            index_medoid = self.__current[index_cluster];\n",
    "            for index_point in cluster:\n",
    "                \n",
    "                estimation += self.__update_DTW_Matrix(index_point, index_medoid);\n",
    "        \n",
    "        return estimation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use a very large dataset ( > 20000 datas), we will use a CLARA algorithm using CLARANS on samples instead of PAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors, weights_mul, global_constraints, band_rad):\n",
    "    \n",
    "    #Initialisation \n",
    "    optimal_medoids = [];\n",
    "    optimal_estimation = float('inf');\n",
    "    optimal_compacity_clusters = [];\n",
    "    \n",
    "    for _ in range(5):\n",
    "        random_sample = random.sample(range(len(time_series)), sample_size)\n",
    "                \n",
    "        # CLARANS on sampled data\n",
    "        time_series_sampled = [time_series[i] for i in random_sample]\n",
    "        clarans_instance = clarans_DTW(time_series_sampled, number_clusters, iterations, maxneighbors, weights_mul, global_constraints, band_rad);\n",
    "        clarans_instance.process();\n",
    "        \n",
    "        # Estimation on all data_sets\n",
    "        optimal_sampled_medoids = clarans_instance.get_medoids();\n",
    "        optimal_sampled_medoid_index = [random_sample[i] for i in optimal_sampled_medoids]\n",
    "        clusters = update_clusters(time_series, optimal_sampled_medoid_index, weights_mul, global_constraints, band_rad)[0]\n",
    "        estimation, compacity_clusters = calculate_estimation(time_series, clusters, optimal_sampled_medoid_index, weights_mul, global_constraints, band_rad)\n",
    "        if(estimation < optimal_estimation):\n",
    "            optimal_estimation = estimation\n",
    "            optimal_medoids = optimal_sampled_medoid_index\n",
    "            optimal_compacity_clusters = compacity_clusters\n",
    "    clusters = update_clusters(time_series, optimal_medoids, weights_mul, global_constraints, band_rad)[0]\n",
    "    return clusters, optimal_medoids, optimal_estimation, optimal_compacity_clusters \n",
    "    \n",
    "def calculate_estimation(time_series, clusters, medoid, weights_mul, global_constraints, band_rad):\n",
    "    estimation = 0.0;\n",
    "    compacity_clusters = [];\n",
    "    for index_cluster in range(len(clusters)):\n",
    "        compacity_cluster = 0.0;\n",
    "        cluster = clusters[index_cluster];\n",
    "        index_medoid = medoid[index_cluster];\n",
    "        for index_point in cluster:\n",
    "            DTW_distance = update_DTW_Matrix(time_series, index_point, index_medoid, weights_mul, global_constraints, band_rad);\n",
    "            estimation += DTW_distance\n",
    "            compacity_cluster += DTW_distance\n",
    "        compacity_clusters = compacity_clusters + [compacity_cluster/float(len(cluster))]\n",
    "    return estimation, compacity_clusters;\n",
    "\n",
    "def update_clusters(time_series, medoids, weights_mul, global_constraints, band_rad):\n",
    "\n",
    "    belong = [0] * len(time_series);\n",
    "    clusters = [[] for i in range(len(medoids))];\n",
    "    for index_point in range(len(time_series)):\n",
    "        index_optim = -1;\n",
    "        dist_optim = 0.0;\n",
    "        \n",
    "        for index in range(len(medoids)):\n",
    "            dist = update_DTW_Matrix(time_series, index_point, medoids[index], weights_mul, global_constraints, band_rad);\n",
    "            if ((dist < dist_optim) or (index is 0)):\n",
    "                index_optim = index;\n",
    "                dist_optim = dist;\n",
    "        clusters[index_optim].append(index_point);\n",
    "        belong[index_point] = index_optim;\n",
    "    # If cluster is not able to capture object it should be removed\n",
    "    clusters = [cluster for cluster in clusters if len(cluster) > 0];\n",
    "    return clusters, belong\n",
    "\n",
    "def update_DTW_Matrix(time_series,index_point1,index_point2, weights_mul, global_constraints, band_rad):\n",
    "    D = librosa.dtw(time_series[index_point1], time_series[index_point2], weights_mul= weights_mul, global_constraints= global_constraints, band_rad = band_rad)[0]\n",
    "    dist = D[len(time_series[index_point1])-1,len(time_series[index_point2])-1]\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of matrix of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time_series data\n",
    "size_dataset_time_series = len(selected_pitch_contours)\n",
    "\n",
    "time_series = []\n",
    "per_none_interpo = []\n",
    "start_expiration = []\n",
    "duration_expiration = []\n",
    "audio_file_path = []\n",
    "\n",
    "for i in range(size_dataset_time_series):\n",
    "    time_series = time_series + [selected_pitch_contours.iloc[i]['pitch_contour']]\n",
    "    per_none_interpo = per_none_interpo + [selected_pitch_contours.iloc[i]['per_none_interpo']]\n",
    "    start_expiration = start_expiration + [selected_pitch_contours.iloc[i]['start']]  \n",
    "    duration_expiration = duration_expiration + [selected_pitch_contours.iloc[i]['duration']]\n",
    "    audio_file_path = audio_file_path + [selected_pitch_contours.iloc[i]['audio_file_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot histogram of all duration : 80% between 0 and 1 second\n",
    "\n",
    "n, bins, patches = plt.hist(duration_expiration, bins = 'auto');\n",
    "print('Number of Expiration : ' + str(n[0:20]))\n",
    "print('Bins of duration : ' + str(bins[0:21]))\n",
    "plt.title(\"Histogram of duration of expiration\");\n",
    "plt.xlabel(\"Value of duration (in ms)\");\n",
    "plt.xlim([0,1500])\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering for different k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 2\n",
    "sample_size = 1000\n",
    "weights_mul = np.array([2, 1, 1]) \n",
    "global_constraints = True\n",
    "band_rad = 0.1\n",
    "\n",
    "number_clusters = 2\n",
    "maxneighbors = int(round(0.0125*number_clusters*(sample_size-number_clusters)))\n",
    "\n",
    "# CLARANS algorithm k = 2\n",
    "\n",
    "start = time.time()\n",
    "clusters1, optimal_medoids1, optimal_estimation1, compacity_clusters1 = clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors,weights_mul,global_constraints,band_rad);\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_clusters = 3\n",
    "maxneighbors = int(round(0.0125*number_clusters*(sample_size-number_clusters)))\n",
    "\n",
    "# CLARANS algorithm k = 3\n",
    "\n",
    "start = time.time()\n",
    "clusters2, optimal_medoids2, optimal_estimation2, compacity_clusters2 = clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors,weights_mul,global_constraints,band_rad);\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_clusters = 4\n",
    "maxneighbors = int(round(0.0125*number_clusters*(sample_size-number_clusters)))\n",
    "\n",
    "# CLARANS algorithm k = 4\n",
    "\n",
    "start = time.time()\n",
    "clusters3, optimal_medoids3, optimal_estimation3, compacity_clusters3 = clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors,weights_mul,global_constraints,band_rad);\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_clusters = 5\n",
    "maxneighbors = int(round(0.0125*number_clusters*(sample_size-number_clusters)))\n",
    "\n",
    "# CLARANS algorithm k = 5\n",
    "\n",
    "start = time.time()\n",
    "clusters4, optimal_medoids4, optimal_estimation4, compacity_clusters4 = clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors,weights_mul,global_constraints,band_rad);\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_clusters = 6\n",
    "maxneighbors = int(round(0.0125*number_clusters*(sample_size-number_clusters)))\n",
    "\n",
    "# CLARANS algorithm k = 6\n",
    "\n",
    "start = time.time()\n",
    "clusters5, optimal_medoids5, optimal_estimation5, compacity_clusters5 = clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors,weights_mul,global_constraints,band_rad);\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_clusters = 7\n",
    "maxneighbors = int(round(0.0125*number_clusters*(sample_size-number_clusters)))\n",
    "\n",
    "# CLARANS algorithm k = 7\n",
    "\n",
    "start = time.time()\n",
    "clusters6, optimal_medoids6, optimal_estimation6, compacity_clusters6 = clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors,weights_mul,global_constraints,band_rad);\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_clusters = 8\n",
    "maxneighbors = int(round(0.0125*number_clusters*(sample_size-number_clusters)))\n",
    "\n",
    "# CLARANS algorithm k = 8\n",
    "\n",
    "start = time.time()\n",
    "clusters7, optimal_medoids7, optimal_estimation7, compacity_clusters7 = clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors,weights_mul,global_constraints,band_rad);\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_clusters = 9\n",
    "maxneighbors = int(round(0.0125*number_clusters*(sample_size-number_clusters)))\n",
    "\n",
    "# CLARANS algorithm k = 9\n",
    "\n",
    "start = time.time()\n",
    "clusters8, optimal_medoids8, optimal_estimation8, compacity_clusters8 = clara_clarans_DTW(time_series, number_clusters, sample_size, iterations, maxneighbors,weights_mul,global_constraints,band_rad);\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_clusters = {}\n",
    "all_clusters['clusters1'] = clusters1\n",
    "all_clusters['clusters2'] = clusters2\n",
    "all_clusters['clusters3'] = clusters3\n",
    "all_clusters['clusters4'] = clusters4\n",
    "all_clusters['clusters5'] = clusters5\n",
    "all_clusters['clusters6'] = clusters6\n",
    "all_clusters['clusters7'] = clusters7\n",
    "all_clusters['clusters8'] = clusters8\n",
    "\n",
    "np.save('all_clusters.npy', all_clusters)\n",
    "\n",
    "all_optimal_medoids = {}\n",
    "all_optimal_medoids['optimal_medoids1'] = optimal_medoids1\n",
    "all_optimal_medoids['optimal_medoids2'] = optimal_medoids2\n",
    "all_optimal_medoids['optimal_medoids3'] = optimal_medoids3\n",
    "all_optimal_medoids['optimal_medoids4'] = optimal_medoids4\n",
    "all_optimal_medoids['optimal_medoids5'] = optimal_medoids5\n",
    "all_optimal_medoids['optimal_medoids6'] = optimal_medoids6\n",
    "all_optimal_medoids['optimal_medoids7'] = optimal_medoids7\n",
    "all_optimal_medoids['optimal_medoids8'] = optimal_medoids8\n",
    "\n",
    "np.save('all_optimal_medoids.npy', all_optimal_medoids)\n",
    "\n",
    "all_compacity_clusters = {}\n",
    "all_compacity_clusters['compacity_clusters1'] = compacity_clusters1\n",
    "all_compacity_clusters['compacity_clusters2'] = compacity_clusters2\n",
    "all_compacity_clusters['compacity_clusters3'] = compacity_clusters3\n",
    "all_compacity_clusters['compacity_clusters4'] = compacity_clusters4\n",
    "all_compacity_clusters['compacity_clusters5'] = compacity_clusters5\n",
    "all_compacity_clusters['compacity_clusters6'] = compacity_clusters6\n",
    "all_compacity_clusters['compacity_clusters7'] = compacity_clusters7\n",
    "all_compacity_clusters['compacity_clusters8'] = compacity_clusters8\n",
    "\n",
    "np.save('all_compacity_clusters.npy', all_compacity_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now choose k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Search for elbow point\n",
    "\n",
    "axis = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "optimal_estimation1 = 52296208.599524878\n",
    "optimal_estimation2 = 45948909.03598284\n",
    "optimal_estimation3 = 44192257.60516759\n",
    "optimal_estimation4 = 41114328.305629186\n",
    "optimal_estimation5 = 39954813.372435041\n",
    "optimal_estimation6 = 37958668.298182145\n",
    "optimal_estimation7 = 37231241.40754097\n",
    "optimal_estimation8 = 36079329.843272269\n",
    "\n",
    "y = [optimal_estimation1, optimal_estimation2, optimal_estimation3, optimal_estimation4, optimal_estimation5, optimal_estimation6, optimal_estimation7, optimal_estimation8]\n",
    "\n",
    "plt.plot(axis, y)\n",
    "\n",
    "title('Estimation (cost) of the current clusters in function of the Number of clusters');\n",
    "plt.xlabel('Number of clusters');\n",
    "plt.ylabel('Estimation (cost) of the current clusters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relative size of clusters for different k\n",
    "\n",
    "all_clusters = np.load('all_clusters.npy').item()\n",
    "all_compacity_clusters = np.load('all_compacity_clusters.npy').item()\n",
    "\n",
    "all_relative_clusters_size_2dec = {}\n",
    "\n",
    "for k in range(8):\n",
    "    clusters = all_clusters['clusters'+str(k+1)]\n",
    "    compacity_clusters = all_compacity_clusters['compacity_clusters'+str(k+1)] \n",
    "    relative_clusters_size = []\n",
    "    for j in range(k+2):\n",
    "        relative_clusters_size = relative_clusters_size + [(len(clusters[j])/float(size_dataset_time_series))*100]\n",
    "    print(\"--------------------\")\n",
    "    print(\"Relative size of clusters for k = \" + str(k+2) + \" in % :\")\n",
    "    relative_clusters_size_2dec = [\"{0:0.2f}\".format(x) for x in relative_clusters_size]\n",
    "    all_relative_clusters_size_2dec['relative_clusters'+str(k+1)] = relative_clusters_size_2dec \n",
    "    print(str(relative_clusters_size_2dec))  \n",
    "    #\n",
    "    print(\"Compacity of clusters for k = \" + str(k+2) + ' :')\n",
    "    compacity_clusters = [\"{0:0.2f}\".format(x) for x in compacity_clusters]\n",
    "    print(str(compacity_clusters))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing k = 8, we now plot medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot medoids for one number of clusters\n",
    "\n",
    "number_clusters = 8\n",
    "\n",
    "HOP_SIZE = 440 # Swipe hop_size\n",
    "FS = 44100\n",
    "\n",
    "all_optimal_medoids = np.load('all_optimal_medoids.npy').item()\n",
    "optimal_medoids = all_optimal_medoids['optimal_medoids'+str(number_clusters-1)]\n",
    "\n",
    "relative_clusters_size_2dec = all_relative_clusters_size_2dec['relative_clusters'+str(number_clusters-1)]\n",
    "compacity_clusters = all_compacity_clusters['compacity_clusters'+str(number_clusters-1)]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 18)\n",
    "\n",
    "# Plot all the medoids\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for j in range(number_clusters):\n",
    "    if (j < int(j/2)):\n",
    "        ax = plt.subplot(number_clusters,1,j+1)\n",
    "    else:\n",
    "        ax = plt.subplot(number_clusters,2,j+1)\n",
    "    #\n",
    "    axis = map(float,range(len(time_series[optimal_medoids[j]])))\n",
    "    axis = [x*float(HOP_SIZE)/FS for x in axis]\n",
    "    ax.plot(axis,time_series[optimal_medoids[j]]);\n",
    "    #\n",
    "    title('Medoids (per_none_inter = ' + \"{0:0.2f}\".format(per_none_interpo[optimal_medoids[j]]) + ') of cluster ' + str(j+1) + ' (relative size of cluster = ' + str(relative_clusters_size_2dec[j]) + ' % and compacity = ' + \"{0:0.2f}\".format(compacity_clusters[j]) + ')')\n",
    "    plt.ylabel('Frequency (in Cents)')\n",
    "    plt.xlabel('time (in s)')\n",
    "    ax.set_ylim([-500, 500])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot corresponding histograms for each clusters\n",
    "\n",
    "all_clusters = np.load('all_clusters.npy').item()\n",
    "clusters = all_clusters['clusters'+str(number_clusters-1)]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 18)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for j in range(number_clusters):\n",
    "    if (j < int(j/2)):\n",
    "        plt.subplot(number_clusters,1,j+1)\n",
    "    else:\n",
    "        plt.subplot(number_clusters,2,j+1)\n",
    "    duration_clusters = [duration_expiration[x] for x in clusters[j]]\n",
    "    plt.hist(duration_clusters, bins = 'auto');\n",
    "    plt.title(\"Histogram of duration of expiration\" + ') of cluster ' + str(j+1) + ' (relative size of cluster = ' + str(relative_clusters_size_2dec[j]) + ' % and compacity = ' + \"{0:0.2f}\".format(compacity_clusters[j]) + ')');\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel(\"Value of duration (in ms)\");\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now examine tendencies inside clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to plot all the figures of a cluster on the same plot\n",
    "\n",
    "number_clusters = 7\n",
    "index_cluster = 0\n",
    "\n",
    "clusters = all_clusters['clusters'+str(number_clusters-1)]\n",
    "cluster = clusters[index_cluster]\n",
    "\n",
    "optimal_medoids = all_optimal_medoids['optimal_medoids'+str(number_clusters-1)]\n",
    "\n",
    "len_max_signal = 0\n",
    "\n",
    "for j in range(len(cluster)):\n",
    "    if (len_max_signal < len(time_series[cluster[j]])):\n",
    "        len_max_signal = len(time_series[cluster[j]])\n",
    "\n",
    "resampled_cluster = []\n",
    "\n",
    "for j in range(len(cluster)):\n",
    "    resampled_cluster = resampled_cluster + [scipy.signal.resample(time_series[cluster[j]], len_max_signal)]\n",
    "\n",
    "ax = sb.tsplot(data = np.array(resampled_cluster), err_style=\"unit_traces\")\n",
    "\n",
    "medoid = scipy.signal.resample(time_series[optimal_medoids[index_cluster]], len_max_signal)\n",
    "\n",
    "ax = sb.tsplot(data = np.array(medoid), color = 'w')\n",
    "\n",
    "plt.title(\"All pitch contour\" + ' of cluster ' + str(index_cluster+1) + ' (relative size of cluster = ' + str(relative_clusters_size_2dec[index_cluster]) + ' % and compacity = ' + \"{0:0.2f}\".format(compacity_clusters[index_cluster]) + ')');\n",
    "plt.ylabel('Frequency (in cents)')\n",
    "plt.xlabel(\"Normalized value of duration (in ms)\");\n",
    "ax.set_ylim([-1000,1000]);\n",
    "sb.set_style('whitegrid') #make whitegrid\n",
    "sb.set_style('ticks') #clean ticks (useful when exporting to pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to plot all the figures of a cluster on the same plot\n",
    "\n",
    "number_clusters = 8\n",
    "\n",
    "all_optimal_medoids = np.load('all_optimal_medoids.npy').item()\n",
    "all_clusters = np.load('all_clusters.npy').item()\n",
    "\n",
    "clusters = all_clusters['clusters'+str(number_clusters-1)]\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "optimal_medoids = all_optimal_medoids['optimal_medoids'+str(number_clusters-1)]\n",
    "\n",
    "relative_clusters_size_2dec = all_relative_clusters_size_2dec['relative_clusters'+str(number_clusters-1)]\n",
    "compacity_clusters = all_compacity_clusters['compacity_clusters'+str(number_clusters-1)]\n",
    "\n",
    "for k in range(number_clusters):\n",
    "        \n",
    "    cluster = clusters[k]\n",
    "\n",
    "    len_max_signal = 0\n",
    "\n",
    "    for j in range(len(cluster)):\n",
    "        if (len_max_signal < len(time_series[cluster[j]])):\n",
    "            len_max_signal = len(time_series[cluster[j]])\n",
    "\n",
    "    resampled_cluster = []\n",
    "\n",
    "    for j in range(len(cluster)):\n",
    "        resampled_cluster = resampled_cluster + [scipy.signal.resample(time_series[cluster[j]], len_max_signal)]\n",
    "    \n",
    "    if (k < int(k/2)):\n",
    "        plt.subplot(number_clusters,1,k+1)\n",
    "    else:\n",
    "        plt.subplot(number_clusters,2,k+1)\n",
    "    \n",
    "    ax = sb.tsplot(data = np.array(resampled_cluster), err_style=\"unit_traces\")\n",
    "    medoid = scipy.signal.resample(time_series[optimal_medoids[k]], len_max_signal)\n",
    "    ax = sb.tsplot(data = np.array(medoid), color = 'w')\n",
    "    \n",
    "    plt.title(\"All pitch contour\" + ' of cluster ' + str(k+1) + ' (relative size of cluster = ' + str(relative_clusters_size_2dec[k]) + ' % and compacity = ' + \"{0:0.2f}\".format(compacity_clusters[k]) + ')');\n",
    "    plt.ylabel('Frequency (in cents)')\n",
    "    plt.xlabel(\"Normalized value of duration (in ms)\");\n",
    "    ax.set_ylim([-1000,1000]);\n",
    "    sb.set_style('whitegrid') #make whitegrid\n",
    "    sb.set_style('ticks') #clean ticks (useful when exporting to pdf)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot random series in a cluster and hear the corresponding cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot one medoids and random time series in his clusters\n",
    "\n",
    "number_clusters = 8\n",
    "\n",
    "HOP_SIZE = 440 # Swipe hop_size\n",
    "FS = 44100\n",
    "\n",
    "all_optimal_medoids = np.load('all_optimal_medoids.npy').item()\n",
    "optimal_medoids = all_optimal_medoids['optimal_medoids'+str(number_clusters-1)]\n",
    "\n",
    "relative_clusters_size_2dec = all_relative_clusters_size_2dec['relative_clusters'+str(number_clusters-1)]\n",
    "compacity_clusters = all_compacity_clusters['compacity_clusters'+str(number_clusters-1)]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 14)\n",
    "\n",
    "# Plot the medoids chosen\n",
    "\n",
    "index_medoids = 3\n",
    "plt.figure()\n",
    "\n",
    "axis = map(float,range(len(time_series[optimal_medoids[index_medoids]])))\n",
    "axis = [x*float(HOP_SIZE)/FS for x in axis]\n",
    "ax = plt.subplot(6, 1, 1)\n",
    "ax.plot(axis,time_series[optimal_medoids[index_medoids]]);\n",
    "title('Medoids (per_none_inter = ' + \"{0:0.2f}\".format(per_none_interpo[optimal_medoids[index_medoids]]) + ') of cluster ' + str(index_medoids+1) + ' (relative size = ' + str(relative_clusters_size_2dec[index_medoids]) + ' % and compacity = ' + \"{0:0.2f}\".format(compacity_clusters[index_medoids]) + ')')\n",
    "plt.ylabel('Frequency (in Cents)')\n",
    "plt.xlabel('time (in s)')\n",
    "ax.set_ylim([-400, 400])\n",
    "\n",
    "# Plot random time series from his clusters\n",
    "\n",
    "all_clusters = np.load('all_clusters.npy').item()\n",
    "clusters = all_clusters['clusters'+str(number_clusters-1)]\n",
    "random_time_series = random.sample(clusters[index_medoids], 5)\n",
    "\n",
    "for j in range(5):\n",
    "    ax = plt.subplot(6, 1, j+2)\n",
    "    #\n",
    "    axis = map(float,range(len(time_series[random_time_series[j]])))\n",
    "    axis = [x*float(HOP_SIZE)/FS for x in axis]\n",
    "    ax.plot(axis,time_series[random_time_series[j]]);\n",
    "    ax.set_ylim([-400, 400])\n",
    "    #\n",
    "    title('Random time series number ' + str(j) + ' (per_none_inter = ' + \"{0:0.2f}\".format(per_none_interpo[random_time_series[j]]) + ') of clusters ' + str(index_medoids+1))\n",
    "    plt.ylabel('Frequency (in Cents)')\n",
    "    plt.xlabel('time (in s)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hearing the expiration phase corresponding to the chosen medois\n",
    "\n",
    "print('----- Phase of expiration corresponding to Medoids of cluster ' + str(index_medoids+1))\n",
    "\n",
    "audio_data = librosa.load(audio_file_path[optimal_medoids[index_medoids]], offset = start_expiration[optimal_medoids[index_medoids]]/float(1000), duration = duration_expiration[optimal_medoids[index_medoids]]/float(1000),sr = FS)[0]\n",
    "audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "IPython.display.Audio(audio_data, rate=FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hearing the random expiration corresponding to random time series number\n",
    "\n",
    "number_random_time_series = 0\n",
    "print('----- Phase of expiration corresponding to random time series number ' + str(number_random_time_series) + ' of cluster ' + str(index_medoids+1))\n",
    "audio_data = librosa.load(audio_file_path[random_time_series[number_random_time_series]], offset = start_expiration[random_time_series[number_random_time_series]]/float(1000), duration = duration_expiration[random_time_series[number_random_time_series]]/float(1000),sr = FS)[0]\n",
    "audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "IPython.display.Audio(audio_data, rate=FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_clusters = 8\n",
    "\n",
    "all_clusters = np.load('all_clusters.npy').item()\n",
    "\n",
    "clusters = all_clusters['clusters'+str(number_clusters-1)]\n",
    "\n",
    "class_pitch_contour = np.zeros(len(time_series))\n",
    "\n",
    "class_pitch_contour = [int(x) for x in class_pitch_contour]\n",
    "\n",
    "for k in range(len(clusters)):\n",
    "    cluster = clusters[k]\n",
    "    for j in range(len(cluster)):\n",
    "        class_pitch_contour[cluster[j]] = int(k)\n",
    "        \n",
    "# We drop cluster 0\n",
    "        \n",
    "selected_pitch_contours['class_pitch_contour'] = pd.Series(class_pitch_contour, index=selected_pitch_contours.index)\n",
    "\n",
    "classified_pitch_contours = selected_pitch_contours[['baby_id','sex','month','context','duration','pitch_contour','class_pitch_contour','audio_file_path']]\n",
    "\n",
    "classified_pitch_contours = classified_pitch_contours[classified_pitch_contours.class_pitch_contour != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified_pitch_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of distribution of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To convert '01m' to a int (too many degree of a liberty if we consider month as a categorical variable and not\n",
    "# a continuous variable)\n",
    "    \n",
    "def correct_baby_month(month):\n",
    "    return int(month[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month = ['00m','01m','02m','03m','04m','05m','06m','07m','08m','09m','10m','11m','12m','13m']\n",
    "\n",
    "month = range(8)\n",
    "\n",
    "context = ['hungry', 'pee', 'sleepy']\n",
    "\n",
    "baby_id = ['011','012','042','043','044','045','046','047','048','049','050','051','052','053','054','055','056','057','058','059','060','061','062','063','064','065','066','067','068','069','070','071','072']\n",
    "\n",
    "classified_pitch_contours['month']=classified_pitch_contours['month'].apply(correct_baby_month)\n",
    "classified_pitch_contours_design = classified_pitch_contours[classified_pitch_contours.month.isin(month)]\n",
    "classified_pitch_contours_design = classified_pitch_contours_design[classified_pitch_contours_design.context.isin(context)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified_pitch_contours_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baby_sex_data = pd.DataFrame.from_csv('baby_sex_data.csv',sep='\\t',index_col=None, header = None)\n",
    "baby_sex_data.columns = ['baby_id','sex']\n",
    "baby_sex_data['baby_id']=baby_sex_data['baby_id'].apply(correct_baby_id)\n",
    "\n",
    "sex = list(baby_sex_data.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distribution clusters in function of months\n",
    "\n",
    "baby_number = []\n",
    "sex_baby = []\n",
    "month_number = []\n",
    "class_pitch_contour = []\n",
    "frequency_class = []\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 18)\n",
    "\n",
    "# display the difference between sex if 1 else 0\n",
    "\n",
    "display_sex = 0\n",
    "\n",
    "for j in range(len(baby_id)):\n",
    "    for k in range(len(month)):\n",
    "        temp = classified_pitch_contours_design\n",
    "        #\n",
    "        temp = temp[temp.baby_id == baby_id[j]]\n",
    "        temp = temp[temp.month == month[k]]\n",
    "        temp_class_pitch_contours = list(temp.class_pitch_contour)\n",
    "        c = Counter(temp_class_pitch_contours)\n",
    "        sum_c = 0\n",
    "        for i in range(7):\n",
    "            sum_c += c[i+1]\n",
    "        if (sum_c == 0):\n",
    "            pourcentage = [NaN,NaN,NaN,NaN,NaN,NaN,NaN]\n",
    "        else:\n",
    "            pourcentage = [\"{0:0.2f}\".format(c[i+1]/float(sum_c)) for i in range(7)]\n",
    "        for i in range(7):\n",
    "            baby_number = baby_number + [baby_id[j]]\n",
    "            sex_baby = sex_baby + [sex[j]]\n",
    "            month_number = month_number + [month[k]]\n",
    "            class_pitch_contour = class_pitch_contour + [str(i+1)]\n",
    "            frequency_class = frequency_class + [float(pourcentage[i])]\n",
    "            \n",
    "distrib_culsters_month = pd.DataFrame({'baby_id' : baby_number,\\\n",
    "                                       'sex' : sex_baby,\\\n",
    "                                       'month' : month_number,\\\n",
    "                                       'number_of_cluster' : class_pitch_contour,\\\n",
    "                                       'frequency_of_class' : frequency_class\n",
    "                                      })\n",
    "\n",
    "distrib_culsters_month = distrib_culsters_month[['baby_id','sex','month','number_of_cluster','frequency_of_class']]\n",
    "\n",
    "distrib_culsters_month = distrib_culsters_month.dropna(axis = 0)\n",
    "\n",
    "#\n",
    "\n",
    "if (display_sex == 1):\n",
    "    g1 = sb.factorplot(x=\"number_of_cluster\", y=\"frequency_of_class\", hue=\"sex\", row=\"month\",row_order = month, data=distrib_culsters_month, kind=\"bar\", size=4, aspect=4);\n",
    "else:\n",
    "    g1 = sb.factorplot(x=\"number_of_cluster\", y=\"frequency_of_class\", row=\"month\",row_order = month, data=distrib_culsters_month, kind=\"bar\", size=4, aspect=4);\n",
    "\n",
    "#\n",
    "        \n",
    "if (display_sex == 1):\n",
    "    g2 = sb.factorplot(x=\"month\", order = month, y=\"frequency_of_class\", hue=\"sex\", row=\"number_of_cluster\", data=distrib_culsters_month, size=2, aspect=4);\n",
    "else:\n",
    "    g2 = sb.factorplot(x=\"month\", order = month, y=\"frequency_of_class\", row=\"number_of_cluster\", data=distrib_culsters_month, size=2, aspect=4);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distribution clusters in function of contexts\n",
    "\n",
    "baby_number = []\n",
    "sex_baby = []\n",
    "context_number = []\n",
    "class_pitch_contour = []\n",
    "frequency_class = []\n",
    "\n",
    "# display the difference between sex if 1 else 0\n",
    "\n",
    "display_sex = 0\n",
    "\n",
    "for j in range(len(baby_id)):\n",
    "    for k in range(len(context)):\n",
    "        temp = classified_pitch_contours\n",
    "        #\n",
    "        temp = temp[temp.baby_id == baby_id[j]]\n",
    "        temp = temp[temp.context == context[k]]\n",
    "        temp_class_pitch_contours = list(temp.class_pitch_contour)\n",
    "        c = Counter(temp_class_pitch_contours)\n",
    "        sum_c = 0\n",
    "        for i in range(7):\n",
    "            sum_c += c[i+1]\n",
    "        if (sum_c == 0):\n",
    "            pourcentage = [NaN,NaN,NaN,NaN,NaN,NaN,NaN]\n",
    "        else:\n",
    "            pourcentage = [\"{0:0.2f}\".format(c[i+1]/float(sum_c)) for i in range(7)]\n",
    "        for i in range(7):\n",
    "            baby_number = baby_number + [baby_id[j]]\n",
    "            sex_baby = sex_baby + [sex[j]]\n",
    "            context_number = context_number + [context[k]]\n",
    "            class_pitch_contour = class_pitch_contour + [str(i+1)]\n",
    "            frequency_class = frequency_class + [float(pourcentage[i])]\n",
    "            \n",
    "distrib_culsters_context = pd.DataFrame({'baby_id' : baby_number,\\\n",
    "                                       'sex' : sex_baby,\\\n",
    "                                       'context' : context_number,\\\n",
    "                                       'number_of_cluster' : class_pitch_contour,\\\n",
    "                                       'frequency_of_class' : frequency_class\n",
    "                                      })\n",
    "\n",
    "distrib_culsters_context = distrib_culsters_context[['baby_id','sex','context','number_of_cluster','frequency_of_class']]\n",
    "\n",
    "distrib_culsters_context = distrib_culsters_context.dropna(axis = 0)\n",
    "\n",
    "if (display_sex == 1):\n",
    "    g1 = sb.factorplot(x=\"number_of_cluster\", y=\"frequency_of_class\",hue=\"sex\", row=\"context\", data=distrib_culsters_context, kind=\"bar\", size=4, aspect=4);\n",
    "else:\n",
    "    g1 = sb.factorplot(x=\"number_of_cluster\", y=\"frequency_of_class\", row=\"context\", data=distrib_culsters_context, kind=\"bar\", size=4, aspect=4);\n",
    "\n",
    "#\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 36)\n",
    "        \n",
    "if (display_sex == 1):\n",
    "    g2 = sb.factorplot(x=\"context\", y=\"frequency_of_class\",hue=\"sex\", row=\"number_of_cluster\", data=distrib_culsters_context, size=2, aspect=4);\n",
    "else:\n",
    "    g2 = sb.factorplot(x=\"context\", y=\"frequency_of_class\", row=\"number_of_cluster\", data=distrib_culsters_context, size=2, aspect=4);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distribution clusters in function of months and contexts \n",
    "\n",
    "baby_number = []\n",
    "sex_baby = []\n",
    "month_number = []\n",
    "class_pitch_contour = []\n",
    "frequency_class = []\n",
    "context_name = []\n",
    "month_context_name = []\n",
    "\n",
    "x_order = []\n",
    "\n",
    "for j in range(len(month)):\n",
    "    for k in range(len(context)):\n",
    "        x_order = x_order + ['0' + str(month[j]) + 'm-' + context[k]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 18)\n",
    "\n",
    "#\n",
    "\n",
    "for j in range(len(baby_id)):\n",
    "    for k in range(len(month)):\n",
    "        for l in range(len(context)):\n",
    "            temp = classified_pitch_contours_design\n",
    "            #\n",
    "            temp = temp[temp.baby_id == baby_id[j]]\n",
    "            temp = temp[temp.month == month[k]]\n",
    "            temp = temp[temp.context == context[l]]\n",
    "            temp_class_pitch_contours = list(temp.class_pitch_contour)\n",
    "            c = Counter(temp_class_pitch_contours)\n",
    "            sum_c = 0\n",
    "            for i in range(7):\n",
    "                sum_c += c[i+1]\n",
    "            if (sum_c == 0):\n",
    "                pourcentage = [NaN,NaN,NaN,NaN,NaN,NaN,NaN]\n",
    "            else:\n",
    "                pourcentage = [\"{0:0.2f}\".format(c[i+1]/float(sum_c)) for i in range(7)]\n",
    "            for i in range(7):\n",
    "                baby_number = baby_number + [baby_id[j]]\n",
    "                sex_baby = sex_baby + [sex[j]]\n",
    "                month_number = month_number + [month[k]]\n",
    "                context_name = context_name + [context[l]]\n",
    "                class_pitch_contour = class_pitch_contour + [str(i+1)]\n",
    "                frequency_class = frequency_class + [float(pourcentage[i])]\n",
    "                month_context_name = month_context_name + ['0' + str(month[k]) + 'm-' + context[l]]\n",
    "            \n",
    "distrib_culsters_month_context = pd.DataFrame({'baby_id' : baby_number,\\\n",
    "                                       'sex' : sex_baby,\\\n",
    "                                       'month' : month_number,\\\n",
    "                                       'context' : context_name,\\\n",
    "                                       'month_and_context' : month_context_name,\\\n",
    "                                       'number_of_cluster' : class_pitch_contour,\\\n",
    "                                       'frequency_of_class' : frequency_class\n",
    "                                      })\n",
    "\n",
    "distrib_culsters_month_context = distrib_culsters_month_context[['baby_id','sex','month','context','month_and_context','number_of_cluster','frequency_of_class']]\n",
    "\n",
    "distrib_culsters_month_context = distrib_culsters_month_context.dropna(axis = 0)\n",
    "\n",
    "g1h = sb.factorplot(x=\"number_of_cluster\", y=\"frequency_of_class\", row=\"month\", row_order = month, col = \"context\", col_order = context,data=distrib_culsters_month_context, kind=\"bar\",size=8, aspect=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 30)\n",
    "\n",
    "for k in range(7):\n",
    "    temp = distrib_culsters_month_context[distrib_culsters_month_context.number_of_cluster == str(k+1)]\n",
    "    temp = temp.groupby(['month', 'context'])['frequency_of_class'].mean().reset_index()\n",
    "    temp = temp.pivot(\"month\", \"context\", \"frequency_of_class\")\n",
    "    plt.subplot(7, 2, k+1)\n",
    "    ax = sb.heatmap(temp, vmin = 0, vmax = 0.5, annot=True)\n",
    "    title('Distribution of clusters ' + str(k+1) + ' of male')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distribution clusters in function of months and contexts for male only\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 18)\n",
    "\n",
    "distrib_culsters_month_contextm = distrib_culsters_month_context[distrib_culsters_month_context.sex == 'M']\n",
    "\n",
    "distrib_culsters_month_contextf = distrib_culsters_month_context[distrib_culsters_month_context.sex == 'F']\n",
    "\n",
    "g1h = sb.factorplot(x=\"number_of_cluster\", y=\"frequency_of_class\", row=\"month\", row_order = month, col = \"context\", col_order = context,data=distrib_culsters_month_contextm, kind=\"bar\",size=8, aspect=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 30)\n",
    "\n",
    "for k in range(7):\n",
    "    temp = distrib_culsters_month_contextm[distrib_culsters_month_contextm.number_of_cluster == str(k+1)]\n",
    "    temp = temp.groupby(['month', 'context'])['frequency_of_class'].mean().reset_index()\n",
    "    temp = temp.pivot(\"month\", \"context\", \"frequency_of_class\")\n",
    "    plt.subplot(7, 2, k+1)\n",
    "    ax = sb.heatmap(temp, vmin = 0, vmax = 0.5, annot=True)\n",
    "    title('Distribution of clusters ' + str(k+1) + ' of male')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distribution clusters in function of months and contexts for Female only\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 18)\n",
    "\n",
    "g1f = sb.factorplot(x=\"number_of_cluster\", y=\"frequency_of_class\", row=\"month\", row_order = month, col = \"context\",data=distrib_culsters_month_contextf, kind=\"bar\",size=8, aspect=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 30)\n",
    "\n",
    "for k in range(7):\n",
    "    temp = distrib_culsters_month_contextf[distrib_culsters_month_contextf.number_of_cluster == str(k+1)]\n",
    "    temp = temp.groupby(['month', 'context'])['frequency_of_class'].mean().reset_index()\n",
    "    temp = temp.pivot(\"month\", \"context\", \"frequency_of_class\")\n",
    "    plt.subplot(7, 2, k+1)\n",
    "    ax = sb.heatmap(temp, vmin = 0, vmax = 0.5, annot=True)\n",
    "    title('Distribution of clusters ' + str(k+1) + ' of female')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply LME to see if month and context are  statistically significance for class pitch contour.\n",
    "As lme4 only estimate logistic regression model, we estimate a logistic model for each case. For this, we reduce the dataset to only two class_pitch_contour possible (0 is one class, 1 all the others).\n",
    "It is made possible by begg and gray approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same design as design 2 => all babies, 8 months, 3 contexts\n",
    "\n",
    "def define_two_class_pitch(class_pitch_contour, choice_classe):\n",
    "    if(class_pitch_contour == choice_classe):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "def function_glme_binomial1(choice_classe,classified_pitch_contours_design):\n",
    "    temp_design = classified_pitch_contours_design.copy()    \n",
    "    temp_design = temp_design[['baby_id','sex','month','context','class_pitch_contour']]\n",
    "    temp_design['class_pitch_contour'] = temp_design['class_pitch_contour'].apply(define_two_class_pitch, args=(choice_classe,))\n",
    "    rdf_lme=pandas2ri.py2ri(temp_design)\n",
    "\n",
    "    # print(R.summary(rdf_lme))\n",
    "\n",
    "    # Case ANOVA\n",
    "\n",
    "    fml_nul = Formula(\"class_pitch_contour ~ 1 + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v0 = Formula(\"class_pitch_contour ~ month + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v1 = Formula(\"class_pitch_contour ~ context + month + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v2 = Formula(\"class_pitch_contour ~ month * context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "\n",
    "    nul = lme4.glmer(fml_nul, data = rdf_lme, family = 'binomial')\n",
    "    v0 = lme4.glmer(fml_v0, data = rdf_lme, family = 'binomial') \n",
    "    v1 = lme4.glmer(fml_v1, data = rdf_lme, family = 'binomial') \n",
    "    v2 = lme4.glmer(fml_v2, data = rdf_lme, family = 'binomial')\n",
    "\n",
    "    print(R.anova(nul,v0,v1,v2))\n",
    "\n",
    "    \n",
    "def function_glme_binomial2(choice_classe,classified_pitch_contours_design):\n",
    "    temp_design = classified_pitch_contours_design.copy()    \n",
    "    temp_design = temp_design[['baby_id','sex','month','context','class_pitch_contour']]\n",
    "    temp_design['class_pitch_contour'] = temp_design['class_pitch_contour'].apply(define_two_class_pitch, args=(choice_classe,))\n",
    "    rdf_lme=pandas2ri.py2ri(temp_design)\n",
    "\n",
    "    # print(R.summary(rdf_lme))\n",
    "\n",
    "    # Case ANOVA\n",
    "\n",
    "    fml_nul = Formula(\"class_pitch_contour ~ 1 + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v0 = Formula(\"class_pitch_contour ~ sex + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v1 = Formula(\"class_pitch_contour ~ sex + month + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v2 = Formula(\"class_pitch_contour ~ sex + month + context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v31 = Formula(\"class_pitch_contour ~ sex * month + context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v32 = Formula(\"class_pitch_contour ~ sex * context + month + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v33 = Formula(\"class_pitch_contour ~ sex + month * context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "    fml_v4 = Formula(\"class_pitch_contour ~ sex * month * context + (1 | baby_id) + (0 + month * context | baby_id)\")\n",
    "\n",
    "    nul = lme4.glmer(fml_nul, data = rdf_lme, family = 'binomial')\n",
    "    v0 = lme4.glmer(fml_v0, data = rdf_lme, family = 'binomial') \n",
    "    v1 = lme4.glmer(fml_v1, data = rdf_lme, family = 'binomial') \n",
    "    v2 = lme4.glmer(fml_v2, data = rdf_lme, family = 'binomial')\n",
    "    v31 = lme4.glmer(fml_v31, data = rdf_lme, family = 'binomial')\n",
    "    v32 = lme4.glmer(fml_v32, data = rdf_lme, family = 'binomial')\n",
    "    v33 = lme4.glmer(fml_v33, data = rdf_lme, family = 'binomial')\n",
    "    v4 = lme4.glmer(fml_v4, data = rdf_lme, family = 'binomial')\n",
    "\n",
    "    print(R.anova(nul,v0,v1,v2,v31,v4))\n",
    "    print(R.anova(nul,v0,v1,v2,v32,v4))\n",
    "    print(R.anova(nul,v0,v1,v2,v33,v4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "function_glme_binomial1(1,classified_pitch_contours_design)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "start = time.time()\n",
    "function_glme_binomial2(1,classified_pitch_contours_design)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('-----1-----')\n",
    "start = time.time()\n",
    "function_glme_binomial1(1,classified_pitch_contours_design)\n",
    "end = time.time()\n",
    "print('-----------')\n",
    "\n",
    "print('-----2-----')\n",
    "function_glme_binomial1(2,classified_pitch_contours_design)\n",
    "print('-----------')\n",
    "\n",
    "print('-----3-----')\n",
    "function_glme_binomial1(3,classified_pitch_contours_design)\n",
    "print('-----------')\n",
    "\n",
    "print('-----4-----')\n",
    "function_glme_binomial1(4,classified_pitch_contours_design)\n",
    "print('-----------')\n",
    "\n",
    "print('-----5-----')\n",
    "function_glme_binomial1(5,classified_pitch_contours_design)\n",
    "print('-----------')\n",
    "\n",
    "print('-----7-----')\n",
    "function_glme_binomial1(7,classified_pitch_contours_design)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('-----1-----')\n",
    "start = time.time()\n",
    "function_glme_binomial2(1,classified_pitch_contours_design)\n",
    "end = time.time()\n",
    "print('-----------')\n",
    "\n",
    "print('-----2-----')\n",
    "function_glme_binomial2(2,classified_pitch_contours_design)\n",
    "print('-----------')\n",
    "\n",
    "print('-----3-----')\n",
    "function_glme_binomial2(3,classified_pitch_contours_design)\n",
    "print('-----------')\n",
    "\n",
    "print('-----4-----')\n",
    "function_glme_binomial2(4,classified_pitch_contours_design)\n",
    "print('-----------')\n",
    "\n",
    "print('-----5-----')\n",
    "function_glme_binomial2(5,classified_pitch_contours_design)\n",
    "print('-----------')\n",
    "\n",
    "print('-----7-----')\n",
    "function_glme_binomial2(7,classified_pitch_contours_design)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment : free sorting experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Perceptual validation of pitch contour clustering </h1>\n",
    "\n",
    "This notebook is used to analyse results from a Tcl-LabX free sort experiment (2017-7) in which participants grouped  35 short baby cries (5 instances from 7 computationally generated clusters) into an arbitrary number of clusters corresponding to their perceptual similarity. Individual sorts are aggregated into a global co-occurrence matrix, which is then subjected to hierarchical clustering. We then evaluate whether computationally generated clusters distribute coherently with these perceptual clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1) Parse result files</h4>\n",
    "\n",
    "Data collected in each configuration correspond to the same set of stimuli, only presented in random order. In order to collate stimuli co-occurrences across participants, we need to give a unique id to each stimulus. To do so, we first build a list of the sound files (nb_stim=35), as a pd.Dataframe, by scanning the listing.txt file of a random participant (subject_id=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list stimuli\n",
    "result_path = \"configs/\"\n",
    "subject_id = 10 #use one sample subject id to list all stimuli\n",
    "nb_stim = 35\n",
    "names = []\n",
    "clusters= []\n",
    "listing_file = result_path + \"configs\" + str(subject_id) + \"/results/\" + str(subject_id) + \"-listing.txt\"\n",
    "with open(listing_file, mode ='rU') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "lines = lines[3:nb_stim+3] #3 lines of headers before list of stims\n",
    "for line in lines:\n",
    "    [stim_id, stim_name]=line.split('\\t')\n",
    "    [id1,str1,str2,contour_id,cluster_id] = stim_name.split('_')\n",
    "    names.append(contour_id)\n",
    "    clusters.append(cluster_id.split('.')[0])\n",
    "stim_df = pd.DataFrame.from_dict({'name':names, 'contour':clusters})\n",
    "\n",
    "# Add context to stim_df\n",
    "\n",
    "experiment_set = pd.read_excel('experiment_set.xlsx')\n",
    "\n",
    "context = []\n",
    "\n",
    "for k in range(len(stim_df)):\n",
    "    name_expiration = stim_df.name[k]\n",
    "    context = context + [str(experiment_set[experiment_set.name == int(name_expiration)].context.iloc[0])]\n",
    "\n",
    "test = pd.Series(context)\n",
    "\n",
    "stim_df['context'] = test\n",
    "\n",
    "stim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then scan the result files from each participant (in subject_ids, to be updated with complete list), convert the randomized stimulus id into the corresponding unique contour id using the index prestored above, and then cumulatively build a (nb_stim,nb_stim) co-occurrence matrix (which values range between 0 and n_subjects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject_ids = [2,3,4,6,7,8,9,10,11]\n",
    "result_path = \"configs/\"\n",
    "\n",
    "cooc_matrix = np.zeros((nb_stim,nb_stim))\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "    \n",
    "    # build stim order correspondance\n",
    "    stim_order = {}\n",
    "    listing_file = result_path + \"configs\" + str(subject_id) + \"/results/\" + str(subject_id) + \"-listing.txt\"\n",
    "    if  (subject_id < 10):\n",
    "        with open(listing_file,mode='U') as f:\n",
    "            lines = [line.rstrip('\\n') for line in f]\n",
    "    else:\n",
    "        with open(listing_file,mode='rU') as f:\n",
    "            lines = [line.rstrip('\\n') for line in f]\n",
    "    lines = lines[3:nb_stim+3] #3 lines of headers before list of stims\n",
    "    for line in lines:\n",
    "        [stim_id, stim_name]=line.split('\\t')\n",
    "        [id1,str1,str2,contour,cluster] = stim_name.split('_')\n",
    "        contour_id = stim_df[stim_df['name']==contour].index.tolist()[0]    \n",
    "        stim_order[stim_id]=contour_id\n",
    "        \n",
    "    # read co-occurrences\n",
    "    result_file = result_path + \"configs\" + str(subject_id) + \"/results/\" + str(subject_id) + \"-class.txt\"\n",
    "    if (subject_id < 10):\n",
    "        with open(result_file,mode='U') as f:\n",
    "            lines = [line.rstrip('\\n') for line in f]\n",
    "    else:\n",
    "        with open(result_file,mode='rU') as f:\n",
    "            lines = [line.rstrip('\\n') for line in f]\n",
    "    lines = lines[1:]\n",
    "    for cluster in lines:\n",
    "        items = cluster.split('\\t')\n",
    "        for pair in itertools.product(items,repeat=2):\n",
    "            cooc_matrix[stim_order[pair[0]],stim_order[pair[1]]]=cooc_matrix[stim_order[pair[0]],stim_order[pair[1]]]+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3.2))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('co-occurrence matrix')\n",
    "plt.imshow(cooc_matrix)\n",
    "ax.set_aspect('equal')\n",
    "cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "cax.get_xaxis().set_visible(False)\n",
    "cax.get_yaxis().set_visible(False)\n",
    "cax.patch.set_alpha(0)\n",
    "cax.set_frame_on(False)\n",
    "plt.colorbar(orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2) Hierarchical clustering </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build hierarchical clusters based on the above co-occurrence data. scipy.cluster.hierarchy needs to operate on a distance, rather than similarity, matrix, so we first convert to distances using max-cooc, and then convert it to a compressed distance matrix (required as input by the hierarchy.linkage method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert cooccurrences to distances (max - cooc)\n",
    "max_count = np.max(cooc_matrix)\n",
    "n = cooc_matrix.shape[0]\n",
    "dist_matrix = max_count*np.ones((n,n))-cooc_matrix\n",
    "\n",
    "#compress dist matrix for scipy.linkage\n",
    "X = squareform(dist_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build clusters with linkage (default method: ward; would be useful to try other methods if needed), and display the corresponding dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = linkage(X, 'ward')\n",
    "\n",
    "# function to change label display from contour id to filename and cluster; would be interesting to also display context\n",
    "label = lambda id: str(stim_df.get_value(index=id,col='contour'))+\" (\"+ stim_df.get_value(index=id,col='context')+\")\"\n",
    "\n",
    "def my_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram')\n",
    "        plt.xlabel('Cluster of the cry (cluster)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k', linestyle='--')\n",
    "    return ddata\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Clustering by co-occurrences')\n",
    "plt.xlabel('Cry index (cluster)')\n",
    "plt.ylabel('distance')\n",
    "my_dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=75.,  # rotates the x axis labels\n",
    "    leaf_font_size=12,  # font size for the x axis labels\n",
    "    labels = map(label,range(0,35)),\n",
    "    max_d = 9.5\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we cut to have seven clusters :\n",
    "    - cluster 1 : 8 - 8 - 8 - 8\n",
    "    - cluster 2 : 6 - 3 - 3 - 2 - 5 - 2 - 3 - 7 - 2\n",
    "    - cluster 3 : 4 - 4 - 6 - 4 - 6 - 4\n",
    "    - cluster 4 : 7 - 5 - 5 - 5 - 5\n",
    "    - cluster 5 : 4 - 2 - 3 - 8 - 2\n",
    "    - cluster 6 : 3 - 6\n",
    "    - cluster 7 : 7 - 7 - 6 - 7\n",
    "    \n",
    "Contours 8, 4, 5 seem to be well defined perceptually. Contours 3 and 2 seem to be identical. Contours 6 and 7 don't seem to be well defined perceptually.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3) Statistical significance </h4>\n",
    "\n",
    "We test the significance of the association between perceptual clusters and contour prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retrieve clusters\n",
    "\n",
    "nb_clusters = 7\n",
    "clusters= fcluster(Z,nb_clusters,criterion='maxclust')\n",
    "stim_df['cluster'] = clusters\n",
    "\n",
    "#cross tab\n",
    "\n",
    "tab = pd.crosstab(stim_df.contour, stim_df.cluster)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi2 test is normally invalid, because the expected counts in most cells are < 5 (there are only 5 exemplars of each contour, which are expected to distribute into the 7 clusters as 0.57, 1.28, 0.85, 0.71, 0.71, 0.28, 0.57). We give it for comparative purposes: the Chi2 statistics (91) is very high, and even though the estimated p-value should not be trusted, this indicates that the association departs very significantly from what would be expected by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Independance test\n",
    "\n",
    "[chi_t, p_val, dof, expected] =  stats_scipy.chi2_contingency(tab)\n",
    "print(\"Chi2(%.3g)=%.2g, p=%.5g\" % (dof, chi_t, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given low cell counts, a better (=appropriate) test of association is the Fisher Exact test, using a Monte-Carlo simulation of the p-value. This is not implemented in Python, but can be run from R. The estimated p-value for 10000 simulations is found here to be 1/(10001), which means none of the simulated contingency tables were as extreme as the one observed here. i.e. the association is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statspackage = importr('stats',  robject_translations={'format_perc': '_format_perc'}) \n",
    "rdf=pandas2ri.py2ri(tab)\n",
    "result = statspackage.fisher_test(rdf, simulate_p_value = True, B = 10000)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
