{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop\n",
    "\n",
    "a = 0\n",
    "\n",
    "if a == 0:\n",
    "    b = 1\n",
    "else:\n",
    "    b = 2\n",
    "    \n",
    "# Dictionary \n",
    "    \n",
    "dict = {'Name': 'Zara', 'Age': 7, 'Class': 'First'}\n",
    "\n",
    "# How to write in python ?\n",
    "\n",
    "# Constant -> NAME_SURNAME\n",
    "# Type -> NameSurname\n",
    "# Global variable, local variable, fonction -> name_surname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path Desktop\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "from sklearn.externals import joblib\n",
    "from pylab import *\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import mixture\n",
    "\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "\n",
    "import math\n",
    "import time\n",
    "import glob,os\n",
    "import pickle\n",
    "import random\n",
    "import seaborn\n",
    "import librosa.display\n",
    "import numpy as np, scipy, matplotlib.pyplot as plt, sklearn, pandas as pd, librosa, urllib, IPython.display, os.path\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNOTATIONS EXCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the labels file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame.from_csv('labels.csv')\n",
    "\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_files = ['044/044.xls','050/050.xls','051/051.xls']\n",
    "pages = ['hungry','pee','sleepy']\n",
    "\n",
    "labels = pd.DataFrame()\n",
    "for label_file in label_files:\n",
    "    for page in pages: \n",
    "        data = pd.read_excel(label_file,page)\n",
    "        data = data[['Expiration/Inspiration', 'syllable duration', 'syllable start', \"comments\"]]\n",
    "        data.rename(inplace=True, columns={'Expiration/Inspiration':'label', 'syllable duration':'dur', 'syllable start':'start', 'comments':'file'})\n",
    "        labels = pd.concat([labels,data],ignore_index=True)  \n",
    "        \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Localisation Colums 0-3 : test.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename labels : 'SI' -> 0, 'IN' -> 1 et 'EX' -> 2. \n",
    "\n",
    "def rename_label(label_str):\n",
    "    if label_str == 'IN':\n",
    "        return 1\n",
    "    if label_str == 'EX':\n",
    "        return 2\n",
    "    return label_str\n",
    "    \n",
    "labels['label']=labels['label'].apply(rename_label)\n",
    "labels['end']=labels['start']+labels['dur']\n",
    "\n",
    "# Switch colums\n",
    "\n",
    "labels = labels[['label', 'dur','start','end','file']]\n",
    "\n",
    "labels.to_csv('labels.csv')\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the first file if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_name1 = '01m00d-1(pee&hungry)_050.wav'\n",
    "test_name2 = '01m13d-1(hungry)_051.wav'\n",
    "labels[labels.file == test_name2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATION DATA BASE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the mfcc file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_file_mfcc_norm = np.load('all_file_mfcc_norm.npy').item()\n",
    "all_file_mfcc = np.load('all_file_mfcc.npy').item()\n",
    "\n",
    "# all_file_mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the labels per frames file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_file_frame_labels = np.load('all_file_frame_labels.npy').item()\n",
    "\n",
    "# all_file_frame_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction de l'audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Windowing parameters for MFCCs\n",
    "\n",
    "SAMPLING_RATE = 44100\n",
    "L_WINDOWS = 0.020\n",
    "L_HOPS = 0.010\n",
    "L_WINDOW_N = int(round((L_WINDOWS)/(1./SAMPLING_RATE)))\n",
    "L_HOP_N = int(round((L_HOPS)/(1./SAMPLING_RATE)))\n",
    "L_WINDOW_N2 = 2**(L_WINDOW_N-1).bit_length() # POWER OF 2 FOR FFT\n",
    "L_HOP_N2 = 2**(L_WINDOW_N-1).bit_length()\n",
    "N_MFCC = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "babies = ['044','050','051']\n",
    "\n",
    "mfcc_scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# Dictionary of mfcc\n",
    "\n",
    "all_file_mfcc = {}\n",
    "all_file_mfcc_norm = {}\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states: \n",
    "        file_path = baby +'/'+baby_state+'/audio/'  \n",
    "        for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "            audio_data = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "            # normalize\n",
    "            audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "            # mfcc extraction\n",
    "            file_mfcc = librosa.feature.mfcc(audio_data, sr=SAMPLING_RATE, \\\n",
    "                                            n_mfcc=N_MFCC, n_fft = L_WINDOW_N, \\\n",
    "                                            hop_length = L_HOP_N).T\n",
    "            all_file_mfcc[os.path.basename(audio_file_path)]= file_mfcc\n",
    "            all_file_mfcc_norm[os.path.basename(audio_file_path)]= mfcc_scaler.fit_transform(file_mfcc)\n",
    "\n",
    "# We transpose the result to accommodate scikit-learn which assumes that each row is one observation, and each column is one feature dimension\n",
    "\n",
    "np.save('all_file_mfcc.npy', all_file_mfcc)\n",
    "np.save('all_file_mfcc_norm.npy', all_file_mfcc_norm) \n",
    "\n",
    "# all_file_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract label for each frame in each file\n",
    "\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "babies = ['044','050','051']\n",
    "\n",
    "# Dictionary of labels\n",
    "\n",
    "all_file_frame_labels = {}\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        file_path = baby +'/'+baby_state+'/audio/'\n",
    "        for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "            audio_file = os.path.basename(audio_file_path)\n",
    "            file_labels = labels[labels['file']==audio_file]\n",
    "            # frame labels are 0 unless they're in a IN or EX segment (=1 or 2)\n",
    "            file_frame_labels = []\n",
    "            frame_positions = [10+10*x for x in range(len(all_file_mfcc[os.path.basename(audio_file_path)]))]\n",
    "            for frame_position in frame_positions:\n",
    "                segment = file_labels[(file_labels['start']<frame_position) &(file_labels['end']>frame_position)]\n",
    "                if not segment.empty:\n",
    "                    file_frame_labels.append(segment.label.values[0])\n",
    "                else:\n",
    "                    file_frame_labels.append(0)\n",
    "            all_file_frame_labels[os.path.basename(audio_file_path)]= file_frame_labels\n",
    "                    \n",
    "np.save('all_file_frame_labels.npy', all_file_frame_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate MFCC files, concatenate label-frame files.\n",
    "Creating the data base by taking size_dset random consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creation data-set with all contexts and all baby\n",
    "\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "babies = ['044','050','051']\n",
    "\n",
    "concatenate_all_file_mfcc_norm = np.array([]).reshape(0,13)\n",
    "concatenate_all_file_mfcc = np.array([]).reshape(0,13)\n",
    "concatenate_all_file_frame_labels = []\n",
    "audio_files = []\n",
    "\n",
    "size_dset = 10000\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        file_path = baby +'/'+baby_state+'/audio/'\n",
    "        for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "            audio_file = os.path.basename(audio_file_path)\n",
    "            audio_files = audio_files + [audio_file]\n",
    "            \n",
    "# Shuffle files for keeping time relations for HMM\n",
    "\n",
    "random.shuffle(audio_files)\n",
    "            \n",
    "for audio_file in audio_files:\n",
    "    concatenate_all_file_frame_labels = concatenate_all_file_frame_labels + all_file_frame_labels[audio_file]\n",
    "    concatenate_all_file_mfcc = np.append(concatenate_all_file_mfcc,all_file_mfcc[audio_file], axis = 0)\n",
    "    concatenate_all_file_mfcc_norm = np.append(concatenate_all_file_mfcc_norm,all_file_mfcc_norm[audio_file], axis = 0)\n",
    "    \n",
    "startpoint = int(floor((len(concatenate_all_file_frame_labels)-size_dset)*random.random()))\n",
    "\n",
    "labels_dset = copy(concatenate_all_file_frame_labels[startpoint:startpoint+size_dset])\n",
    "mfcc_dset = copy(concatenate_all_file_mfcc[startpoint:startpoint+size_dset])\n",
    "mfcc_dset_norm = copy(concatenate_all_file_mfcc_norm[startpoint:startpoint+size_dset])\n",
    "\n",
    "print(shape(labels_dset),shape(mfcc_dset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ON A FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 44100\n",
    "\n",
    "audio_file_path = '044/hungry/audio/00m12d-1(hungry)_044.wav'\n",
    "\n",
    "audio_data = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "\n",
    "IPython.display.Audio(audio_data, rate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot WaveSound\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.waveplot(audio_data, SAMPLING_RATE)\n",
    "plt.ylabel('Sound Amplitude')\n",
    "\n",
    "# Plot excel annotations\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(all_file_frame_labels[os.path.basename(audio_file_path)])\n",
    "plt.ylabel('Label')\n",
    "plt.xlabel('Annotated Frames')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM creation and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose hyperparameters C and Gamma with tenfold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-3, 3, 7)\n",
    "gamma_range = np.logspace(-3, 3, 7)\n",
    "param_grid = {'gamma' : gamma_range, 'C' : C_range}\n",
    "cv = sklearn.model_selection.KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(mfcc_dset_norm, labels_dset)\n",
    "print(\"The best parameters are \" + str(grid.best_params_) + \" with a score of \" + str(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finer tuning on basis 2 for Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_range = [1.0]\n",
    "gamma_range = [0.015625,0.03125,0.0625,0.125,0.5]\n",
    "param_grid = {'gamma' : gamma_range, 'C' : C_range}\n",
    "cv = sklearn.model_selection.KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(mfcc_dset_norm, labels_dset)\n",
    "print(\"The best parameters are \" + str(grid.best_params_) + \" with a score of \" + str(grid.best_score_))\n",
    "\n",
    "C = grid.best_params_['C']\n",
    "gamma = grid.best_params_['gamma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing C and gamma. C is always 1 and gamma is often 0.0625 or 0.125. We are taking C = 1 and gamma = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create svm classifer model object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_svm = sklearn.svm.SVC(C = C, gamma = gamma, probability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_gmm3(features_train,labels_train,n_components):\n",
    "    M0 = mixture.GaussianMixture(n_components = n_components, max_iter = 50, n_init = 20, init_params = 'kmeans')\n",
    "    M1 = mixture.GaussianMixture(n_components = n_components, max_iter = 50, n_init = 20, init_params = 'kmeans')\n",
    "    M2 = mixture.GaussianMixture(n_components = n_components, max_iter = 50, n_init = 20, init_params = 'kmeans')\n",
    "    #\n",
    "    label_0 = features_train[np.where(labels_train == 0)]\n",
    "    label_1 = features_train[np.where(labels_train == 1)]\n",
    "    label_2 = features_train[np.where(labels_train == 2)]\n",
    "    #\n",
    "    M0.fit(label_0)\n",
    "    M1.fit(label_1)\n",
    "    M2.fit(label_2)\n",
    "    #\n",
    "    model = [M0,M1,M2]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_gmm3(features_test,model):\n",
    "    M0 = model[0]\n",
    "    M1 = model[1]\n",
    "    M2 = model[2]\n",
    "    # log likelihood per labels\n",
    "    llhood_0 = M0.score_samples(features_test)\n",
    "    llhood_1 = M1.score_samples(features_test)\n",
    "    llhood_2 = M2.score_samples(features_test)\n",
    "    llhood = [llhood_0,llhood_1,llhood_2]\n",
    "    lhood = np.exp(llhood)\n",
    "    # add exits and entries states\n",
    "    a = np.zeros((1,lhood.shape[1]))\n",
    "    a[a < 1e-100] = 1e-100\n",
    "    b = lhood\n",
    "    b[b < 1e-100] = 1e-100\n",
    "    llhood = np.log(concatenate((a,b,a)))\n",
    "    # predict !\n",
    "    state_seq = np.zeros(len(llhood_0))\n",
    "    state_seq [np.where((llhood_0 > llhood_1)&(llhood_0 > llhood_2))] = 0\n",
    "    state_seq [np.where((llhood_1 > llhood_0)&(llhood_1 > llhood_2))] = 1\n",
    "    state_seq [np.where((llhood_2 > llhood_0)&(llhood_2 > llhood_1))] = 2\n",
    "    return llhood, state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gmm3_score(features_test,labels_test,model):\n",
    "    state_seq = predict_gmm3(features_test,model)[1]\n",
    "    new_score = float(len(np.squeeze(np.where(state_seq == labels_test))))/float(len(labels_test))\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the outputs of SVM into a probability distribution over classes with Platt scaling extended for the multiclasse case by Wu et al. (A,B) are estimated by maximization of maximum likelihood, probabilities given by 5fold cross validation.     \n",
    "\n",
    "See \"Probability Estimates for Multi-class Classification by Pairwise Coupling\" (2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_lhood_svm(features,model):\n",
    "    predicted_proba_labels = model.predict_proba(features)\n",
    "    predicted_proba_labels = predicted_proba_labels.transpose()\n",
    "    # Add entry and exit states for viterbi\n",
    "    a = np.zeros((1,predicted_proba_labels.shape[1]))\n",
    "    a[a < 1e-100] = 1e-100\n",
    "    b = predicted_proba_labels\n",
    "    b[b < 1e-100] = 1e-100\n",
    "    # take the log-probability\n",
    "    log_lhood = np.log(concatenate((a,b,a)))\n",
    "    return log_lhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training the Transition probability Matrix for Viterbi decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_trans_matrix(states):\n",
    "    transition_prob_matrix = np.zeros((3,3))\n",
    "    for test in range(len(states)-1):\n",
    "        state1 = states[test]\n",
    "        state2 = states[test+1]\n",
    "        transition_prob_matrix[state1,state2] = transition_prob_matrix[state1,state2] + 1\n",
    "    # Normalize\n",
    "    row_sums = transition_prob_matrix.sum(axis=1)\n",
    "    row_sums = row_sums[:, np.newaxis]\n",
    "    transition_prob_matrix = transition_prob_matrix/(row_sums*np.ones((1,3))+1e-100)\n",
    "    # Add entry and exit states into transition matrix\n",
    "    transition_prob_matrix2 = np.zeros((5,5))\n",
    "    transition_prob_matrix2[1:4,1:4] = transition_prob_matrix\n",
    "    transition_prob_matrix2[0,1:4] = np.ones((1,3))/np.ones((1,3)).sum()\n",
    "    transition_prob_matrix2[transition_prob_matrix2 < 1e-100] = 1e-100\n",
    "    # return the log matrix transition\n",
    "    log_trans = np.log(transition_prob_matrix2)\n",
    "    return log_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the classifications decisions using the viterbi algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_viterbi(log_lhood, log_trans):\n",
    "    # Parameters and initialization \n",
    "    numStates, numPts = log_lhood.shape\n",
    "    delta = np.zeros(numStates)\n",
    "    psi = np.zeros((numPts,numStates))\n",
    "    # State 0 : 'entry state', state 1 : 'SI', state 2 : 'IN', state 3 : 'EX', state 4 :'exit state'\n",
    "    for i in range(1, numStates-1):\n",
    "        delta[i] = log_trans[0,i] + log_lhood[i,0]\n",
    "        psi[0,i] = 0\n",
    "    # Recursion\n",
    "    for t in range(1,numPts):\n",
    "        deltabefore = delta\n",
    "        for i in range(1,numStates-1):\n",
    "            temp = deltabefore[1:numStates-1] + log_trans[1:numStates-1,i]\n",
    "            maxDelta = np.amax(temp)\n",
    "            index = np.argmax(temp)\n",
    "            delta[i] = maxDelta + log_lhood[i,t]\n",
    "            psi[t,i] = index + 1\n",
    "    # termination\n",
    "    state_seq = np.zeros(numPts+2)\n",
    "    state_seq[numPts+1] = numStates-1\n",
    "    temp = delta[1:numStates-1] + log_trans[1:numStates-1,numStates-1]\n",
    "    maxDelta = np.amax(temp)\n",
    "    index = np.argmax(temp)\n",
    "    state_seq[numPts] = index + 1\n",
    "    #Backtracking\n",
    "    for t in range(numPts-1,0,-1):\n",
    "        state_seq[t] = psi[t,int(state_seq[t+1])]\n",
    "    state_seq[0] = 0\n",
    "    state_seq = state_seq[1:numPts+1]\n",
    "    state_seq = state_seq -1\n",
    "    return state_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score after viterbi filtering : model is SVM or GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_viterbi_score(features_test,labels_test,log_trans,model):\n",
    "    log_lhood = log_lhood_svm(features_test,model)\n",
    "    state_seq = log_viterbi(log_lhood, log_trans)\n",
    "    new_score = float(len(np.squeeze(np.where(state_seq == labels_test))))/float(len(labels_test))\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm3_viterbi_score(features_test,labels_test,log_trans,model):\n",
    "    log_lhood = predict_gmm3(features_test,model)[0]\n",
    "    state_seq = log_viterbi(log_lhood, log_trans)\n",
    "    new_score = float(len(np.squeeze(np.where(state_seq == labels_test))))/float(len(labels_test))\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing for the two classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly load the two svm model file here, if it's already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_svm = joblib.load('model_svm.pkl')\n",
    "# model_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data base in a training base (90% of the data base) and a test base (10% of the data base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate in training and data base\n",
    "\n",
    "number_train = int(round((0.90*len(labels_dset))))\n",
    "\n",
    "labels_dset_train = copy(labels_dset[:number_train])\n",
    "labels_dset_test = copy(labels_dset[number_train:])\n",
    "mfcc_dset_train = copy(mfcc_dset[:number_train])\n",
    "mfcc_dset_test = copy(mfcc_dset[number_train:])\n",
    "\n",
    "mfcc_dset_train_norm = copy(mfcc_dset_norm[:number_train])\n",
    "mfcc_dset_test_norm = copy(mfcc_dset_norm[number_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the svm classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_svm.fit(mfcc_dset_train_norm, labels_dset_train)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model_svm, 'model_svm.pkl') \n",
    "\n",
    "model_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the gmm classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "\n",
    "start = time.time()\n",
    "model_gmm3_norm = train_gmm3(mfcc_dset_train_norm,labels_dset_train,n_components)\n",
    "model_gmm3 = train_gmm3(mfcc_dset_train,labels_dset_train,n_components)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "model_gmm3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the log transition Matrix for Viterbi decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_trans = log_trans_matrix(labels_dset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy score of the two classifier on the train data : frame per frame and fpf + viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = {}\n",
    "\n",
    "score_svm = model_svm.score(mfcc_dset_train_norm, labels_dset_train)\n",
    "score_svm_vtb = svm_viterbi_score(mfcc_dset_train_norm,labels_dset_train,log_trans,model_svm)\n",
    "score_gmm3 = gmm3_score(mfcc_dset_train,labels_dset_train,model_gmm3)\n",
    "score_gmm3_vtb = gmm3_viterbi_score(mfcc_dset_train,labels_dset_train,log_trans,model_gmm3)\n",
    "score_gmm3_norm = gmm3_score(mfcc_dset_train_norm,labels_dset_train,model_gmm3_norm)\n",
    "score_gmm3_vtb_norm = gmm3_viterbi_score(mfcc_dset_train_norm,labels_dset_train,log_trans,model_gmm3_norm)\n",
    "\n",
    "score['score_svm']= score_svm\n",
    "score['score_svm_vtb'] = score_svm_vtb\n",
    "score['score_gmm3']= score_gmm3\n",
    "score['score_gmm3_vtb'] = score_gmm3_vtb\n",
    "score['score_gmm3_norm']= score_gmm3_norm\n",
    "score['score_gmm3_vtb_norm'] = score_gmm3_vtb_norm\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute the accuracy score of the classifier on the test data: frame per frame and fpf + viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = {}\n",
    "\n",
    "score_svm = model_svm.score(mfcc_dset_test_norm, labels_dset_test)\n",
    "score_svm_vtb = svm_viterbi_score(mfcc_dset_test_norm,labels_dset_test,log_trans,model_svm)\n",
    "score_gmm3 = gmm3_score(mfcc_dset_test,labels_dset_test,model_gmm3)\n",
    "score_gmm3_vtb = gmm3_viterbi_score(mfcc_dset_test,labels_dset_test,log_trans,model_gmm3)\n",
    "score_gmm3_norm = gmm3_score(mfcc_dset_test_norm,labels_dset_test,model_gmm3_norm)\n",
    "score_gmm3_vtb_norm = gmm3_viterbi_score(mfcc_dset_test_norm,labels_dset_test,log_trans,model_gmm3_norm)\n",
    "\n",
    "score['score_svm']= score_svm\n",
    "score['score_svm_vtb'] = score_svm_vtb\n",
    "score['score_gmm3']= score_gmm3\n",
    "score['score_gmm3_vtb'] = score_gmm3_vtb\n",
    "score['score_gmm3_norm']= score_gmm3_norm\n",
    "score['score_gmm3_vtb_norm'] = score_gmm3_vtb_norm\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ON A FILE FOR SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between audio, excel annotations, classification by frame per frame, segmentation per fpf + viterbi.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 14)\n",
    "\n",
    "SAMPLING_RATE = 44100\n",
    "audio_file_path = '044/hungry/audio/00m12d-1(hungry)_044.wav'\n",
    "audio_data = librosa.load(audio_file_path, sr = SAMPLING_RATE)[0]\n",
    "audio_data = (audio_data - np.mean(audio_data))/ max(abs(audio_data-np.mean(audio_data)))\n",
    "\n",
    "# Plot WaveSound\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(8, 1, 1)\n",
    "librosa.display.waveplot(audio_data, SAMPLING_RATE)\n",
    "plt.ylabel('Sound Amplitude')\n",
    "\n",
    "# Plot excel annotations\n",
    "\n",
    "plt.subplot(8, 1, 2)\n",
    "plt.plot(all_file_frame_labels[os.path.basename(audio_file_path)])\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Annotated Frames')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (SVM)\n",
    "\n",
    "file_mfcc_norm = all_file_mfcc_norm[os.path.basename(audio_file_path)]\n",
    "predicted_labels = model_svm.predict(file_mfcc_norm)\n",
    "\n",
    "plt.subplot(8, 1, 3)\n",
    "plt.plot(predicted_labels)\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by SVM')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (SVM) + viterbi \n",
    "\n",
    "log_lhood = log_lhood_svm(file_mfcc_norm,model_svm)\n",
    "state_seq = log_viterbi(log_lhood, log_trans)\n",
    "\n",
    "plt.subplot(8, 1, 4)\n",
    "plt.plot(state_seq)\n",
    "axes = plt.gca()\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by SVM + vtb')\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (GMM)\n",
    "\n",
    "file_mfcc = all_file_mfcc[os.path.basename(audio_file_path)]\n",
    "log_lhood, predicted_labels = predict_gmm3(file_mfcc,model_gmm3)\n",
    "\n",
    "plt.subplot(8, 1, 5)\n",
    "plt.plot(predicted_labels)\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by GMM')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (GMM) + viterbi \n",
    "\n",
    "state_seq = log_viterbi(log_lhood, log_trans)\n",
    "\n",
    "plt.subplot(8, 1, 6)\n",
    "plt.plot(state_seq)\n",
    "axes = plt.gca()\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by GMM + vtb')\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot classification by frame per frame (GMM) normalized\n",
    "\n",
    "log_lhood, predicted_labels = predict_gmm3(file_mfcc_norm,model_gmm3_norm)\n",
    "\n",
    "plt.subplot(8, 1, 7)\n",
    "plt.plot(predicted_labels)\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by GMM normalized')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "\n",
    "# Plot classification by frame per frame (GMM) + viterbi normalized\n",
    "\n",
    "state_seq = log_viterbi(log_lhood, log_trans)\n",
    "\n",
    "plt.subplot(8, 1, 8)\n",
    "plt.plot(state_seq)\n",
    "axes = plt.gca()\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(arange(3),('SI', 'IN', 'EX'))\n",
    "plt.xlabel('Esteemed Frames by GMM + vtb normalized')\n",
    "axes.set_xlim([0,len(all_file_frame_labels[os.path.basename(audio_file_path)])])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of states duration and start with labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def states_description(state_seq):\n",
    "    # Init\n",
    "    start_states = []\n",
    "    duration_states = []\n",
    "    type_states = []\n",
    "    #\n",
    "    duration_state = 0\n",
    "    start_state = 0\n",
    "    frame_before = state_seq[0]\n",
    "    for i in range(len(state_seq)):\n",
    "        if (frame_before != state_seq[i]):\n",
    "            start_states = start_states + [start_state]\n",
    "            duration_states = duration_states + [duration_state]\n",
    "            type_states = type_states + [frame_before]\n",
    "            #\n",
    "            start_state = start_state + duration_state + 10\n",
    "            duration_state = 0\n",
    "        duration_state = duration_state + 10\n",
    "        frame_before = state_seq[i]\n",
    "    # Add the last one\n",
    "    start_states = start_states + [start_state]\n",
    "    duration_states = duration_states + [duration_state]\n",
    "    type_states = type_states + [frame_before]\n",
    "    # Creation a DataFrame\n",
    "    df_states = pd.DataFrame({'Start' : start_states,\\\n",
    "                         'Duration' : duration_states,\\\n",
    "                         'label' : type_states})\n",
    "    return df_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_states = states_description(state_seq)\n",
    "\n",
    "def rename_label(label_str):\n",
    "    if label_str == 1:\n",
    "        return 'IN'\n",
    "    if label_str == 2:\n",
    "        return 'EX'\n",
    "    if label_str == 0:\n",
    "        return 'SI'\n",
    "    return label_str\n",
    "    \n",
    "df_states['label']=df_states['label'].apply(rename_label)\n",
    "\n",
    "df_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION AND ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ten fold cross validation : model_type = 1 for svm and svm+vtb, model_type = 2 for gmm and gmm+vtb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy_cv(features,labels,model_type,C,gamma,n_components):\n",
    "    # Initialization\n",
    "    kf = KFold(n_splits=10)\n",
    "    scores_model = np.array([])\n",
    "    scores_model_vtb = np.array([])\n",
    "    # Choice of algorithm\n",
    "    if (model_type == 1):\n",
    "        algo = 'svm'\n",
    "    else:\n",
    "        algo = 'gmm'\n",
    "    # Cross_validation\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        features_train, features_test = features[train_index], features[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "        if (model_type == 1):\n",
    "            model = sklearn.svm.SVC(C = C, gamma = gamma, probability=True)\n",
    "            model.fit(features_train, labels_train)\n",
    "            score_model = model.score(features_test, labels_test)\n",
    "            log_trans = log_trans_matrix(labels_train)\n",
    "            score_model_vtb = svm_viterbi_score(features_test,labels_test,log_trans,model)\n",
    "        else:\n",
    "            model = train_gmm3(features_train,labels_train,n_components)\n",
    "            score_model = gmm3_score(features_test,labels_test,model)\n",
    "            log_trans = log_trans_matrix(labels_train)\n",
    "            score_model_vtb = gmm3_viterbi_score(features_test,labels_test,log_trans,model)\n",
    "        scores_model = np.append(scores_model,score_model)\n",
    "        scores_model_vtb = np.append(scores_model_vtb,score_model_vtb)\n",
    "    # display scores\n",
    "    print('Accuracy ' + algo +  ' with ten fold cross validation :')\n",
    "    print(scores_model)\n",
    "    accuracy_model = scores_model.mean()\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy_model, scores_model.std() * 2))\n",
    "    print('\\n')\n",
    "    print('Accuracy ' + algo + ' + viterbi with ten fold cross validation :')\n",
    "    print(scores_model_vtb)\n",
    "    accuracy_model_vtb = scores_model_vtb.mean()\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy_model_vtb, scores_model_vtb.std() * 2))\n",
    "    print('\\n')\n",
    "    return accuracy_model, accuracy_model_vtb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION FOR DIFFERENTS DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creation 6 data-set with 1 contexts and 1 baby\n",
    "\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "babies = ['044','050']\n",
    "\n",
    "labels_dsets = {}\n",
    "mfcc_dsets = {}\n",
    "mfcc_dsets_norm = {}\n",
    "\n",
    "size_dset = 40000\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        concatenate_all_file_mfcc = np.array([]).reshape(0,13)\n",
    "        concatenate_all_file_mfcc_norm = np.array([]).reshape(0,13)\n",
    "        concatenate_all_file_frame_labels = []\n",
    "        audio_files = []\n",
    "        file_path = baby +'/'+baby_state+'/audio/'\n",
    "        for audio_file_path in glob.glob(file_path+'*.wav'):\n",
    "            audio_file = os.path.basename(audio_file_path)\n",
    "            audio_files = audio_files + [audio_file]\n",
    "        # Shuffle files for keeping time relations for HMM   \n",
    "        random.shuffle(audio_files)\n",
    "        for audio_file in audio_files:\n",
    "            concatenate_all_file_frame_labels = concatenate_all_file_frame_labels + all_file_frame_labels[audio_file]\n",
    "            concatenate_all_file_mfcc = np.append(concatenate_all_file_mfcc,all_file_mfcc[audio_file], axis = 0)\n",
    "            concatenate_all_file_mfcc_norm = np.append(concatenate_all_file_mfcc_norm,all_file_mfcc_norm[audio_file], axis = 0)\n",
    "        # Taking size_dset frames    \n",
    "        startpoint = int(floor((len(concatenate_all_file_frame_labels)-size_dset)*random.random()))\n",
    "        labels_dset = copy(concatenate_all_file_frame_labels[startpoint:startpoint+size_dset])\n",
    "        mfcc_dset = copy(concatenate_all_file_mfcc[startpoint:startpoint+size_dset])\n",
    "        mfcc_dset_norm = copy(concatenate_all_file_mfcc_norm[startpoint:startpoint+size_dset])\n",
    "        labels_dsets[baby + '-' + baby_state] = labels_dset\n",
    "        mfcc_dsets[baby + '-' + baby_state] = mfcc_dset\n",
    "        mfcc_dsets_norm[baby + '-' + baby_state] = mfcc_dset_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "gamma = 0.1\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        name_dataset = baby + '-' + baby_state\n",
    "        mfcc_dset = mfcc_dsets[name_dataset]\n",
    "        mfcc_dset_norm = mfcc_dsets_norm[name_dataset]\n",
    "        labels_dset = labels_dsets[name_dataset]\n",
    "        # SVM + SVM/Vtb\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,1,C,gamma,5)\n",
    "        accuracies[baby + '-' + baby_state + '-' + 'svm'] = accuracy_model        \n",
    "        accuracies[baby + '-' + baby_state + '-' + 'svm+vtb'] = accuracy_model_vtb\n",
    "        # GMM5 + GMM5/vtb\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset,labels_dset,2,C,gamma,5)\n",
    "        accuracies[baby + '-' + baby_state + '-' + 'gmm5'] = accuracy_model        \n",
    "        accuracies[baby + '-' + baby_state + '-' + 'gmm5+vtb'] = accuracy_model_vtb\n",
    "        # GMM5 + GMM5/vtb normalized\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,2,C,gamma,5)\n",
    "        accuracies[baby + '-' + baby_state + '-' + 'gmm5 norm'] = accuracy_model        \n",
    "        accuracies[baby + '-' + baby_state + '-' + 'gmm5+vtb norm'] = accuracy_model_vtb\n",
    "        # GMM20 + GMM20/vtb\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset,labels_dset,2,C,gamma,20)\n",
    "        accuracies[baby + '-' + baby_state + '-' + 'gmm20'] = accuracy_model        \n",
    "        accuracies[baby + '-' + baby_state + '-' + 'gmm20+vtb'] = accuracy_model_vtb\n",
    "        # GMM20 + GMM20/vtb normalized\n",
    "        accuracy_model, accuracy_model_vtb = accuracy_cv(mfcc_dset_norm,labels_dset,2,C,gamma,20)\n",
    "        accuracies[baby + '-' + baby_state + '-' + 'gmm20 norm'] = accuracy_model        \n",
    "        accuracies[baby + '-' + baby_state + '-' + 'gmm20+vtb norm'] = accuracy_model_vtb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babies = ['044','050']\n",
    "baby_states = ['hungry','pee',\"sleepy\"]\n",
    "types_algo = ['svm','svm+vtb','gmm5','gmm5+vtb','gmm5 norm','gmm5+vtb norm','gmm20','gmm20+vtb','gmm20 norm','gmm20+vtb norm']\n",
    "\n",
    "index = ['44', '44', '44', '50','50', '50']\n",
    "\n",
    "df = pd.DataFrame({'Context' : ['Hung', 'Pee', 'Sleepy', 'Hung','Pee', 'Sleepy'],\\\n",
    "                          'svm (%)' : np.random.randn(6),\\\n",
    "                   'svm+vtb (%)' : np.random.randn(6),\\\n",
    "                  'gmm5 (%)' : np.random.randn(6),\\\n",
    "                  'gmm5+vtb (%)' : np.random.randn(6),\\\n",
    "                  'gmm5 norm (%)' : np.random.randn(6),\\\n",
    "                  'gmm5+vtb norm (%)' : np.random.randn(6),\\\n",
    "                  'gmm20 (%)' : np.random.randn(6),\\\n",
    "                  'gmm20+vtb (%)' : np.random.randn(6),\\\n",
    "                  'gmm20 norm (%)' : np.random.randn(6),\\\n",
    "                  'gmm20+vtb norm (%)' : np.random.randn(6)}, index = index)\n",
    "\n",
    "df = df[['Context','svm (%)','svm+vtb (%)','gmm5 (%)','gmm5+vtb (%)','gmm5 norm (%)','gmm5+vtb norm (%)','gmm20 (%)','gmm20+vtb (%)','gmm20 norm (%)','gmm20+vtb norm (%)']]\n",
    "\n",
    "i = 0\n",
    "\n",
    "for baby in babies:\n",
    "    for baby_state in baby_states:\n",
    "        for type_algo in types_algo:\n",
    "            if (type_algo == 'svm'):\n",
    "                df.iat[i,1] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'svm+vtb'):\n",
    "                df.iat[i,2] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm5'):\n",
    "                df.iat[i,3] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm5+vtb'):\n",
    "                df.iat[i,4] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm5 norm'):\n",
    "                df.iat[i,5] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm5+vtb norm'):\n",
    "                df.iat[i,6] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm20'):\n",
    "                df.iat[i,7] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm20+vtb'):\n",
    "                df.iat[i,8] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm20 norm'):\n",
    "                df.iat[i,9] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "            elif (type_algo == 'gmm20+vtb norm'):\n",
    "                df.iat[i,10] = accuracies[baby + '-' + baby_state + '-' + type_algo]\n",
    "        i = i + 1\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
